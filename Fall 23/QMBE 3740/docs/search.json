[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Dec 11, 2023\n\n\nXander\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nXander Chapman\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nXander Chapman\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\nXander Chapman\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nXander Chapman\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\nXander Chapman\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nXander C1\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2023\n\n\nXander C2\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nQMBE 3740: Data Mining\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html#footnotes",
    "href": "posts.html#footnotes",
    "title": "Blog Posts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEmail achapman03@hamline.edu. Position Student↩︎\nEmail achapman03@hamline.edu. Position Student↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Xander Chapman",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nThe page your viewing is the “index” page, or the landing page of your site. The site is just a quarto document. And you can put anything on it that could be in a quarto document.\nThere are also settings to get a list of site items you may want people to have easy access to. For example, a list of your blog posts, projects, etc.\nThere are different styles of landing pages. In this particular style, the index.qmd page is used as an about page. Instead of immediately showcasing your work, it showcases you! You can introduce yourself and include your contact information right by your image and description.\nYou can then also decide to have some content show up below the about portion as well."
  },
  {
    "objectID": "posts/Problem Set 9/ps9.html",
    "href": "posts/Problem Set 9/ps9.html",
    "title": "Problem Set 9",
    "section": "",
    "text": "Code\n# Load the dataset\ncollege_data &lt;- read.csv(\"college.csv\")\n\n# Filter for colleges in Indiana\n# This step focuses on a specific geographical area (Indiana) as per the exercise requirement.\nindiana_colleges &lt;- filter(college_data, state == \"IN\")\nindiana_colleges_clustering &lt;- select(indiana_colleges, tuition, faculty_salary_avg)\n\n# Perform k-means clustering with k = 3\nset.seed(123)  # for reproducibility\nkmeans_result &lt;- kmeans(indiana_colleges_clustering, centers = 3)\n\n# Add the cluster information back to the data frame\nindiana_colleges$cluster &lt;- kmeans_result$cluster\n\n# Visualization\nggplot(indiana_colleges, aes(x = faculty_salary_avg, y = tuition, color = factor(cluster))) +\n  geom_point() +\n  scale_color_discrete(name = \"Cluster\") +\n  ggtitle(\"Clusters of Indiana Colleges based on Faculty Salary and Tuition\") +\n  xlab(\"Average Faculty Salary\") +\n  ylab(\"Annual Tuition Rates\") +\n  theme_minimal()\n\n\n\n\n\nWe can see three distinct clusters each represented by a different color. These clusters indicate groupings of colleges with similar faculty salary and tuition rate profiles."
  },
  {
    "objectID": "posts/Problem Set 9/ps9.html#exercise-1-clustering-colleges-in-indiana-based-on-faculty-salary-and-tuition",
    "href": "posts/Problem Set 9/ps9.html#exercise-1-clustering-colleges-in-indiana-based-on-faculty-salary-and-tuition",
    "title": "Problem Set 9",
    "section": "",
    "text": "Code\n# Load the dataset\ncollege_data &lt;- read.csv(\"college.csv\")\n\n# Filter for colleges in Indiana\n# This step focuses on a specific geographical area (Indiana) as per the exercise requirement.\nindiana_colleges &lt;- filter(college_data, state == \"IN\")\nindiana_colleges_clustering &lt;- select(indiana_colleges, tuition, faculty_salary_avg)\n\n# Perform k-means clustering with k = 3\nset.seed(123)  # for reproducibility\nkmeans_result &lt;- kmeans(indiana_colleges_clustering, centers = 3)\n\n# Add the cluster information back to the data frame\nindiana_colleges$cluster &lt;- kmeans_result$cluster\n\n# Visualization\nggplot(indiana_colleges, aes(x = faculty_salary_avg, y = tuition, color = factor(cluster))) +\n  geom_point() +\n  scale_color_discrete(name = \"Cluster\") +\n  ggtitle(\"Clusters of Indiana Colleges based on Faculty Salary and Tuition\") +\n  xlab(\"Average Faculty Salary\") +\n  ylab(\"Annual Tuition Rates\") +\n  theme_minimal()\n\n\n\n\n\nWe can see three distinct clusters each represented by a different color. These clusters indicate groupings of colleges with similar faculty salary and tuition rate profiles."
  },
  {
    "objectID": "posts/Problem Set 9/ps9.html#exercise-2-selecting-optimal-values-for-k",
    "href": "posts/Problem Set 9/ps9.html#exercise-2-selecting-optimal-values-for-k",
    "title": "Problem Set 9",
    "section": "Exercise 2: Selecting Optimal Values for k",
    "text": "Exercise 2: Selecting Optimal Values for k\nElbow Method and Silhouette Score These methods are used to determine the optimal number of clusters.\n\n\nCode\nk_values &lt;- 2:9\nsum_of_squared_distances &lt;- numeric()\nsilhouette_scores &lt;- numeric()\n\nfor (k in k_values) {\n  set.seed(0)\n  kmeans_result &lt;- kmeans(indiana_colleges_clustering, centers = k, nstart = 25)\n  sum_of_squared_distances[k - 1] &lt;- kmeans_result$tot.withinss\n  silhouette_scores[k - 1] &lt;- mean(silhouette(kmeans_result$cluster, dist(indiana_colleges_clustering))[, 3])\n}\n\nplot(k_values, sum_of_squared_distances, type = \"b\", xlab = \"k\", ylab = \"Sum of Squared Distances\", main = \"Elbow Method For Optimal k\")\n\n\n\n\n\nPlotting the Elbow Method and Silhouette Score These plots help in visually identifying the optimal k.\n\n\nCode\nplot(k_values, silhouette_scores, type = \"b\", xlab = \"k\", ylab = \"Silhouette Score\", main = \"Silhouette Score For Different k\")"
  },
  {
    "objectID": "posts/Problem Set 9/ps9.html#exercise-3-generating-cluster-diagrams",
    "href": "posts/Problem Set 9/ps9.html#exercise-3-generating-cluster-diagrams",
    "title": "Problem Set 9",
    "section": "Exercise 3: Generating Cluster Diagrams",
    "text": "Exercise 3: Generating Cluster Diagrams\nPlotting clusters for k=2 and k=4 These plots allow for a visual comparison between the two selected k values.\n\n\nCode\nplot_clusters &lt;- function(data, k) {\n  set.seed(0)\n  kmeans_result &lt;- kmeans(data[, c(\"tuition\", \"faculty_salary_avg\")], centers = k, nstart = 25)\n  data$cluster &lt;- kmeans_result$cluster\n  ggplot(data, aes(x = tuition, y = faculty_salary_avg, color = as.factor(cluster))) + geom_point() +\n    ggtitle(paste(\"Cluster of Indiana Colleges with k=\", k)) + xlab(\"Annual Tuition ($)\") + ylab(\"Average Faculty Salary ($)\")\n}\n\nplot_clusters(indiana_colleges_clustering, 2)\n\n\n\n\n\nCode\nplot_clusters(indiana_colleges_clustering, 4)\n\n\n\n\n\nThe choice between k=2 and k=4 depends on what you want to use the clustering for. k=2 provides broad categorizations, useful for general analysis. k=4 offers a more detailed view, which can be useful specific analyses."
  },
  {
    "objectID": "posts/Problem Set 9/ps9.html#exercise-4-determining-the-number-of-clusters",
    "href": "posts/Problem Set 9/ps9.html#exercise-4-determining-the-number-of-clusters",
    "title": "Problem Set 9",
    "section": "Exercise 4: Determining the Number of Clusters",
    "text": "Exercise 4: Determining the Number of Clusters\nUsing the Cereals.csv dataset, we have read in the data, dropped all missing values, and selected all variables except for name, mfr, type, weight, shelf, cups, rating.\n\n\nCode\n# Load the dataset\nfile_path &lt;- \"Cereals.csv\"\ncereals_df &lt;- read.csv(file_path, stringsAsFactors = FALSE)\n\n# Dropping missing values\ncereals_df &lt;- na.omit(cereals_df)\n\n# Selecting specific columns for clustering (excluding name, mfr, type, weight, shelf, cups, rating)\ncolumns_for_clustering &lt;- setdiff(names(cereals_df), c(\"name\", \"mfr\", \"type\", \"weight\", \"shelf\", \"cups\", \"rating\"))\ncereals_subset &lt;- cereals_df[columns_for_clustering]\n\n# Elbow Method to determine the optimal number of clusters\ncalculate_wcss &lt;- function(data) {\n  wcss &lt;- numeric(10)\n  for (n in 1:10) {\n    set.seed(42)\n    kmeans_result &lt;- kmeans(data, centers = n, nstart = 10)\n    wcss[n] &lt;- kmeans_result$tot.withinss\n  }\n  return(wcss)\n}\n\nwcss &lt;- calculate_wcss(cereals_subset)\n\nggplot() +\n  geom_line(aes(x = 1:10, y = wcss), color = \"blue\") +\n  geom_point(aes(x = 1:10, y = wcss), color = \"red\") +\n  labs(title = \"Elbow Method\", x = \"Number of clusters\", y = \"WCSS\") +\n  theme_minimal()\n\n\n\n\n\nCode\n# Silhouette Score to assess the quality of clusters\ncalculate_silhouette_scores &lt;- function(data) {\n  silhouette_scores &lt;- numeric(9)\n  for (n in 2:10) {\n    set.seed(42)\n    kmeans_result &lt;- kmeans(data, centers = n, nstart = 10)\n    silhouette_avg &lt;- mean(silhouette(kmeans_result$cluster, dist(data))[, \"sil_width\"])\n    silhouette_scores[n - 1] &lt;- silhouette_avg\n  }\n  return(silhouette_scores)\n}\n\nsilhouette_scores &lt;- calculate_silhouette_scores(cereals_subset)\n\nggplot() +\n  geom_line(aes(x = 2:10, y = silhouette_scores), color = \"blue\") +\n  geom_point(aes(x = 2:10, y = silhouette_scores), color = \"red\") +\n  labs(title = \"Silhouette Scores for Different Numbers of Clusters\", x = \"Number of Clusters\", y = \"Silhouette Score\") +\n  theme_minimal()\n\n\n\n\n\nCode\nnum_clusters &lt;- 3\n\n\nBased on Elbow Method and Silhouette Score, choose k = 3 and k = 2 for clustering"
  },
  {
    "objectID": "posts/Problem Set 9/ps9.html#exercise-5-performing-k-means-clustering-exercise-6-naming-the-clusters",
    "href": "posts/Problem Set 9/ps9.html#exercise-5-performing-k-means-clustering-exercise-6-naming-the-clusters",
    "title": "Problem Set 9",
    "section": "Exercise 5: Performing k-means Clustering & Exercise 6: Naming the Clusters",
    "text": "Exercise 5: Performing k-means Clustering & Exercise 6: Naming the Clusters\n\n\nCode\n# Performing k-means clustering with k = 3\nset.seed(42)\nkmeans_k3 &lt;- kmeans(cereals_subset, centers = 3, nstart = 10)\ncereals_df$cluster_k3 &lt;- as.factor(kmeans_k3$cluster)\n\n# Create names for the clusters for k = 3\ncluster_names_k3 &lt;- c(\"High Sugar, High Calories\", \"Mid Sugar, Mid Calories\", \"Low Sugar, Low Calories\")\n\n# Assign names to the clusters in cereals_df for k = 3\ncereals_df$cluster_name_k3 &lt;- cluster_names_k3[cereals_df$cluster_k3]\n\n# Displaying cluster centers for k = 3\ncluster_centers_k3 &lt;- as.data.frame(kmeans_k3$centers)\n\n# Visualization of cluster centers for k = 3 with named clusters\nggplot(cereals_df, aes(x = sugars, y = calories, color = cluster_name_k3)) +\n  geom_point() +\n  geom_point(data = cluster_centers_k3, aes(x = sugars, y = calories, label = cluster_names_k3), color = \"black\", size = 4, shape = 4) +\n  labs(title = \"Cluster Distribution in Sugars-Calories Space (k=3)\", x = \"Sugars\", y = \"Calories\") +\n  theme_minimal()\n\n\n\n\n\nCode\n## Performing k-means Clustering for k = 2\n\n# Performing k-means clustering with k = 2\nset.seed(42)\nkmeans_k2 &lt;- kmeans(cereals_subset, centers = 2, nstart = 10)\ncereals_df$cluster_k2 &lt;- as.factor(kmeans_k2$cluster)\n\n# Create names for the clusters for k = 2\ncluster_names_k2 &lt;- c(\"Lower Calories\", \"Higher Calories\")\n\n# Assign names to the clusters in cereals_df for k = 2\ncereals_df$cluster_name_k2 &lt;- cluster_names_k2[cereals_df$cluster_k2]\n\n# Displaying cluster centers for k = 2\ncluster_centers_k2 &lt;- as.data.frame(kmeans_k2$centers)\n\n# Visualization of cluster centers for k = 2 with named clusters\nggplot(cereals_df, aes(x = sugars, y = calories, color = cluster_name_k2)) +\n  geom_point() +\n  geom_point(data = cluster_centers_k2, aes(x = sugars, y = calories, label = cluster_names_k2), color = \"black\", size = 4, shape = 4) +\n  labs(title = \"Cluster Distribution in Sugars-Calories Space (k=2)\", x = \"Sugars\", y = \"Calories\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/Problem Set 9/ps9.html#exercise-7-determine-the-number-of-clusters",
    "href": "posts/Problem Set 9/ps9.html#exercise-7-determine-the-number-of-clusters",
    "title": "Problem Set 9",
    "section": "Exercise 7: Determine the Number of Clusters",
    "text": "Exercise 7: Determine the Number of Clusters\n\n\nCode\n# Read the dataset\nsoap_data &lt;- read_csv(\"BathSoapHousehold.csv\")\n\n\nRows: 600 Columns: 46\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (46): Member id, SEC, FEH, MT, SEX, AGE, EDU, HS, CHILD, CS, Affluence I...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# Select the relevant columns and scale\nsoap_scaled &lt;- scale(soap_data[, c(\"CHILD\", \"Affluence Index\")])\n\nset.seed(123) # For reproducibility\n\nfviz_nbclust(soap_scaled, FUNcluster = stats::kmeans, method = \"wss\") +\n  geom_vline(xintercept = 4, linetype = 2) +\n  labs(subtitle = \"Elbow method\")\n\n\n\n\n\nCode\nfviz_nbclust(soap_scaled, FUNcluster = stats::kmeans, method = \"silhouette\") +\n  labs(subtitle = \"Silhouette method\")"
  },
  {
    "objectID": "posts/Problem Set 9/ps9.html#exercise-8-visualize-the-clusters-and-describe-them",
    "href": "posts/Problem Set 9/ps9.html#exercise-8-visualize-the-clusters-and-describe-them",
    "title": "Problem Set 9",
    "section": "Exercise 8: Visualize the Clusters and Describe Them",
    "text": "Exercise 8: Visualize the Clusters and Describe Them\n\n\nCode\nset.seed(123) # For reproducibility\nk_optimal &lt;- 4 # Replace this with the number of clusters you choose\nkmeans_result &lt;- kmeans(soap_scaled, centers = k_optimal, nstart = 25)\n\nfviz_cluster(kmeans_result, data = soap_scaled, geom = \"point\")\n\n\n\n\n\nCluster 1: Lower Affluence Index, Higher number of Children This group likely consist of families that are more price-sensitive due to there being more children and a lower affluence index. They may prioritize basic necessities and bulk purchases that offer value. Targeted promotions for family packs and budget-friendly options could be effective here.\nCluster 2: Moderate Affluence Index, Lower number of Children This segment likely represents middle-income small families or couples who have moderate spending power who may be focused on a balance of quality and cost. Targeted promotions on mid-range products could be effective.\nCluster 3: High Affluence Index, High number of Children This group likely represents households with children that are more likely to spend on premium products, including those that offer conveinence, enhanced quality, or luxury. Targeted promotion for higher-end products and services could be effective.\nCluster 4: Varying Affluence Index, Lowest number of Children. This group most likely consists of members new to the work force, couples without children, and older adults whose children have moved out. This group will have diverse needs and their interests will range from higher-end luxury goods to practical and high-quality items. Targeted promotion and variety product offerings could be effective for this groups diverse needs."
  },
  {
    "objectID": "posts/Problem Set 9/ps9.html#exercise-9-create-a-table-of-average-value-and-total-volume-for-each-cluster",
    "href": "posts/Problem Set 9/ps9.html#exercise-9-create-a-table-of-average-value-and-total-volume-for-each-cluster",
    "title": "Problem Set 9",
    "section": "Exercise 9: Create a Table of Average Value and Total Volume for Each Cluster",
    "text": "Exercise 9: Create a Table of Average Value and Total Volume for Each Cluster\n\n\nCode\nsoap_data_with_clusters &lt;- soap_data %&gt;%\n  mutate(cluster = kmeans_result$cluster)\n\naverage_values &lt;- soap_data_with_clusters %&gt;%\n  group_by(cluster) %&gt;%\n  summarise_at(vars(Value, `Total Volume`), mean)\n\nprint(average_values)\n\n\n# A tibble: 4 × 3\n  cluster Value `Total Volume`\n    &lt;int&gt; &lt;dbl&gt;          &lt;dbl&gt;\n1       1  594.          5557.\n2       2 1405.         13158.\n3       3 1599.         12076.\n4       4 1451.         13384.\n\n\nCluster 3 (High Value, Less Volume): Given that Cluster 3 has the highest average value, it suggests that this group of customers tends to purchase more premium or high-margin products, although not necessarily in the largest quantities. These could be customers who opt for luxury or specialized products that are priced higher but bought less frequently.\nCluster 4 (Less Value, High Volume): Despite having a lower average value, Cluster 4 has the highest total volume. This might indicate that the group consists of frequent buyers or customers who purchase items in bulk, but these items may be of lower individual value. This could be a segment that prioritizes essential items or more economically priced goods, which they purchase regularly."
  },
  {
    "objectID": "posts/Problem Set 6/probset6.html",
    "href": "posts/Problem Set 6/probset6.html",
    "title": "Problem Set 6",
    "section": "",
    "text": "x &lt;- data %&gt;% select(-PersonalLoan)\ny &lt;- data$PersonalLoan\n\ndata$PersonalLoan &lt;- as.factor(data$PersonalLoan)\n\ntrain_index &lt;- createDataPartition(data$PersonalLoan, p = .8,\n                                   list = FALSE,\n                                   times = 1)\n\ntrain_data &lt;- data[train_index, ]\ntest_data &lt;- data[-train_index, ]"
  },
  {
    "objectID": "posts/Problem Set 6/probset6.html#partition-the-data",
    "href": "posts/Problem Set 6/probset6.html#partition-the-data",
    "title": "Problem Set 6",
    "section": "",
    "text": "x &lt;- data %&gt;% select(-PersonalLoan)\ny &lt;- data$PersonalLoan\n\ndata$PersonalLoan &lt;- as.factor(data$PersonalLoan)\n\ntrain_index &lt;- createDataPartition(data$PersonalLoan, p = .8,\n                                   list = FALSE,\n                                   times = 1)\n\ntrain_data &lt;- data[train_index, ]\ntest_data &lt;- data[-train_index, ]"
  },
  {
    "objectID": "posts/Problem Set 6/probset6.html#build-the-best-tuned-decision-tree-you-can",
    "href": "posts/Problem Set 6/probset6.html#build-the-best-tuned-decision-tree-you-can",
    "title": "Problem Set 6",
    "section": "2. Build the best tuned decision tree you can",
    "text": "2. Build the best tuned decision tree you can\n\ntuneGrid &lt;- expand.grid(.cp = seq(0.001, 0.1, by = 0.001))\ntuned_model &lt;- train(\n  PersonalLoan ~ ., \n  data = train_data, \n  method = \"rpart\", \n  trControl = trainControl(\"cv\", number = 10), \n  tuneGrid = tuneGrid\n)\nbest_cp &lt;- tuned_model$bestTune\ndt_fit &lt;- rpart(PersonalLoan ~ ., data = train_data, method = \"class\", control = rpart.control(cp = best_cp$.cp))\n\npred_dt &lt;- as.factor(ifelse(predict(dt_fit, test_data, type = \"prob\")[, 2] &gt; 0.5, 1, 0))"
  },
  {
    "objectID": "posts/Problem Set 6/probset6.html#build-the-best-tuned-random-forest-you-can",
    "href": "posts/Problem Set 6/probset6.html#build-the-best-tuned-random-forest-you-can",
    "title": "Problem Set 6",
    "section": "3. Build the best tuned random forest you can",
    "text": "3. Build the best tuned random forest you can\n\nrf_fit &lt;- randomForest(PersonalLoan ~ ., data = train_data, ntree = 100)\npred_rf &lt;- as.factor(ifelse(predict(rf_fit, test_data, type = \"prob\")[, 2] &gt; 0.5, 1, 0))"
  },
  {
    "objectID": "posts/Problem Set 6/probset6.html#build-the-best-tuned-gradient-boosting-machine-you-can",
    "href": "posts/Problem Set 6/probset6.html#build-the-best-tuned-gradient-boosting-machine-you-can",
    "title": "Problem Set 6",
    "section": "4. Build the best tuned gradient boosting machine you can",
    "text": "4. Build the best tuned gradient boosting machine you can\n\nh2o.init()  # Initialize the H2O cluster\n\n\nH2O is not running yet, starting it now...\n\nNote:  In case of errors look at the following log files:\n    /var/folders/6w/gv7ywr811sd43m8x8fbtdns80000gn/T//Rtmp06BUtk/fileb04a34541bfa/h2o_xander_started_from_r.out\n    /var/folders/6w/gv7ywr811sd43m8x8fbtdns80000gn/T//Rtmp06BUtk/fileb04a7df8d4ec/h2o_xander_started_from_r.err\n\n\nStarting H2O JVM and connecting: ...... Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         6 seconds 511 milliseconds \n    H2O cluster timezone:       America/Chicago \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.42.0.2 \n    H2O cluster version age:    3 months and 24 days \n    H2O cluster name:           H2O_started_from_R_xander_pyg683 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   1.77 GB \n    H2O cluster total cores:    8 \n    H2O cluster allowed cores:  8 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.3.2 (2023-10-31) \n\n# Convert your train and test data into H2O Frame\ntrain_h2o &lt;- as.h2o(train_data)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntest_h2o &lt;- as.h2o(test_data)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Set the predictor names and the response column name\npredictors &lt;- setdiff(names(train_data), \"PersonalLoan\")\nresponse &lt;- \"PersonalLoan\"\n\n# Train the GBM model\ngbm_fit &lt;- h2o.gbm(\n  x = predictors,\n  y = response,\n  training_frame = train_h2o,\n  validation_frame = test_h2o,\n  max_depth = 5,\n  learn_rate = 0.1\n)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |======================================================================| 100%\n\n# Obtain probability predictions\n# Obtain probability predictions\nprob_gbm &lt;- h2o.predict(gbm_fit, test_h2o)$p1  # Adjusted to prob_gbm\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nprob_gbm &lt;- as.vector(prob_gbm)  # Convert H2O Frame to R vector\npred_gbm &lt;- ifelse(prob_gbm &gt; 0.5, 1, 0)\npred_gbm &lt;- as.factor(pred_gbm)"
  },
  {
    "objectID": "posts/Problem Set 6/probset6.html#compare-the-precision-and-sensitivity-of-all-three-models-on-the-testing-data",
    "href": "posts/Problem Set 6/probset6.html#compare-the-precision-and-sensitivity-of-all-three-models-on-the-testing-data",
    "title": "Problem Set 6",
    "section": "5. Compare the precision and sensitivity of all three models on the testing data",
    "text": "5. Compare the precision and sensitivity of all three models on the testing data\n\ncm_dt &lt;- confusionMatrix(pred_dt, test_data$PersonalLoan)\ncm_rf &lt;- confusionMatrix(pred_rf, test_data$PersonalLoan)\npred_gbm &lt;- as.factor(pred_gbm)\ntest_data$PersonalLoan &lt;- as.factor(test_data$PersonalLoan)\nlevels(pred_gbm) &lt;- levels(test_data$PersonalLoan)\ncm_gbm &lt;- confusionMatrix(pred_gbm, test_data$PersonalLoan)\n\n\n# For Decision Tree model\nprecision_dt &lt;- cm_dt$byClass['Pos Pred Value']\nsensitivity_dt &lt;- cm_dt$byClass['Sensitivity']\n\n# For Random Forest model\nprecision_rf &lt;- cm_rf$byClass['Pos Pred Value']\nsensitivity_rf &lt;- cm_rf$byClass['Sensitivity']\n\n# For GBM model\nprecision_gbm &lt;- cm_gbm$byClass['Pos Pred Value']\nsensitivity_gbm &lt;- cm_gbm$byClass['Sensitivity']\n\n# Creating a data frame to hold the metrics for easy comparison\ncomparison_df &lt;- data.frame(\n  Model = c(\"Decision Tree\", \"Random Forest\", \"H2O GBM\"),\n  Precision = c(precision_dt, precision_rf, precision_gbm),\n  Sensitivity = c(sensitivity_dt, sensitivity_rf, sensitivity_gbm)\n)\nprint(comparison_df)\n\n          Model Precision Sensitivity\n1 Decision Tree 0.9040000   1.0000000\n2 Random Forest 0.9923161   1.0000000\n3       H2O GBM 0.9911894   0.9955752"
  },
  {
    "objectID": "posts/Problem Set 6/probset6.html#create-an-roc-plot-comparing-all-three-models-on-the-testing-data.-which-has-the-greatest-auc",
    "href": "posts/Problem Set 6/probset6.html#create-an-roc-plot-comparing-all-three-models-on-the-testing-data.-which-has-the-greatest-auc",
    "title": "Problem Set 6",
    "section": "6. Create an ROC plot comparing all three models on the testing data. Which has the greatest AUC?",
    "text": "6. Create an ROC plot comparing all three models on the testing data. Which has the greatest AUC?\n\ny_test &lt;- test_data$PersonalLoan\n\nroc_dt &lt;- roc(y_test, as.numeric(pred_dt))\nroc_rf &lt;- roc(y_test, as.numeric(pred_rf))\nroc_gbm &lt;- roc(y_test, as.numeric(pred_gbm))\n\n\n\n\n\n\n[1] \"Decision Tree ROC 0.5\"\n\n\n[1] \"Random Forest ROC 0.963541666666667\"\n\n\n[1] \"H2O GBM ROC 0.956120943952802\""
  },
  {
    "objectID": "posts/Problem Set 6/probset6.html#write-a-couple-of-paragraphs-explaining-the-importance-of-data-partitioning-to-a-manager",
    "href": "posts/Problem Set 6/probset6.html#write-a-couple-of-paragraphs-explaining-the-importance-of-data-partitioning-to-a-manager",
    "title": "Problem Set 6",
    "section": "7. Write a couple of paragraphs explaining the importance of data partitioning to a manager",
    "text": "7. Write a couple of paragraphs explaining the importance of data partitioning to a manager\nData partitioning is imoprtant to our model creation because without it the model would train and test on the same data. When we split the data into training and testing data, we allow the model to test itself on a “fresh” set of data that is different from the data it trained on."
  },
  {
    "objectID": "posts/Problem Set 6/probset6.html#write-a-couple-of-paragraphs-carefully-explaining-to-a-manager-how-bagging-and-ensemble-models-can-improve-model-accuracy-and-performance",
    "href": "posts/Problem Set 6/probset6.html#write-a-couple-of-paragraphs-carefully-explaining-to-a-manager-how-bagging-and-ensemble-models-can-improve-model-accuracy-and-performance",
    "title": "Problem Set 6",
    "section": "8. Write a couple of paragraphs carefully explaining to a manager how bagging and ensemble models can improve model accuracy and performance",
    "text": "8. Write a couple of paragraphs carefully explaining to a manager how bagging and ensemble models can improve model accuracy and performance\nBagging and ensemble methods allow us to use multiple model’s predictions intead of just one. This allows us to combine the insights from multiple different models, giving us a more accurate solution or answer."
  },
  {
    "objectID": "posts/Problem Set 8/probset8.html",
    "href": "posts/Problem Set 8/probset8.html",
    "title": "Problem Set 8",
    "section": "",
    "text": "You work in a hospital and have access to patient medical records. You decide to use association rules on a variety of datasets available to you. In this context, what are examples of association rules that you might discover that fit into each of the following cateogries?\n\n\n\nAn actionable rule is one that can be used to make decisions or take actions that can benefit the hospital or patients. For example: a patient perscribed on medication A, should also take supplement J.\n\n\n\nA trivial rule is one that is obvious or well-known, and thus doesn’t provide new insight. For example: a patient with flu symptoms often have a fever.\n\n\n\nAn inexplicable rule is one that, while statistically significant, doesn’t have an obvious rationale or clinical explanation. For example: patients who come in tuesdays are more likely to be diabetic than those who come in on other days.\n\n\n\nThink of an organization where you currently work, have worked in the past, or an organization you are familiar with (like a school, community group, etc.). What is an application of association rules that might be useful in that environment?\nA place that I have worked previously would be in summer outdoor education. An association rule that would be helpful in that environment would be understanding participant’s interests. For example, someone who signs up for a climbing class, might also be interested in a landscape photography class. This would allow for targeted suggestions which would increase class participation.\n\n\n\nContinue to explore the groceries.csv dataset that we used in class and that was presented in the Chapter 11 case study. Answer the following questions.\n\n\n\nWhat are the 10 least frequently purchased items?\n\n\n            baby food  sound storage medium preservation products \n                    1                     1                     2 \n                 bags       kitchen utensil        baby cosmetics \n                    4                     4                     6 \n       frozen chicken        toilet cleaner       make up remover \n                    6                     7                     8 \n       salad dressing \n                    8 \n\n\n\n\n\nIf you change the minimum rule length to 3, how many rules do you generate? What if you change it to 4?\n\n\n[1] \"Number of rules with minimum length of 3: 32252\"\n\n\n[1] \"Number of rules with minimum length of 4: 14378\"\n\n\n\n\n\nChange the minimum rule length back to 2 and produce a list of rules involving either soda or whipped/sour cream.\n\n\n    lhs                   rhs    support     confidence coverage    lift    \n[1] {tidbits}          =&gt; {soda} 0.001016777 0.4347826  0.002338587 2.493345\n[2] {snack products}   =&gt; {soda} 0.001118454 0.3666667  0.003050330 2.102721\n[3] {bathroom cleaner} =&gt; {soda} 0.001016777 0.3703704  0.002745297 2.123961\n    count\n[1] 10   \n[2] 11   \n[3] 10"
  },
  {
    "objectID": "posts/Problem Set 8/probset8.html#part-1",
    "href": "posts/Problem Set 8/probset8.html#part-1",
    "title": "Problem Set 8",
    "section": "",
    "text": "You work in a hospital and have access to patient medical records. You decide to use association rules on a variety of datasets available to you. In this context, what are examples of association rules that you might discover that fit into each of the following cateogries?\n\n\n\nAn actionable rule is one that can be used to make decisions or take actions that can benefit the hospital or patients. For example: a patient perscribed on medication A, should also take supplement J.\n\n\n\nA trivial rule is one that is obvious or well-known, and thus doesn’t provide new insight. For example: a patient with flu symptoms often have a fever.\n\n\n\nAn inexplicable rule is one that, while statistically significant, doesn’t have an obvious rationale or clinical explanation. For example: patients who come in tuesdays are more likely to be diabetic than those who come in on other days.\n\n\n\nThink of an organization where you currently work, have worked in the past, or an organization you are familiar with (like a school, community group, etc.). What is an application of association rules that might be useful in that environment?\nA place that I have worked previously would be in summer outdoor education. An association rule that would be helpful in that environment would be understanding participant’s interests. For example, someone who signs up for a climbing class, might also be interested in a landscape photography class. This would allow for targeted suggestions which would increase class participation.\n\n\n\nContinue to explore the groceries.csv dataset that we used in class and that was presented in the Chapter 11 case study. Answer the following questions.\n\n\n\nWhat are the 10 least frequently purchased items?\n\n\n            baby food  sound storage medium preservation products \n                    1                     1                     2 \n                 bags       kitchen utensil        baby cosmetics \n                    4                     4                     6 \n       frozen chicken        toilet cleaner       make up remover \n                    6                     7                     8 \n       salad dressing \n                    8 \n\n\n\n\n\nIf you change the minimum rule length to 3, how many rules do you generate? What if you change it to 4?\n\n\n[1] \"Number of rules with minimum length of 3: 32252\"\n\n\n[1] \"Number of rules with minimum length of 4: 14378\"\n\n\n\n\n\nChange the minimum rule length back to 2 and produce a list of rules involving either soda or whipped/sour cream.\n\n\n    lhs                   rhs    support     confidence coverage    lift    \n[1] {tidbits}          =&gt; {soda} 0.001016777 0.4347826  0.002338587 2.493345\n[2] {snack products}   =&gt; {soda} 0.001118454 0.3666667  0.003050330 2.102721\n[3] {bathroom cleaner} =&gt; {soda} 0.001016777 0.3703704  0.002745297 2.123961\n    count\n[1] 10   \n[2] 11   \n[3] 10"
  },
  {
    "objectID": "posts/Problem Set 8/probset8.html#part-2",
    "href": "posts/Problem Set 8/probset8.html#part-2",
    "title": "Problem Set 8",
    "section": "Part 2",
    "text": "Part 2\nUse the Market_Basket_Optimisation.csv dataset provided on Canvas and perform association rule miningas we did in class with both the groceries and lastfm datasets. Perform the following tasks and answer the related questions.\n\n1\nRead the transactions into R.\n\n# Display the first few transactions\ninspect(head(transactions))\n\n    items               \n[1] {almonds,           \n     antioxydant juice, \n     avocado,           \n     cottage cheese,    \n     energy drink,      \n     frozen smoothie,   \n     green grapes,      \n     green tea,         \n     honey,             \n     low fat yogurt,    \n     mineral water,     \n     olive oil,         \n     salad,             \n     salmon,            \n     shrimp,            \n     spinach,           \n     tomato juice,      \n     vegetables mix,    \n     whole weat flour,  \n     yams}              \n[2] {burgers,           \n     eggs,              \n     meatballs}         \n[3] {chutney}           \n[4] {avocado,           \n     turkey}            \n[5] {energy bar,        \n     green tea,         \n     milk,              \n     mineral water,     \n     whole wheat rice}  \n[6] {low fat yogurt}    \n\n\n\n\n2\nUse the summary() function to answer the quetsions:\n\n\n[1] \"Number of transactions: 7501\"\n\n\n[1] \"Number of distinct items: 119\"\n\n\n[1] \"Number of possible itemsets: 6.64613997892458e+35\"\n\n\n[1] \"Number of possible itemsets: 6.64613997892458e+35\"\n\n\n\n\n3\nUsing the summary() function output, create a graph showing the distribution of transaction sizes in the data.\n\n\n\n\n\n\n\n4\nUsing the itemFrequency() function, create a dataset of items and their frequencies and determine the ten most frequent items, and the ten least frequent items.\n\n\n[1] \"The ten most frequent items are:\"\n\n\n                               Item Frequency\nmineral water         mineral water      1788\neggs                           eggs      1348\nspaghetti                 spaghetti      1306\nfrench fries           french fries      1282\nchocolate                 chocolate      1229\ngreen tea                 green tea       991\nmilk                           milk       972\nground beef             ground beef       737\nfrozen vegetables frozen vegetables       715\npancakes                   pancakes       713\n\n\n[1] \"The ten least frequent items are:\"\n\n\n                           Item Frequency\nketchup                 ketchup        33\noatmeal                 oatmeal        33\nchocolate bread chocolate bread        32\nchutney                 chutney        31\nmashed potato     mashed potato        31\ntea                         tea        29\nbramble                 bramble        14\ncream                     cream         7\nnapkins                 napkins         5\nwater spray         water spray         3\n\n\n\n\n5\nUse descriptives statistics on the item frequencies to determine a reasonable support threshold (use confidence=0.25 and minlen = 2) and generate the association rules using the apriori algorithm.\n\n\nset of 4 rules\n\nrule length distribution (lhs + rhs):sizes\n2 \n4 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      2       2       2       2       2       2 \n\nsummary of quality measures:\n    support          confidence        coverage           lift      \n Min.   :0.05093   Min.   :0.2506   Min.   :0.1638   Min.   :1.189  \n 1st Qu.:0.05223   1st Qu.:0.2752   1st Qu.:0.1715   1st Qu.:1.308  \n Median :0.05619   Median :0.3024   Median :0.1769   Median :1.394  \n Mean   :0.05576   Mean   :0.2996   Mean   :0.1890   Mean   :1.354  \n 3rd Qu.:0.05973   3rd Qu.:0.3268   3rd Qu.:0.1944   3rd Qu.:1.439  \n Max.   :0.05973   Max.   :0.3430   Max.   :0.2384   Max.   :1.439  \n     count      \n Min.   :382.0  \n 1st Qu.:391.8  \n Median :421.5  \n Mean   :418.2  \n 3rd Qu.:448.0  \n Max.   :448.0  \n\nmining info:\n         data ntransactions support confidence\n transactions          7501    0.05       0.25\n                                                                                              call\n apriori(data = transactions, parameter = list(supp = support_threshold, conf = 0.25, minlen = 2))\n\n\n\n\n6\nEvaluate the rules and answer:\n\nHow many association rules were generated?\nHow many different rule lengths are there and how many rules are in each length?\nPrintout the top 12 association rules by confidence.\nPrintout the top 12 association rules by lift.\n\n\n\n[1] \"Number of association rules generated: 4\"\n\n\n[1] \"Rules length distribution:\"\n\n\n\n2 \n4 \n\n\n[1] \"Top 12 rules by confidence:\"\n\n\n    lhs                rhs             support    confidence coverage  lift    \n[1] {spaghetti}     =&gt; {mineral water} 0.05972537 0.3430322  0.1741101 1.439085\n[2] {chocolate}     =&gt; {mineral water} 0.05265965 0.3213995  0.1638448 1.348332\n[3] {eggs}          =&gt; {mineral water} 0.05092654 0.2833828  0.1797094 1.188845\n[4] {mineral water} =&gt; {spaghetti}     0.05972537 0.2505593  0.2383682 1.439085\n    count\n[1] 448  \n[2] 395  \n[3] 382  \n[4] 448  \n\n\n[1] \"Top 12 rules by lift:\"\n\n\n    lhs                rhs             support    confidence coverage  lift    \n[1] {spaghetti}     =&gt; {mineral water} 0.05972537 0.3430322  0.1741101 1.439085\n[2] {mineral water} =&gt; {spaghetti}     0.05972537 0.2505593  0.2383682 1.439085\n[3] {chocolate}     =&gt; {mineral water} 0.05265965 0.3213995  0.1638448 1.348332\n[4] {eggs}          =&gt; {mineral water} 0.05092654 0.2833828  0.1797094 1.188845\n    count\n[1] 448  \n[2] 448  \n[3] 395  \n[4] 382  \n\n\n\n\n7\nUsing the subset() function, printout the top 10 association rules by lift, that do not include the 6 most frequent items.\n\n\n[1] \"Top 10 rules by lift, excluding the 6 most frequent items:\"\n\n\n\n\n8\nDiscuss a couple of the rules you found most interesting and explain how you think they might be used in a retail context.\n\n\nset of 4 rules\n\nrule length distribution (lhs + rhs):sizes\n2 \n4 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      2       2       2       2       2       2 \n\nsummary of quality measures:\n    support          confidence        coverage           lift      \n Min.   :0.05093   Min.   :0.2506   Min.   :0.1638   Min.   :1.189  \n 1st Qu.:0.05223   1st Qu.:0.2752   1st Qu.:0.1715   1st Qu.:1.308  \n Median :0.05619   Median :0.3024   Median :0.1769   Median :1.394  \n Mean   :0.05576   Mean   :0.2996   Mean   :0.1890   Mean   :1.354  \n 3rd Qu.:0.05973   3rd Qu.:0.3268   3rd Qu.:0.1944   3rd Qu.:1.439  \n Max.   :0.05973   Max.   :0.3430   Max.   :0.2384   Max.   :1.439  \n     count      \n Min.   :382.0  \n 1st Qu.:391.8  \n Median :421.5  \n Mean   :418.2  \n 3rd Qu.:448.0  \n Max.   :448.0  \n\nmining info:\n         data ntransactions support confidence\n transactions          7501    0.05       0.25\n                                                                                              call\n apriori(data = transactions, parameter = list(supp = support_threshold, conf = 0.25, minlen = 2))\n\n\n    lhs                rhs             support    confidence coverage  lift    \n[1] {spaghetti}     =&gt; {mineral water} 0.05972537 0.3430322  0.1741101 1.439085\n[2] {mineral water} =&gt; {spaghetti}     0.05972537 0.2505593  0.2383682 1.439085\n[3] {chocolate}     =&gt; {mineral water} 0.05265965 0.3213995  0.1638448 1.348332\n[4] {eggs}          =&gt; {mineral water} 0.05092654 0.2833828  0.1797094 1.188845\n    count\n[1] 448  \n[2] 448  \n[3] 395  \n[4] 382  \n\n\nHere’s a rewritten version of the provided association rules:\n{spaghetti} =&gt; {mineral water}\nSupport: 5.97% Confidence: 34.30% Lift: 1.44 Transactions: 448\nFrom this rule, we can infer that when customers purchase spaghetti, they also frequently buy mineral water. The likelihood of buying mineral water after buying spaghetti is 1.44 times higher than buying mineral water without considering the purchase of spaghetti.\n{chocolate} =&gt; {mineral water}\nSupport: 5.27% Confidence: 32.14% Lift: 1.35 Transactions: 395\nCustomers who buy chocolate also have a tendency to buy mineral water. The probability of purchasing mineral water after buying chocolate is 1.35 times higher than just buying mineral water on its own.\n{eggs} =&gt; {mineral water}\nSupport: 5.09% Confidence: 28.34% Lift: 1.19 Transactions: 382\nThis rule suggests that eggs and mineral water are often bought together. Customers purchasing eggs are 1.19 times more likely to buy mineral water compared to those who don’t buy eggs.\nThese rules might be used to place frequently bought products near each other, for promotional bundling, for targeted advertising, inventory management, and gives us an insight to customer behavior. These things can help save costs and improve sales."
  },
  {
    "objectID": "posts/Problem Set 3/problemset3.html",
    "href": "posts/Problem Set 3/problemset3.html",
    "title": "Data Mining: Problem Set 3",
    "section": "",
    "text": "This analysis aims to explore and model the Toyota Corolla dataset to determine the influence of various factors on the price.\nThe first few rows of the dataset are:\n\n\n  Id                                         Model Price Age_08_04 Mfg_Month\n1  1 TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors 13500        23        10\n2  2 TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors 13750        23        10\n3  3 TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors 13950        24         9\n4  4 TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors 14950        26         7\n5  5   TOYOTA Corolla 2.0 D4D HATCHB SOL 2/3-Doors 13750        30         3\n6  6   TOYOTA Corolla 2.0 D4D HATCHB SOL 2/3-Doors 12950        32         1\n  Mfg_Year    KM Fuel_Type HP Met_Color  Color Automatic   CC Doors Cylinders\n1     2002 46986    Diesel 90         1   Blue         0 2000     3         4\n2     2002 72937    Diesel 90         1 Silver         0 2000     3         4\n3     2002 41711    Diesel 90         1   Blue         0 2000     3         4\n4     2002 48000    Diesel 90         0  Black         0 2000     3         4\n5     2002 38500    Diesel 90         0  Black         0 2000     3         4\n6     2002 61000    Diesel 90         0  White         0 2000     3         4\n  Gears Quarterly_Tax Weight Mfr_Guarantee BOVAG_Guarantee Guarantee_Period ABS\n1     5           210   1165             0               1                3   1\n2     5           210   1165             0               1                3   1\n3     5           210   1165             1               1                3   1\n4     5           210   1165             1               1                3   1\n5     5           210   1170             1               1                3   1\n6     5           210   1170             0               1                3   1\n  Airbag_1 Airbag_2 Airco Automatic_airco Boardcomputer CD_Player Central_Lock\n1        1        1     0               0             1         0            1\n2        1        1     1               0             1         1            1\n3        1        1     0               0             1         0            0\n4        1        1     0               0             1         0            0\n5        1        1     1               0             1         0            1\n6        1        1     1               0             1         0            1\n  Powered_Windows Power_Steering Radio Mistlamps Sport_Model Backseat_Divider\n1               1              1     0         0           0                1\n2               0              1     0         0           0                1\n3               0              1     0         0           0                1\n4               0              1     0         0           0                1\n5               1              1     0         1           0                1\n6               1              1     0         1           0                1\n  Metallic_Rim Radio_cassette Parking_Assistant Tow_Bar\n1            0              0                 0       0\n2            0              0                 0       0\n3            0              0                 0       0\n4            0              0                 0       0\n5            0              0                 0       0\n6            0              0                 0       0\n\n\nChecking the structure and summary of the dataset provides us with an overview of the type of variables and basic statistics.\n\nstr(data)\n\n'data.frame':   1436 obs. of  39 variables:\n $ Id               : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Model            : chr  \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" ...\n $ Price            : int  13500 13750 13950 14950 13750 12950 16900 18600 21500 12950 ...\n $ Age_08_04        : int  23 23 24 26 30 32 27 30 27 23 ...\n $ Mfg_Month        : int  10 10 9 7 3 1 6 3 6 10 ...\n $ Mfg_Year         : int  2002 2002 2002 2002 2002 2002 2002 2002 2002 2002 ...\n $ KM               : int  46986 72937 41711 48000 38500 61000 94612 75889 19700 71138 ...\n $ Fuel_Type        : chr  \"Diesel\" \"Diesel\" \"Diesel\" \"Diesel\" ...\n $ HP               : int  90 90 90 90 90 90 90 90 192 69 ...\n $ Met_Color        : int  1 1 1 0 0 0 1 1 0 0 ...\n $ Color            : chr  \"Blue\" \"Silver\" \"Blue\" \"Black\" ...\n $ Automatic        : int  0 0 0 0 0 0 0 0 0 0 ...\n $ CC               : int  2000 2000 2000 2000 2000 2000 2000 2000 1800 1900 ...\n $ Doors            : int  3 3 3 3 3 3 3 3 3 3 ...\n $ Cylinders        : int  4 4 4 4 4 4 4 4 4 4 ...\n $ Gears            : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Quarterly_Tax    : int  210 210 210 210 210 210 210 210 100 185 ...\n $ Weight           : int  1165 1165 1165 1165 1170 1170 1245 1245 1185 1105 ...\n $ Mfr_Guarantee    : int  0 0 1 1 1 0 0 1 0 0 ...\n $ BOVAG_Guarantee  : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Guarantee_Period : int  3 3 3 3 3 3 3 3 3 3 ...\n $ ABS              : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Airbag_1         : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Airbag_2         : int  1 1 1 1 1 1 1 1 0 1 ...\n $ Airco            : int  0 1 0 0 1 1 1 1 1 1 ...\n $ Automatic_airco  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Boardcomputer    : int  1 1 1 1 1 1 1 1 0 1 ...\n $ CD_Player        : int  0 1 0 0 0 0 0 1 0 0 ...\n $ Central_Lock     : int  1 1 0 0 1 1 1 1 1 0 ...\n $ Powered_Windows  : int  1 0 0 0 1 1 1 1 1 0 ...\n $ Power_Steering   : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Radio            : int  0 0 0 0 0 0 0 0 1 0 ...\n $ Mistlamps        : int  0 0 0 0 1 1 0 0 0 0 ...\n $ Sport_Model      : int  0 0 0 0 0 0 1 0 0 0 ...\n $ Backseat_Divider : int  1 1 1 1 1 1 1 1 0 1 ...\n $ Metallic_Rim     : int  0 0 0 0 0 0 0 0 1 0 ...\n $ Radio_cassette   : int  0 0 0 0 0 0 0 0 1 0 ...\n $ Parking_Assistant: int  0 0 0 0 0 0 0 0 0 0 ...\n $ Tow_Bar          : int  0 0 0 0 0 0 0 0 0 0 ...\n\nsummary(data)\n\n       Id            Model               Price         Age_08_04    \n Min.   :   1.0   Length:1436        Min.   : 4350   Min.   : 1.00  \n 1st Qu.: 361.8   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Median : 721.5   Mode  :character   Median : 9900   Median :61.00  \n Mean   : 721.6                      Mean   :10731   Mean   :55.95  \n 3rd Qu.:1081.2                      3rd Qu.:11950   3rd Qu.:70.00  \n Max.   :1442.0                      Max.   :32500   Max.   :80.00  \n   Mfg_Month         Mfg_Year          KM          Fuel_Type        \n Min.   : 1.000   Min.   :1998   Min.   :     1   Length:1436       \n 1st Qu.: 3.000   1st Qu.:1998   1st Qu.: 43000   Class :character  \n Median : 5.000   Median :1999   Median : 63390   Mode  :character  \n Mean   : 5.549   Mean   :2000   Mean   : 68533                     \n 3rd Qu.: 8.000   3rd Qu.:2001   3rd Qu.: 87021                     \n Max.   :12.000   Max.   :2004   Max.   :243000                     \n       HP          Met_Color         Color             Automatic      \n Min.   : 69.0   Min.   :0.0000   Length:1436        Min.   :0.00000  \n 1st Qu.: 90.0   1st Qu.:0.0000   Class :character   1st Qu.:0.00000  \n Median :110.0   Median :1.0000   Mode  :character   Median :0.00000  \n Mean   :101.5   Mean   :0.6748                      Mean   :0.05571  \n 3rd Qu.:110.0   3rd Qu.:1.0000                      3rd Qu.:0.00000  \n Max.   :192.0   Max.   :1.0000                      Max.   :1.00000  \n       CC            Doors         Cylinders     Gears       Quarterly_Tax   \n Min.   : 1300   Min.   :2.000   Min.   :4   Min.   :3.000   Min.   : 19.00  \n 1st Qu.: 1400   1st Qu.:3.000   1st Qu.:4   1st Qu.:5.000   1st Qu.: 69.00  \n Median : 1600   Median :4.000   Median :4   Median :5.000   Median : 85.00  \n Mean   : 1577   Mean   :4.033   Mean   :4   Mean   :5.026   Mean   : 87.12  \n 3rd Qu.: 1600   3rd Qu.:5.000   3rd Qu.:4   3rd Qu.:5.000   3rd Qu.: 85.00  \n Max.   :16000   Max.   :5.000   Max.   :4   Max.   :6.000   Max.   :283.00  \n     Weight     Mfr_Guarantee    BOVAG_Guarantee  Guarantee_Period\n Min.   :1000   Min.   :0.0000   Min.   :0.0000   Min.   : 3.000  \n 1st Qu.:1040   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.: 3.000  \n Median :1070   Median :0.0000   Median :1.0000   Median : 3.000  \n Mean   :1072   Mean   :0.4095   Mean   :0.8955   Mean   : 3.815  \n 3rd Qu.:1085   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 3.000  \n Max.   :1615   Max.   :1.0000   Max.   :1.0000   Max.   :36.000  \n      ABS            Airbag_1         Airbag_2          Airco       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8134   Mean   :0.9708   Mean   :0.7228   Mean   :0.5084  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Automatic_airco   Boardcomputer      CD_Player       Central_Lock   \n Min.   :0.00000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :0.0000   Median :1.0000  \n Mean   :0.05641   Mean   :0.2946   Mean   :0.2187   Mean   :0.5801  \n 3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.00000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Powered_Windows Power_Steering       Radio          Mistlamps    \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :1.000   Median :1.0000   Median :0.0000   Median :0.000  \n Mean   :0.562   Mean   :0.9777   Mean   :0.1462   Mean   :0.257  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n  Sport_Model     Backseat_Divider  Metallic_Rim    Radio_cassette  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :0.0000   Median :0.0000  \n Mean   :0.3001   Mean   :0.7702   Mean   :0.2047   Mean   :0.1455  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Parking_Assistant     Tow_Bar      \n Min.   :0.000000   Min.   :0.0000  \n 1st Qu.:0.000000   1st Qu.:0.0000  \n Median :0.000000   Median :0.0000  \n Mean   :0.002786   Mean   :0.2779  \n 3rd Qu.:0.000000   3rd Qu.:1.0000  \n Max.   :1.000000   Max.   :1.0000  \n\n\nIt’s essential to check for missing values in the dataset:\n\nmissing_values &lt;- sapply(data, function(x) sum(is.na(x)))\nmissing_values\n\n               Id             Model             Price         Age_08_04 \n                0                 0                 0                 0 \n        Mfg_Month          Mfg_Year                KM         Fuel_Type \n                0                 0                 0                 0 \n               HP         Met_Color             Color         Automatic \n                0                 0                 0                 0 \n               CC             Doors         Cylinders             Gears \n                0                 0                 0                 0 \n    Quarterly_Tax            Weight     Mfr_Guarantee   BOVAG_Guarantee \n                0                 0                 0                 0 \n Guarantee_Period               ABS          Airbag_1          Airbag_2 \n                0                 0                 0                 0 \n            Airco   Automatic_airco     Boardcomputer         CD_Player \n                0                 0                 0                 0 \n     Central_Lock   Powered_Windows    Power_Steering             Radio \n                0                 0                 0                 0 \n        Mistlamps       Sport_Model  Backseat_Divider      Metallic_Rim \n                0                 0                 0                 0 \n   Radio_cassette Parking_Assistant           Tow_Bar \n                0                 0                 0"
  },
  {
    "objectID": "posts/Problem Set 3/problemset3.html#introduction",
    "href": "posts/Problem Set 3/problemset3.html#introduction",
    "title": "Data Mining: Problem Set 3",
    "section": "",
    "text": "This analysis aims to explore and model the Toyota Corolla dataset to determine the influence of various factors on the price.\nThe first few rows of the dataset are:\n\n\n  Id                                         Model Price Age_08_04 Mfg_Month\n1  1 TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors 13500        23        10\n2  2 TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors 13750        23        10\n3  3 TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors 13950        24         9\n4  4 TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors 14950        26         7\n5  5   TOYOTA Corolla 2.0 D4D HATCHB SOL 2/3-Doors 13750        30         3\n6  6   TOYOTA Corolla 2.0 D4D HATCHB SOL 2/3-Doors 12950        32         1\n  Mfg_Year    KM Fuel_Type HP Met_Color  Color Automatic   CC Doors Cylinders\n1     2002 46986    Diesel 90         1   Blue         0 2000     3         4\n2     2002 72937    Diesel 90         1 Silver         0 2000     3         4\n3     2002 41711    Diesel 90         1   Blue         0 2000     3         4\n4     2002 48000    Diesel 90         0  Black         0 2000     3         4\n5     2002 38500    Diesel 90         0  Black         0 2000     3         4\n6     2002 61000    Diesel 90         0  White         0 2000     3         4\n  Gears Quarterly_Tax Weight Mfr_Guarantee BOVAG_Guarantee Guarantee_Period ABS\n1     5           210   1165             0               1                3   1\n2     5           210   1165             0               1                3   1\n3     5           210   1165             1               1                3   1\n4     5           210   1165             1               1                3   1\n5     5           210   1170             1               1                3   1\n6     5           210   1170             0               1                3   1\n  Airbag_1 Airbag_2 Airco Automatic_airco Boardcomputer CD_Player Central_Lock\n1        1        1     0               0             1         0            1\n2        1        1     1               0             1         1            1\n3        1        1     0               0             1         0            0\n4        1        1     0               0             1         0            0\n5        1        1     1               0             1         0            1\n6        1        1     1               0             1         0            1\n  Powered_Windows Power_Steering Radio Mistlamps Sport_Model Backseat_Divider\n1               1              1     0         0           0                1\n2               0              1     0         0           0                1\n3               0              1     0         0           0                1\n4               0              1     0         0           0                1\n5               1              1     0         1           0                1\n6               1              1     0         1           0                1\n  Metallic_Rim Radio_cassette Parking_Assistant Tow_Bar\n1            0              0                 0       0\n2            0              0                 0       0\n3            0              0                 0       0\n4            0              0                 0       0\n5            0              0                 0       0\n6            0              0                 0       0\n\n\nChecking the structure and summary of the dataset provides us with an overview of the type of variables and basic statistics.\n\nstr(data)\n\n'data.frame':   1436 obs. of  39 variables:\n $ Id               : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Model            : chr  \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" ...\n $ Price            : int  13500 13750 13950 14950 13750 12950 16900 18600 21500 12950 ...\n $ Age_08_04        : int  23 23 24 26 30 32 27 30 27 23 ...\n $ Mfg_Month        : int  10 10 9 7 3 1 6 3 6 10 ...\n $ Mfg_Year         : int  2002 2002 2002 2002 2002 2002 2002 2002 2002 2002 ...\n $ KM               : int  46986 72937 41711 48000 38500 61000 94612 75889 19700 71138 ...\n $ Fuel_Type        : chr  \"Diesel\" \"Diesel\" \"Diesel\" \"Diesel\" ...\n $ HP               : int  90 90 90 90 90 90 90 90 192 69 ...\n $ Met_Color        : int  1 1 1 0 0 0 1 1 0 0 ...\n $ Color            : chr  \"Blue\" \"Silver\" \"Blue\" \"Black\" ...\n $ Automatic        : int  0 0 0 0 0 0 0 0 0 0 ...\n $ CC               : int  2000 2000 2000 2000 2000 2000 2000 2000 1800 1900 ...\n $ Doors            : int  3 3 3 3 3 3 3 3 3 3 ...\n $ Cylinders        : int  4 4 4 4 4 4 4 4 4 4 ...\n $ Gears            : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Quarterly_Tax    : int  210 210 210 210 210 210 210 210 100 185 ...\n $ Weight           : int  1165 1165 1165 1165 1170 1170 1245 1245 1185 1105 ...\n $ Mfr_Guarantee    : int  0 0 1 1 1 0 0 1 0 0 ...\n $ BOVAG_Guarantee  : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Guarantee_Period : int  3 3 3 3 3 3 3 3 3 3 ...\n $ ABS              : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Airbag_1         : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Airbag_2         : int  1 1 1 1 1 1 1 1 0 1 ...\n $ Airco            : int  0 1 0 0 1 1 1 1 1 1 ...\n $ Automatic_airco  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Boardcomputer    : int  1 1 1 1 1 1 1 1 0 1 ...\n $ CD_Player        : int  0 1 0 0 0 0 0 1 0 0 ...\n $ Central_Lock     : int  1 1 0 0 1 1 1 1 1 0 ...\n $ Powered_Windows  : int  1 0 0 0 1 1 1 1 1 0 ...\n $ Power_Steering   : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Radio            : int  0 0 0 0 0 0 0 0 1 0 ...\n $ Mistlamps        : int  0 0 0 0 1 1 0 0 0 0 ...\n $ Sport_Model      : int  0 0 0 0 0 0 1 0 0 0 ...\n $ Backseat_Divider : int  1 1 1 1 1 1 1 1 0 1 ...\n $ Metallic_Rim     : int  0 0 0 0 0 0 0 0 1 0 ...\n $ Radio_cassette   : int  0 0 0 0 0 0 0 0 1 0 ...\n $ Parking_Assistant: int  0 0 0 0 0 0 0 0 0 0 ...\n $ Tow_Bar          : int  0 0 0 0 0 0 0 0 0 0 ...\n\nsummary(data)\n\n       Id            Model               Price         Age_08_04    \n Min.   :   1.0   Length:1436        Min.   : 4350   Min.   : 1.00  \n 1st Qu.: 361.8   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Median : 721.5   Mode  :character   Median : 9900   Median :61.00  \n Mean   : 721.6                      Mean   :10731   Mean   :55.95  \n 3rd Qu.:1081.2                      3rd Qu.:11950   3rd Qu.:70.00  \n Max.   :1442.0                      Max.   :32500   Max.   :80.00  \n   Mfg_Month         Mfg_Year          KM          Fuel_Type        \n Min.   : 1.000   Min.   :1998   Min.   :     1   Length:1436       \n 1st Qu.: 3.000   1st Qu.:1998   1st Qu.: 43000   Class :character  \n Median : 5.000   Median :1999   Median : 63390   Mode  :character  \n Mean   : 5.549   Mean   :2000   Mean   : 68533                     \n 3rd Qu.: 8.000   3rd Qu.:2001   3rd Qu.: 87021                     \n Max.   :12.000   Max.   :2004   Max.   :243000                     \n       HP          Met_Color         Color             Automatic      \n Min.   : 69.0   Min.   :0.0000   Length:1436        Min.   :0.00000  \n 1st Qu.: 90.0   1st Qu.:0.0000   Class :character   1st Qu.:0.00000  \n Median :110.0   Median :1.0000   Mode  :character   Median :0.00000  \n Mean   :101.5   Mean   :0.6748                      Mean   :0.05571  \n 3rd Qu.:110.0   3rd Qu.:1.0000                      3rd Qu.:0.00000  \n Max.   :192.0   Max.   :1.0000                      Max.   :1.00000  \n       CC            Doors         Cylinders     Gears       Quarterly_Tax   \n Min.   : 1300   Min.   :2.000   Min.   :4   Min.   :3.000   Min.   : 19.00  \n 1st Qu.: 1400   1st Qu.:3.000   1st Qu.:4   1st Qu.:5.000   1st Qu.: 69.00  \n Median : 1600   Median :4.000   Median :4   Median :5.000   Median : 85.00  \n Mean   : 1577   Mean   :4.033   Mean   :4   Mean   :5.026   Mean   : 87.12  \n 3rd Qu.: 1600   3rd Qu.:5.000   3rd Qu.:4   3rd Qu.:5.000   3rd Qu.: 85.00  \n Max.   :16000   Max.   :5.000   Max.   :4   Max.   :6.000   Max.   :283.00  \n     Weight     Mfr_Guarantee    BOVAG_Guarantee  Guarantee_Period\n Min.   :1000   Min.   :0.0000   Min.   :0.0000   Min.   : 3.000  \n 1st Qu.:1040   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.: 3.000  \n Median :1070   Median :0.0000   Median :1.0000   Median : 3.000  \n Mean   :1072   Mean   :0.4095   Mean   :0.8955   Mean   : 3.815  \n 3rd Qu.:1085   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 3.000  \n Max.   :1615   Max.   :1.0000   Max.   :1.0000   Max.   :36.000  \n      ABS            Airbag_1         Airbag_2          Airco       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8134   Mean   :0.9708   Mean   :0.7228   Mean   :0.5084  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Automatic_airco   Boardcomputer      CD_Player       Central_Lock   \n Min.   :0.00000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :0.0000   Median :1.0000  \n Mean   :0.05641   Mean   :0.2946   Mean   :0.2187   Mean   :0.5801  \n 3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.00000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Powered_Windows Power_Steering       Radio          Mistlamps    \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :1.000   Median :1.0000   Median :0.0000   Median :0.000  \n Mean   :0.562   Mean   :0.9777   Mean   :0.1462   Mean   :0.257  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n  Sport_Model     Backseat_Divider  Metallic_Rim    Radio_cassette  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :0.0000   Median :0.0000  \n Mean   :0.3001   Mean   :0.7702   Mean   :0.2047   Mean   :0.1455  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Parking_Assistant     Tow_Bar      \n Min.   :0.000000   Min.   :0.0000  \n 1st Qu.:0.000000   1st Qu.:0.0000  \n Median :0.000000   Median :0.0000  \n Mean   :0.002786   Mean   :0.2779  \n 3rd Qu.:0.000000   3rd Qu.:1.0000  \n Max.   :1.000000   Max.   :1.0000  \n\n\nIt’s essential to check for missing values in the dataset:\n\nmissing_values &lt;- sapply(data, function(x) sum(is.na(x)))\nmissing_values\n\n               Id             Model             Price         Age_08_04 \n                0                 0                 0                 0 \n        Mfg_Month          Mfg_Year                KM         Fuel_Type \n                0                 0                 0                 0 \n               HP         Met_Color             Color         Automatic \n                0                 0                 0                 0 \n               CC             Doors         Cylinders             Gears \n                0                 0                 0                 0 \n    Quarterly_Tax            Weight     Mfr_Guarantee   BOVAG_Guarantee \n                0                 0                 0                 0 \n Guarantee_Period               ABS          Airbag_1          Airbag_2 \n                0                 0                 0                 0 \n            Airco   Automatic_airco     Boardcomputer         CD_Player \n                0                 0                 0                 0 \n     Central_Lock   Powered_Windows    Power_Steering             Radio \n                0                 0                 0                 0 \n        Mistlamps       Sport_Model  Backseat_Divider      Metallic_Rim \n                0                 0                 0                 0 \n   Radio_cassette Parking_Assistant           Tow_Bar \n                0                 0                 0"
  },
  {
    "objectID": "posts/Problem Set 3/problemset3.html#visualizations",
    "href": "posts/Problem Set 3/problemset3.html#visualizations",
    "title": "Data Mining: Problem Set 3",
    "section": "Visualizations",
    "text": "Visualizations\nVisualizations help in understanding the data distribution and relationships between variables.\nStarting with the Price distribution, we can use a histogram:\n\nhist(data$Price, breaks = 50, main = \"Price Distribution\", xlab = \"Price\")\n\n\n\n\nQuantile-Quantile plots help in assessing if the data follows a normal distribution:\n\nqqnorm(data$Price)\nqqline(data$Price)\n\n\n\n\nScatter plots and box plots can show relationships and distribution across different variables. Here are some visualizations for Price against Age, Kilometers, and Fuel Type:\n\nggplot(data, aes(x = Age_08_04, y = Price)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Price vs Age_08_04\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(data, aes(x = KM, y = Price)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", span = 0.75) + \n  labs(title = \"Price vs KM\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(data, aes(x = Fuel_Type, y = Price)) +\n  geom_boxplot() +\n  labs(title = \"Price Distribution by Fuel Type\")\n\n\n\nggplot(data, aes(x = Age_08_04, y = Price, color = Fuel_Type)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Price vs Age, Colored by Fuel Type\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/Problem Set 3/problemset3.html#model-building",
    "href": "posts/Problem Set 3/problemset3.html#model-building",
    "title": "Data Mining: Problem Set 3",
    "section": "Model Building",
    "text": "Model Building\nTo build a predictive model, it’s necessary first to remove redundant data and then split the dataset into training and test sets.\nColumns with zero variance don’t provide any information, so they can be removed. Afterward, the data is split, ensuring randomness:\n\nnon_price_vars &lt;- setdiff(names(data), \"Price\")\nzero_var_cols &lt;- sapply(data[sapply(data, is.numeric)], function(x) var(x, na.rm = TRUE) == 0)\nzero_var_cols &lt;- names(data)[zero_var_cols]\ndata &lt;- data[, !(names(data) %in% zero_var_cols)]\n\ndata &lt;- droplevels(data)\n\nset.seed(123)\ntrain_indices &lt;- sample(seq_len(nrow(data)), 0.7 * nrow(data))\ntrain_data &lt;- data[train_indices, ]\ntest_data &lt;- data[-train_indices, ]\n\nA regression tree model is trained on the data. This model is then pruned for simplicity and better performance:\n\nfit &lt;- rpart(Price ~ ., data = train_data)\npruned_fit &lt;- rpart::prune(fit, cp = 0.1) \nrpart.plot(pruned_fit, cex = 0.7, main=\"Pruned Regression Tree\", uniform=TRUE, branch = 0.5)\n\n\n\n\nIt’s also beneficial to understand the importance of each feature:\n\npredictor &lt;- Predictor$new(pruned_fit, data = train_data, y = train_data$Price)\nimportance &lt;- FeatureImp$new(predictor, loss = \"rmse\", compare = \"ratio\")\nimportance$plot()\n\n\n\n\nBased on the feature importance, the least contributing features are removed, and the model is retrained:\n\nleast_important_features &lt;- head(importance$results[order(importance$results$importance), ]$feature, 5)\ntrain_data_simplified &lt;- train_data[, !(names(train_data) %in% least_important_features)]\ntest_data_simplified &lt;- test_data[, !(names(test_data) %in% least_important_features)]\n\nfit_simplified &lt;- rpart(Price ~ ., data = train_data_simplified)\npruned_fit_simplified &lt;- rpart::prune(fit_simplified, cp = 0.1)\nrpart.plot(pruned_fit_simplified, cex = 0.7, main=\"Pruned Regression Tree (Simplified)\", uniform=TRUE, branch = 0.5)"
  },
  {
    "objectID": "posts/Problem Set 3/problemset3.html#model-validation",
    "href": "posts/Problem Set 3/problemset3.html#model-validation",
    "title": "Data Mining: Problem Set 3",
    "section": "Model Validation",
    "text": "Model Validation\n10-fold cross-validation is conducted to validate the model’s performance:\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, search = \"grid\", savePredictions = TRUE)\nmodel_caret &lt;- caret::train(Price ~ ., data=train_data_simplified, method = \"rpart\", trControl = train_control, \n                            tuneGrid = data.frame(cp = seq(0.01, 0.2, by = 0.01)))\nprint(model_caret)\n\nCART \n\n1005 samples\n  32 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 905, 905, 905, 903, 905, 904, ... \nResampling results across tuning parameters:\n\n  cp    RMSE      Rsquared   MAE     \n  0.01  1475.406  0.8297773  1050.898\n  0.02  1583.696  0.8022558  1140.492\n  0.03  1710.434  0.7681203  1260.519\n  0.04  1693.411  0.7727436  1251.004\n  0.05  1693.411  0.7727436  1251.004\n  0.06  1693.411  0.7727436  1251.004\n  0.07  1693.411  0.7727436  1251.004\n  0.08  1693.411  0.7727436  1251.004\n  0.09  1693.411  0.7727436  1251.004\n  0.10  1693.411  0.7727436  1251.004\n  0.11  1693.411  0.7727436  1251.004\n  0.12  1918.516  0.6979187  1447.020\n  0.13  2081.320  0.6531096  1636.779\n  0.14  2081.320  0.6531096  1636.779\n  0.15  2081.320  0.6531096  1636.779\n  0.16  2081.320  0.6531096  1636.779\n  0.17  2081.320  0.6531096  1636.779\n  0.18  2081.320  0.6531096  1636.779\n  0.19  2081.320  0.6531096  1636.779\n  0.20  2081.320  0.6531096  1636.779\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.01."
  },
  {
    "objectID": "posts/Problem Set 3/problemset3.html#predictions-and-evaluation",
    "href": "posts/Problem Set 3/problemset3.html#predictions-and-evaluation",
    "title": "Data Mining: Problem Set 3",
    "section": "Predictions and Evaluation",
    "text": "Predictions and Evaluation\nUsing the model, predictions are made on the test set, and the Root Mean Square Error (RMSE) is calculated to evaluate performance:\n\n# Align the factor levels of 'Model' in test_data_simplified with train_data_simplified\ntest_data_simplified$Model &lt;- factor(test_data_simplified$Model, levels = levels(train_data_simplified$Model))\n\n# Filter out rows from test_data_simplified that have NA in the 'Model' column\ntest_data_simplified &lt;- test_data_simplified[!is.na(test_data_simplified$Model), ]\n\npredictions &lt;- predict(pruned_fit_simplified, newdata = test_data_simplified)\ntest_rmse &lt;- rmse(test_data_simplified$Price, predictions)\n\nComparing the RMSE from cross-validation and the test set:\n\ncv_rmse &lt;- sqrt(min(model_caret$results$RMSE))\ncat(\"Cross-Validation RMSE:\", cv_rmse, \"\\n\")\n\nCross-Validation RMSE: 38.41102"
  },
  {
    "objectID": "posts/Problem Set 2/data_mining_ps2.html",
    "href": "posts/Problem Set 2/data_mining_ps2.html",
    "title": " Data Mining: Problem Set 2",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, include = TRUE)\n# Load libraries\n  library(readr)\n  library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n  library(tidyr)\n  library(e1071)\n  library(ggplot2)\n  library(corrplot)\n\ncorrplot 0.92 loaded\n\n  library(caret)\n\nLoading required package: lattice\n\n  library(caretEnsemble)\n\n\nAttaching package: 'caretEnsemble'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    autoplot\nNumber of rows: 731 \n\n\nNumber of features: 10 \n\n\nspc_tbl_ [731 × 10] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ date       : Date[1:731], format: \"2011-01-01\" \"2011-01-02\" ...\n $ season     : num [1:731] 1 1 1 1 1 1 1 1 1 1 ...\n $ holiday    : num [1:731] 0 0 0 0 0 0 0 0 0 0 ...\n $ weekday    : num [1:731] 6 0 1 2 3 4 5 6 0 1 ...\n $ weather    : num [1:731] 2 2 1 1 1 1 2 2 1 1 ...\n $ temperature: num [1:731] 46.7 48.4 34.2 34.5 36.8 ...\n $ realfeel   : num [1:731] 46.4 45.2 25.7 28.4 30.4 ...\n $ humidity   : num [1:731] 0.806 0.696 0.437 0.59 0.437 ...\n $ windspeed  : num [1:731] 6.68 10.35 10.34 6.67 7.78 ...\n $ rentals    : num [1:731] 985 801 1349 1562 1600 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   date = col_date(format = \"\"),\n  ..   season = col_double(),\n  ..   holiday = col_double(),\n  ..   weekday = col_double(),\n  ..   weather = col_double(),\n  ..   temperature = col_double(),\n  ..   realfeel = col_double(),\n  ..   humidity = col_double(),\n  ..   windspeed = col_double(),\n  ..   rentals = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt;\nSkewness of Rentals: -0.04715862"
  },
  {
    "objectID": "posts/Problem Set 2/data_mining_ps2.html#discuss-whether-you-think-the-features-you-have-in-the-data-make-sense-for-learning-to-predict-daily-bike-rentals.",
    "href": "posts/Problem Set 2/data_mining_ps2.html#discuss-whether-you-think-the-features-you-have-in-the-data-make-sense-for-learning-to-predict-daily-bike-rentals.",
    "title": " Data Mining: Problem Set 2",
    "section": "Discuss whether you think the features you have in the data make sense for learning to predict daily bike rentals.",
    "text": "Discuss whether you think the features you have in the data make sense for learning to predict daily bike rentals.\nI believe the features available in the data make sense for learning to predict daily bike rentals. We are able to compare multiple important factors such as weather, day of week, is it a holiday, or what the weather is to be able to predict the amount of bikes that will be rented."
  },
  {
    "objectID": "posts/Problem Set 2/data_mining_ps2.html#discuss-what-is-means-in-this-case-to-train-or-fit-a-model-to-the-data-you-prepared.",
    "href": "posts/Problem Set 2/data_mining_ps2.html#discuss-what-is-means-in-this-case-to-train-or-fit-a-model-to-the-data-you-prepared.",
    "title": " Data Mining: Problem Set 2",
    "section": "Discuss what is means in this case to train or “fit” a model to the data you prepared.",
    "text": "Discuss what is means in this case to train or “fit” a model to the data you prepared.\nTraining a model in this case means comparing real life factors such as weather, day of week to the amount of rentals. This allows the model to learn the rental patterns depending on the day, season, or weather so that the model can predict what future rentals will look like depending on the season, day of the week, and holidays."
  },
  {
    "objectID": "posts/Problem Set 2/data_mining_ps2.html#discuss-which-preparations-you-did-were-required-to-make-the-learning-algorithm-work-and-which-were-not-strictly-required-but-maybe-are-a-good-idea.",
    "href": "posts/Problem Set 2/data_mining_ps2.html#discuss-which-preparations-you-did-were-required-to-make-the-learning-algorithm-work-and-which-were-not-strictly-required-but-maybe-are-a-good-idea.",
    "title": " Data Mining: Problem Set 2",
    "section": "Discuss which preparations you did were required to make the learning algorithm work, and which were not strictly required, but maybe are a good idea.",
    "text": "Discuss which preparations you did were required to make the learning algorithm work, and which were not strictly required, but maybe are a good idea.\nThe required preparations made to make the algorithm work were data type conversion, median imputation, normalization, dummy variables, and training the model. These preparations are required for the algorithm to give us an answer to our question. If these had not been completed, the algorithm would either not give us an answer, or give us a useless answer. The best practice preparations were data visualization, correlation analysis, outlier detection, and model evaluation. These preparations allow for us to make the algorithm work for us. Visualizing, analysizing, outlier detection, and model evaluation, allow for us to tune the algorithm which can lead to better answers."
  },
  {
    "objectID": "posts/Problem Set 2/data_mining_ps2.html#footnotes",
    "href": "posts/Problem Set 2/data_mining_ps2.html#footnotes",
    "title": " Data Mining: Problem Set 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEmail achapman03@hamline.edu. Position Student↩︎\nEmail achapman03@hamline.edu. Position Student↩︎\nEmail achapman03@hamline.edu. Position Student↩︎"
  },
  {
    "objectID": "posts/Problem Set 4/ProblemSet4.html",
    "href": "posts/Problem Set 4/ProblemSet4.html",
    "title": "Data Mining: Problem Set 4",
    "section": "",
    "text": "Classification is the right approach for the NVO’s problem because they’re trying to categorize individuals into two distinct groups: those likely to donate and those who aren’t. By training a classification model on past data, NVO can predict the likelihood of a particular individual becoming a donor. This binary outcome (donor or not) makes it a classification problem rather than a regression problem where continuous outcomes are predicted."
  },
  {
    "objectID": "posts/Problem Set 4/ProblemSet4.html#why-is-classification-the-right-approach-for-the-nvos-problem",
    "href": "posts/Problem Set 4/ProblemSet4.html#why-is-classification-the-right-approach-for-the-nvos-problem",
    "title": "Data Mining: Problem Set 4",
    "section": "",
    "text": "Classification is the right approach for the NVO’s problem because they’re trying to categorize individuals into two distinct groups: those likely to donate and those who aren’t. By training a classification model on past data, NVO can predict the likelihood of a particular individual becoming a donor. This binary outcome (donor or not) makes it a classification problem rather than a regression problem where continuous outcomes are predicted."
  },
  {
    "objectID": "posts/Problem Set 4/ProblemSet4.html#how-could-nvo-use-the-classifier-you-build-to-identify-potential-donors-why-could-it-be-better-than-what-theyve-been-doing",
    "href": "posts/Problem Set 4/ProblemSet4.html#how-could-nvo-use-the-classifier-you-build-to-identify-potential-donors-why-could-it-be-better-than-what-theyve-been-doing",
    "title": "Data Mining: Problem Set 4",
    "section": "How could NVO use the classifier you build to identify potential donors? Why could it be better than what they’ve been doing?",
    "text": "How could NVO use the classifier you build to identify potential donors? Why could it be better than what they’ve been doing?\nOnce the classifier is built, NVO can use it to predict the likelihood of potential donors from a pool of candidates. When the organization receives data on a new set of individuals, the classifier can score each person based on the likelihood of them donating. Using this predictive model can potentially be more efficient than their previous methods because it leverages data to make informed decisions, reducing the costs of reaching out to unlikely donors and optimizing the targeting of potential donors."
  },
  {
    "objectID": "posts/Problem Set 4/ProblemSet4.html#which-measures-from-the-confusion-matrix-youll-use-to-evaluate-the-classifier-performance-and-how-they-relate-to-important-areas-like-mailer-response-rate-and-maximizing-donation-opportunities",
    "href": "posts/Problem Set 4/ProblemSet4.html#which-measures-from-the-confusion-matrix-youll-use-to-evaluate-the-classifier-performance-and-how-they-relate-to-important-areas-like-mailer-response-rate-and-maximizing-donation-opportunities",
    "title": "Data Mining: Problem Set 4",
    "section": "Which measures from the confusion matrix you’ll use to evaluate the classifier performance and how they relate to important areas like mailer response rate, and maximizing donation opportunities?",
    "text": "Which measures from the confusion matrix you’ll use to evaluate the classifier performance and how they relate to important areas like mailer response rate, and maximizing donation opportunities?\nTo evaluate the classifier performance, we would look at measures like precision, recall, accuracy, and F1-score from the confusion matrix. Precision will tell us out of those predicted to be donors, how many were actual donors. Recall will tell us out of all actual donors, how many we correctly predicted. In the context of a mailer campaign, these metrics can guide decisions such as how many mailers to send out and to whom, thereby influencing the mailer response rate and maximizing donation opportunities."
  },
  {
    "objectID": "posts/Problem Set 4/ProblemSet4.html#build-a-logistic-lasso-model-using-cross-validation-on-the-training-data-to-select-the-best-.-view-the-coefficients-at-that-chosen-and-see-what-features-are-in-the-model",
    "href": "posts/Problem Set 4/ProblemSet4.html#build-a-logistic-lasso-model-using-cross-validation-on-the-training-data-to-select-the-best-.-view-the-coefficients-at-that-chosen-and-see-what-features-are-in-the-model",
    "title": "Data Mining: Problem Set 4",
    "section": "Build a logistic LASSO model using cross-validation on the training data to select the best . View the coefficients at that chosen and see what features are in the model",
    "text": "Build a logistic LASSO model using cross-validation on the training data to select the best . View the coefficients at that chosen and see what features are in the model\n\n# Load necessary libraries\nlibrary(caret)\nlibrary(glmnet)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(ROCR)\nlibrary(PRROC)\n\n# Load data\ndata &lt;- read.csv(\"donors.csv\")\n\n# Handle missing data\n# For simplicity, we'll impute with median for numeric and mode for factors\nfor (col in names(data)) {\n  if (is.numeric(data[, col])) {\n    data[is.na(data[, col]), col] &lt;- median(data[, col], na.rm = TRUE)\n  } else {\n    levels &lt;- unique(data[, col])\n    data[is.na(data[, col]), col] &lt;- levels[which.max(tabulate(match(data[, col], levels)))]\n  }\n}"
  },
  {
    "objectID": "posts/Problem Set 4/ProblemSet4.html#build-a-decision-tree-model-using-cross-validation-on-the-training-data-to-select-the-best-cp-value.-use-rpart.plot-to-view-the-decision-tree.-what-key-features-does-it-use",
    "href": "posts/Problem Set 4/ProblemSet4.html#build-a-decision-tree-model-using-cross-validation-on-the-training-data-to-select-the-best-cp-value.-use-rpart.plot-to-view-the-decision-tree.-what-key-features-does-it-use",
    "title": "Data Mining: Problem Set 4",
    "section": "Build a decision tree model using cross-validation on the training data to select the best cp value. Use rpart.plot() to view the decision tree. What key features does it use?",
    "text": "Build a decision tree model using cross-validation on the training data to select the best cp value. Use rpart.plot() to view the decision tree. What key features does it use?\n\nlibrary(caret)       # for createDataPartition\nlibrary(glmnet)      # for cv.glmnet and coef\nlibrary(rpart)       # for rpart and printcp\nlibrary(rpart.plot)  # for rpart.plot\n\n# Partition the data\nset.seed(123)\ntrain_index &lt;- createDataPartition(data$respondedMailing, p = 0.75, list = FALSE)\ntrain_data &lt;- data[train_index, ]\ntest_data &lt;- data[-train_index, ]\n\n# Train a logistic LASSO model\nx &lt;- model.matrix(respondedMailing ~ ., data = train_data)[, -1]  # Design matrix\ny &lt;- train_data$respondedMailing                                  # Response vector\ncv_lasso &lt;- cv.glmnet(x, y, family = \"binomial\")\ncat(\"LASSO Coefficients:\\n\")\n\nLASSO Coefficients:\n\nprint(coef(cv_lasso))\n\n81 x 1 sparse Matrix of class \"dgCMatrix\"\n                                     s1\n(Intercept)                -2.913942639\nage                         .          \nnumberChildren              .          \nincomeRating                .          \nwealthRating                .          \nmailOrderPurchases          .          \ntotalGivingAmount           .          \nnumberGifts                 0.006120926\nsmallestGiftAmount          .          \nlargestGiftAmount           .          \naverageGiftAmount           .          \nyearsSinceFirstDonation     .          \nmonthsSinceLastDonation    -0.005215548\ninHouseDonorTRUE            .          \nplannedGivingDonorTRUE      .          \nsweepstakesDonorTRUE        .          \nP3DonorTRUE                 .          \nstateAE                     .          \nstateAK                     .          \nstateAL                     .          \nstateAP                     .          \nstateAR                     .          \nstateAZ                     .          \nstateCA                     .          \nstateCO                     .          \nstateCT                     .          \nstateDC                     .          \nstateDE                     .          \nstateFL                     .          \nstateGA                     .          \nstateGU                     .          \nstateHI                     .          \nstateIA                     .          \nstateID                     .          \nstateIL                     .          \nstateIN                     .          \nstateKS                     .          \nstateKY                     .          \nstateLA                     .          \nstateMA                     .          \nstateMD                     .          \nstateME                     .          \nstateMI                     .          \nstateMN                     .          \nstateMO                     .          \nstateMS                     .          \nstateMT                     .          \nstateNC                     .          \nstateND                     .          \nstateNE                     .          \nstateNH                     .          \nstateNJ                     .          \nstateNM                     .          \nstateNV                     .          \nstateNY                     .          \nstateOH                     .          \nstateOK                     .          \nstateOR                     .          \nstatePA                     .          \nstateRI                     .          \nstateSC                     .          \nstateSD                     .          \nstateTN                     .          \nstateTX                     .          \nstateUT                     .          \nstateVA                     .          \nstateVI                     .          \nstateVT                     .          \nstateWA                     .          \nstateWI                     .          \nstateWV                     .          \nstateWY                     .          \nurbanicityrural             .          \nurbanicitysuburb            .          \nurbanicitytown              .          \nurbanicityurban             .          \nsocioEconomicStatushighest  .          \nsocioEconomicStatuslowest   .          \nisHomeownerTRUE             .          \ngenderjoint                 .          \ngendermale                  .          \n\n# Train a decision tree model\ncontrol &lt;- rpart.control(cp = 0, xval = 10)  # Setting up 10-fold CV\nfit_tree &lt;- rpart(respondedMailing ~ ., data = train_data, control = control)\n\n# Summarized output: Display top 10 rows of the complexity parameter table\ncat(\"Top 10 rows of Decision Tree Complexity Parameter Table:\\n\")\n\nTop 10 rows of Decision Tree Complexity Parameter Table:\n\nhead(fit_tree$cptable, n = 10)\n\n             CP nsplit rel error    xerror       xstd\n1  0.0043055945      0 1.0000000 1.0000282 0.01530006\n2  0.0008275100      1 0.9956944 0.9958987 0.01518176\n3  0.0007895379      2 0.9948669 0.9959688 0.01516422\n4  0.0006459476      3 0.9940774 0.9967859 0.01515820\n5  0.0005473264      4 0.9934314 0.9982557 0.01515519\n6  0.0005316038      5 0.9928841 0.9983005 0.01514662\n7  0.0005228979      6 0.9923525 0.9985372 0.01514783\n8  0.0004719240      7 0.9918296 0.9989994 0.01514004\n9  0.0004682807      8 0.9913577 0.9990577 0.01513738\n10 0.0004591742     11 0.9899528 0.9992902 0.01513749\n\nrpart.plot(fit_tree)\n\nWarning: labs do not fit even at cex 0.15, there may be some overplotting"
  },
  {
    "objectID": "posts/Problem Set 4/ProblemSet4.html#evaluate-the-performance-on-test-data-and-look-at-and-describe-its-performance-according-to-your-confusion-matrix-measures",
    "href": "posts/Problem Set 4/ProblemSet4.html#evaluate-the-performance-on-test-data-and-look-at-and-describe-its-performance-according-to-your-confusion-matrix-measures",
    "title": "Data Mining: Problem Set 4",
    "section": "Evaluate the performance on test data and look at and describe its performance according to your confusion matrix measures",
    "text": "Evaluate the performance on test data and look at and describe its performance according to your confusion matrix measures\n\nlibrary(caret)\n# Performance on Test Data\n# LASSO\n# Create model matrix for entire dataset\nfull_matrix &lt;- model.matrix(respondedMailing ~ ., data = data)[, -1]\n\n# Split this matrix based on previously created train_index\ntrain_matrix &lt;- full_matrix[train_index, ]\ntest_matrix &lt;- full_matrix[-train_index, ]\n\n# Train the LASSO model using train_matrix\ncv_lasso &lt;- cv.glmnet(train_matrix, y, family = \"binomial\")\n\n# Now, for predictions, use test_matrix\nprobs_lasso &lt;- predict(cv_lasso, newx = test_matrix, type = \"response\")\n\n\n# Decision Tree\n# Combine the data\ncombined_data &lt;- rbind(train_data, test_data)\n\n# Convert state to factor ensuring all levels are present\ncombined_data$state &lt;- factor(combined_data$state)\n\n# Split the data again\ntrain_data &lt;- combined_data[1:nrow(train_data), ]\ntest_data &lt;- combined_data[(nrow(train_data) + 1):nrow(combined_data), ]\n\n# Retrain the Decision Tree Model using the updated train_data\nfit_tree &lt;- rpart(respondedMailing ~ ., data = train_data, control = control)\n\n# Now try predicting again\npredictions_tree &lt;- predict(fit_tree, newdata = test_data, type = \"vector\")\n\nconfusion_tree &lt;- table(test_data$respondedMailing, predictions_tree)\nprint(confusion_tree)\n\n       predictions_tree\n            0 0.0153846153846154 0.0357142857142857 0.0526315789473684\n  FALSE 17544                 23                  7                 57\n  TRUE    922                  0                  0                  2\n       predictions_tree\n        0.0555555555555556 0.0588235294117647 0.0625 0.0666666666666667\n  FALSE                 88                 62     65                 55\n  TRUE                   2                  2      2                  6\n       predictions_tree\n        0.0714285714285714 0.0769230769230769 0.0833333333333333\n  FALSE                 94                 97                 48\n  TRUE                   4                  8                  6\n       predictions_tree\n        0.0909090909090909   0.1 0.102564102564103 0.105263157894737\n  FALSE                 64   104                14                36\n  TRUE                   1     7                 0                 1\n       predictions_tree\n        0.111111111111111 0.117647058823529 0.125 0.133333333333333\n  FALSE               223                75   216                61\n  TRUE                  9                 4    13                 3\n       predictions_tree\n        0.142857142857143 0.153846153846154 0.157894736842105 0.166666666666667\n  FALSE               524                68                70               211\n  TRUE                 36                 1                 3                 8\n       predictions_tree\n        0.176470588235294 0.181818181818182 0.1875   0.2 0.210526315789474\n  FALSE                46                99     75   127                37\n  TRUE                  1                 5      2     9                 3\n       predictions_tree\n        0.214285714285714 0.222222222222222 0.230769230769231 0.235294117647059\n  FALSE               120               173               106                56\n  TRUE                  4                11                 3                 6\n       predictions_tree\n         0.25 0.263157894736842 0.266666666666667 0.272727272727273\n  FALSE   286                24                26                76\n  TRUE     10                 2                 1                 5\n       predictions_tree\n        0.277777777777778 0.285714285714286 0.294117647058824   0.3\n  FALSE                25               238                50    58\n  TRUE                  0                10                 4     0\n       predictions_tree\n        0.307692307692308 0.3125 0.31578947368421 0.333333333333333\n  FALSE                33     50                4               200\n  TRUE                  0      5                1                15\n       predictions_tree\n        0.352941176470588 0.357142857142857 0.363636363636364 0.368421052631579\n  FALSE                20                56                55                 9\n  TRUE                  0                 2                 3                 2\n       predictions_tree\n        0.375 0.384615384615385 0.388888888888889   0.4 0.411764705882353\n  FALSE    63                32                23    37                21\n  TRUE      2                 3                 3     1                 2\n       predictions_tree\n        0.416666666666667 0.421052631578947 0.428571428571429 0.4375\n  FALSE                21                26               145     10\n  TRUE                  3                 2                 8      2\n       predictions_tree\n        0.444444444444444 0.454545454545455 0.461538461538462 0.466666666666667\n  FALSE                59                32                 7                13\n  TRUE                  4                 4                 2                 0\n       predictions_tree\n        0.470588235294118 0.473684210526316   0.5 0.533333333333333\n  FALSE                 7                 7   114                 2\n  TRUE                  0                 0    12                 2\n       predictions_tree\n        0.538461538461538 0.545454545454545 0.555555555555556 0.571428571428571\n  FALSE                 3                17                22                26\n  TRUE                  1                 1                 2                 4\n       predictions_tree\n        0.583333333333333 0.588235294117647   0.6 0.625 0.666666666666667   0.7\n  FALSE                 6                 7    15    28                12     3\n  TRUE                  1                 1     1     2                 0     0\n       predictions_tree\n        0.714285714285714  0.75 0.777777777777778   0.8\n  FALSE                11    10                 1     7\n  TRUE                  2     1                 0     0"
  },
  {
    "objectID": "posts/Problem Set 4/ProblemSet4.html#create-a-roc-plot-with-auc-to-compare-the-two-models-performance-and-explain-to-nvo-what-the-plot-tells-you",
    "href": "posts/Problem Set 4/ProblemSet4.html#create-a-roc-plot-with-auc-to-compare-the-two-models-performance-and-explain-to-nvo-what-the-plot-tells-you",
    "title": "Data Mining: Problem Set 4",
    "section": "Create a ROC plot (with AUC) to compare the two model’s performance and explain to NVO what the plot tells you",
    "text": "Create a ROC plot (with AUC) to compare the two model’s performance and explain to NVO what the plot tells you\n\n# ROC plot\n# Ensure the target variable is a factor\ntrain_data$respondedMailing &lt;- as.factor(train_data$respondedMailing)\ntest_data$respondedMailing &lt;- as.factor(test_data$respondedMailing)\n\n# Retrain the Decision Tree Model\nfit_tree &lt;- rpart(respondedMailing ~ ., data = train_data, control = control)\n\n# Predict probabilities for the LASSO model\npred_lasso &lt;- prediction(probs_lasso, test_data$respondedMailing)\nperf_lasso &lt;- performance(pred_lasso, \"tpr\", \"fpr\")\n\n# Predict probabilities for the Decision Tree model\nprobs_tree &lt;- predict(fit_tree, newdata = test_data, type = \"prob\")[, 2]\npred_tree &lt;- prediction(probs_tree, test_data$respondedMailing)\nperf_tree &lt;- performance(pred_tree, \"tpr\", \"fpr\")\n\n# Plot ROC curves\nplot(perf_lasso, col = \"red\")\nplot(perf_tree, col = \"blue\", add = TRUE)\nlegend(\"bottomright\", legend = c(\"LASSO\", \"Decision Tree\"), col = c(\"red\", \"blue\"), lty = 1)"
  },
  {
    "objectID": "posts/Problem Set 4/ProblemSet4.html#pick-the-best-performing-model-and-view-its-precision-recall-chart-and-its-cumulative-gain-chart",
    "href": "posts/Problem Set 4/ProblemSet4.html#pick-the-best-performing-model-and-view-its-precision-recall-chart-and-its-cumulative-gain-chart",
    "title": "Data Mining: Problem Set 4",
    "section": "Pick the best performing model, and view its precision recall chart and its cumulative gain chart",
    "text": "Pick the best performing model, and view its precision recall chart and its cumulative gain chart\n\n# For LASSO\npr_lasso &lt;- pr.curve(scores.class0 = probs_lasso, weights.class0 = test_data$respondedMailing == \"FALSE\")\n\n# For Decision Tree\nprobs_tree &lt;- predict(fit_tree, newdata = test_data, type = \"prob\")[, 2]\npr_tree &lt;- pr.curve(scores.class0 = probs_tree, weights.class0 = test_data$respondedMailing == \"FALSE\")\n\n# Compare the two models by AUC\ncat(\"AUC for LASSO:\", pr_lasso$auc.integral, \"\\n\")\n\nAUC for LASSO: 0.9362001 \n\ncat(\"AUC for Decision Tree:\", pr_tree$auc.integral, \"\\n\")\n\nAUC for Decision Tree: 0.9376775"
  },
  {
    "objectID": "posts/Problem Set 4/ProblemSet4.html#use-the-charts-from-parts-6-and-7-to-describe-how-the-model-should-perform-for-nvo-and-what-it-could-mean-if-they-do-a-mailer-campaign-for-50000-people",
    "href": "posts/Problem Set 4/ProblemSet4.html#use-the-charts-from-parts-6-and-7-to-describe-how-the-model-should-perform-for-nvo-and-what-it-could-mean-if-they-do-a-mailer-campaign-for-50000-people",
    "title": "Data Mining: Problem Set 4",
    "section": "Use the charts from parts 6 and 7 to describe how the model should perform for NVO and what it could mean if they do a mailer campaign for 50,000 people",
    "text": "Use the charts from parts 6 and 7 to describe how the model should perform for NVO and what it could mean if they do a mailer campaign for 50,000 people\nFor NVO’s mailer campaign targeting 50,000 people, the model’s performance will be predicting likely potential donors. Given the ROC curves, if NVO were to use the LASSO model, they might expect a slightly higher true positive rate for a given false positive rate, compared to the Decision Tree model. This means that, among the people they reach out to, a higher proportion might be actual potential donors."
  },
  {
    "objectID": "posts/Problem Set 1/data_mining_ps1.html",
    "href": "posts/Problem Set 1/data_mining_ps1.html",
    "title": " Data Mining: Problem Set 1",
    "section": "",
    "text": "The purpose of this document is to simulataneously analyze data on US crime rates and become more familiar with the syntax and abilities of R-markdown to combine code and analysis in a progressional document. Blockquotes look better in HTML typically, but you can see their general effect in any document. The text is highlighted differently in RStudio so you know its part of the block quote. Also, the margins of the text in the final document are narrower to separate the block quote from normal text."
  },
  {
    "objectID": "posts/Problem Set 1/data_mining_ps1.html#the-structure-of-the-data",
    "href": "posts/Problem Set 1/data_mining_ps1.html#the-structure-of-the-data",
    "title": " Data Mining: Problem Set 1",
    "section": "The Structure of the Data",
    "text": "The Structure of the Data\n\n\nThe USArrests dataset contains the number of arrests for assault, murder, and rape per 100,000 people across all 50 states in 1973. It also contains the percentage of people living in urban areas vs rural areas.\n\n\n\n'data.frame':   50 obs. of  4 variables:\n $ Murder  : num  13.2 10 8.1 8.8 9 7.9 3.3 5.9 15.4 17.4 ...\n $ Assault : int  236 263 294 190 276 204 110 238 335 211 ...\n $ UrbanPop: int  58 48 80 50 91 78 77 72 80 60 ...\n $ Rape    : num  21.2 44.5 31 19.5 40.6 38.7 11.1 15.8 31.9 25.8 ...\nNULL\n\n\n\n\nThe USArrests dataset contains 200 observations with 50 observations over 4 variables. The 4 variables being Murder, Assault, UrbanPop, and Rape. This allows us to measure the number of Murder, Assault, and Rape arrests compared to the Urban Population. These variables can tell us if there is a trend in the amount of arrests to the size of the population, or if specific states are outliers in certain crime types."
  },
  {
    "objectID": "posts/Problem Set 1/data_mining_ps1.html#summary-of-features",
    "href": "posts/Problem Set 1/data_mining_ps1.html#summary-of-features",
    "title": " Data Mining: Problem Set 1",
    "section": "Summary of Features",
    "text": "Summary of Features\n\n\n\n\n\n\nMurder\nAssault\nUrbanPop\nRape\n\n\n\n\n\nMin. : 0.800\nMin. : 45.0\nMin. :32.00\nMin. : 7.30\n\n\n\n1st Qu.: 4.075\n1st Qu.:109.0\n1st Qu.:54.50\n1st Qu.:15.07\n\n\n\nMedian : 7.250\nMedian :159.0\nMedian :66.00\nMedian :20.10\n\n\n\nMean : 7.788\nMean :170.8\nMean :65.54\nMean :21.23\n\n\n\n3rd Qu.:11.250\n3rd Qu.:249.0\n3rd Qu.:77.75\n3rd Qu.:26.18\n\n\n\nMax. :17.400\nMax. :337.0\nMax. :91.00\nMax. :46.00\n\n\n\n\n\n\n\nTo summarize this data, we can see that per 100,000 people, there are around 7 murder arrests, 171 assault arrests, and 21 rape arrests."
  },
  {
    "objectID": "posts/Problem Set 1/data_mining_ps1.html#relationships-between-features",
    "href": "posts/Problem Set 1/data_mining_ps1.html#relationships-between-features",
    "title": " Data Mining: Problem Set 1",
    "section": "Relationships Between Features",
    "text": "Relationships Between Features\n\n\n  scaled_data &lt;- as.data.frame(sapply(USArrests, scale))\n  ggplot(gather(scaled_data, cols, value), aes(x = value)) +\n  geom_histogram(aes(y = ..density..), bins = 10) +\n  geom_density(alpha = .2, fill = \"#FF6666\") +\n  facet_grid(. ~ cols) +\n  ggtitle(\"Feature Histograms for the Scaled US Arrests Data\")\n\n\n\n\n\n\nIn the scaled data above, we can see a small amount of skew to the left in the arrest features, and a right skew in the population feature.\n\n\n\n\n\n\nFacet Grid of Scatter Plots\n\n\n\n\n\n\nWhen looking for relationships in the scatter plots above, we can see that there is some linaer relationship between the features of the dataset. This tells us that the more of one type of arrest or higher population generally indicates that there are going to be more arrests of other types.\n\n\n\n\n\n\nVariable\nMean\n\n\n\n\nMurder\n7.788\n\n\nAssault\n170.76\n\n\nUrbanPop\n65.54\n\n\nRape\n21.232"
  },
  {
    "objectID": "posts/Problem Set 1/data_mining_ps1.html#machine-learning-questions",
    "href": "posts/Problem Set 1/data_mining_ps1.html#machine-learning-questions",
    "title": " Data Mining: Problem Set 1",
    "section": "Machine Learning Questions",
    "text": "Machine Learning Questions\nIn this section, you will type your paragraph answers to the following questions presented below. Do your best to answer the questions after reading chapter 1 of the textbook and watching the assigned videos.\n\nWhat are the 7 basic steps of machine learning?\n\nThe 7 basic steps of machine learning are Gathering Data, Data Preparation, Choosing a model, Training, Evaluation, Parameter turning, and prediciton.\n\n\n\nIn your own words, please explain the bias-variance tradeoff in supervised machine learning and make sure to include proper terminology?\n\nThe bias-variance tradeoff in supervised machine learning is give and take of overfitting and underfitting your maching learning model. Bias would be that your model is underfitting and variance would be that your model is overfitting. As you increase a model’s complexity, you fit the training data better, increase the variance, and reduce the bias of the model. As you decrease a model’s complexity, you give the algorithim more bias, decrease the variance, but also won’t fit the training data better.\n\n\n\nExplain, in your own words, why cross-validation is important and useful?\n\nCross validation is useful in Machine Learning becasue it can be used to ensure your model is well tuned. Cross validation can be used for model assessment, understanding the bias-variance tradeoff, parameter turning, and overall robustness of your model."
  },
  {
    "objectID": "posts/Problem Set 1/data_mining_ps1.html#footnotes",
    "href": "posts/Problem Set 1/data_mining_ps1.html#footnotes",
    "title": " Data Mining: Problem Set 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEmail achapman03@hamline.edu. Position Student↩︎\nEmail achapman03@hamline.edu. Position Student↩︎\nEmail achapman03@hamline.edu. Position Student↩︎"
  },
  {
    "objectID": "posts/Problem Set 7/probset7.html",
    "href": "posts/Problem Set 7/probset7.html",
    "title": "Problem Set 7",
    "section": "",
    "text": "The overarching business challenge faced by LHBA is predicting the selling price of homes in a manner that is consistent, profitable, and aligns with market realities. Historically, LHBA has relied on county-assessed property values as a benchmark for pricing homes, but these values have often deviated from actual market prices. This discrepancy has adversely affected LHBA’s operations, especially in the wake of the housing market crisis. The organization is now faced with the dilemma of whether to continue using county-assessed values, which many stakeholders are familiar with and trust, or to adopt a new, potentially more accurate, but also more complex pricing model.\n\n\n\nUnderstanding the extent of the discrepancy between county-assessed values and actual selling prices both before and after the housing market crisis. Determining whether the housing bubble was the primary reason for the variance in prices or if other factors are at play. Establishing a method that accurately predicts home selling prices post-crisis, while also being transparent and explainable to stakeholders."
  },
  {
    "objectID": "posts/Problem Set 7/probset7.html#big-picture-business-problem-and-sub-problems",
    "href": "posts/Problem Set 7/probset7.html#big-picture-business-problem-and-sub-problems",
    "title": "Problem Set 7",
    "section": "",
    "text": "The overarching business challenge faced by LHBA is predicting the selling price of homes in a manner that is consistent, profitable, and aligns with market realities. Historically, LHBA has relied on county-assessed property values as a benchmark for pricing homes, but these values have often deviated from actual market prices. This discrepancy has adversely affected LHBA’s operations, especially in the wake of the housing market crisis. The organization is now faced with the dilemma of whether to continue using county-assessed values, which many stakeholders are familiar with and trust, or to adopt a new, potentially more accurate, but also more complex pricing model.\n\n\n\nUnderstanding the extent of the discrepancy between county-assessed values and actual selling prices both before and after the housing market crisis. Determining whether the housing bubble was the primary reason for the variance in prices or if other factors are at play. Establishing a method that accurately predicts home selling prices post-crisis, while also being transparent and explainable to stakeholders."
  },
  {
    "objectID": "posts/Problem Set 7/probset7.html#high-quality-questions-for-lhba-stakeholders",
    "href": "posts/Problem Set 7/probset7.html#high-quality-questions-for-lhba-stakeholders",
    "title": "Problem Set 7",
    "section": "High Quality Questions for LHBA Stakeholders",
    "text": "High Quality Questions for LHBA Stakeholders\nWhat are the key factors you believe influence the selling price of a home apart from the county-assessed values?\nHow do you currently adjust or factor in market conditions, if at all, when considering the selling price of a home based on county-assessed values?\nWould you be open to implementing a more sophisticated model if it proves to be more accurate, even if it means a steeper learning curve for stakeholders?"
  },
  {
    "objectID": "posts/Problem Set 7/probset7.html#translate-lhbas-business-problem-into-an-analytics-problem",
    "href": "posts/Problem Set 7/probset7.html#translate-lhbas-business-problem-into-an-analytics-problem",
    "title": "Problem Set 7",
    "section": "Translate LHBA’s Business Problem into an Analytics Problem",
    "text": "Translate LHBA’s Business Problem into an Analytics Problem\n\nObjective: Predict the selling price of homes in the current market using available data.\nMeasures to Calculate: Difference between county-assessed values and actual selling prices pre- and post-crisis.\nThings to Predict: Expected selling price of homes in the current market.\nVisualization/Presentation: Graphical comparison of predicted prices vs. county-assessed values and actual past selling prices."
  },
  {
    "objectID": "posts/Problem Set 7/probset7.html#connecting-problems-to-decisions",
    "href": "posts/Problem Set 7/probset7.html#connecting-problems-to-decisions",
    "title": "Problem Set 7",
    "section": "Connecting Problems to Decisions",
    "text": "Connecting Problems to Decisions\n\nBusiness Problem: Inaccurate home pricing based on county-assessed values.\nDecision: Whether to continue using county-assessed values or adopt a new pricing model.\nValue of Improved Decision: Accurate pricing leads to better profitability, reduced unsold inventory, and improved stakeholder trust.\nAnalytics Problem: Predicting home selling prices with available data.\nDecision: Choosing the best predictive model and features.\nValue of Improved Decision: An accurate model can lead to better pricing decisions, aligning with market realities, and maximizing revenue."
  },
  {
    "objectID": "posts/Problem Set 7/probset7.html#application-of-crisp-dm",
    "href": "posts/Problem Set 7/probset7.html#application-of-crisp-dm",
    "title": "Problem Set 7",
    "section": "Application of CRISP-DM",
    "text": "Application of CRISP-DM\nThe CRISP-DM (Cross-Industry Standard Process for Data Mining) model offers a structured approach to tackle the problems faced by LHBA:\n\nBusiness Understanding: Here, we’ve identified LHBA’s need to accurately price homes in alignment with market realities.\nData Understanding: We’ll explore the datasets provided to grasp the nature of the data and understand potential features.\nData Preparation: Clean and preprocess the data, handle missing values, and possibly engineer new features.\nModeling: Use the pre-crisis and post-crisis data to train predictive models, evaluating different algorithms to find the best fit.\nEvaluation: Test the model’s predictions on the test set and assess its accuracy. Compare the model’s predictions with county-assessed values.\nDeployment: If the model proves valuable, integrate it into LHBA’s decision-making process, ensuring stakeholders understand its workings."
  },
  {
    "objectID": "posts/Problem Set 7/probset7.html#preparing-data-and-model-training",
    "href": "posts/Problem Set 7/probset7.html#preparing-data-and-model-training",
    "title": "Problem Set 7",
    "section": "Preparing Data and Model Training",
    "text": "Preparing Data and Model Training\n\nPre-Crisis Data Summary\n\n\n  Property LandValue BuildingValue Acres AboveSpace Basement Deck Baths Toilets\n1        1     15200         52400  0.26        695        0    0     1       0\n2        2     16100        123300  0.28       1054      450  180     2       0\n3        3     17900         63900  0.50        891        0  180     1       0\n4        4     25400        132400  0.26       1080      450   64     2       0\n5        5     23000         99800  0.18       1020      500    0     2       0\n6        6     25900        118100  0.29       1317        0  100     2       1\n  Fireplaces Beds Rooms AC  Age Car PoorCondition GoodCondition  Price\n1          0    2     2  1 50.8   0             0             0  74818\n2          1    3     2  1 30.8   0             0             1 137462\n3          0    2     3  0 53.8 576             0             0  56850\n4          1    3     2  1 30.9 330             0             1 137462\n5          1    3     2  1 30.7   0             0             1 131008\n6          0    3     2  1 16.7 400             0             0 139973\n\n\n\n\nPost-Crisis Data Summary\n\n\n  Property LandValue BuildingValue Acres AboveSpace Basement Deck Baths Toilets\n1     3979     18200         81300  0.17        892        0  364     1       1\n2     3980     15700         67000  0.15        692        0    0     1       0\n3     3981     18400         91500  0.53       1068      350    0     2       0\n4     3982     16600         88500  0.15       1278        0  160     1       0\n5     3983     18100         75100  0.26        939      200  192     1       0\n6     3984     25900         88900  0.30       1073        0    0     1       0\n  Fireplaces Beds Rooms AC  Age Car PoorCondition GoodCondition  Price\n1          1    2     3  1 68.9   0             1             0  54175\n2          0    2     2  1 53.8   0             0             0  92589\n3          0    3     2  1 52.8   0             0             1 122040\n4          0    4     2  1 92.9   0             0             1 112289\n5          1    3     2  1 52.9   0             0             1  95544\n6          0    3     2  1 13.9   0             0             0 115146\n\n\n\n\nTest Data Summary\n\n\n  Property LandValue BuildingValue Acres AboveSpace Basement Deck Baths Toilets\n1     3636     15500         73700 0.160        912      200    0     1       0\n2     3637     20300         69600 0.178        768        0  180     1       0\n3     3638     20100         56700 0.223        660        0    0     1       0\n4     3639      4000         28800 0.131        865        0  112     1       0\n5     3640     10400         39300 0.152        617        0    0     1       0\n6     3641      7300         41700 0.081        768        0    0     1       0\n  Fireplaces Beds Rooms AC      Age Car PoorCondition GoodCondition Price\n1          0    3     2  1 55.77808   0             0             1     0\n2          0    2     2  1 49.77260   0             0             0     0\n3          0    1     3  1 61.78082   0             0             0     0\n4          0    2     3  0 88.80000   0             0             0     0\n5          0    2     2  0 58.78082   0             0             0     0\n6          0    2     2  0 90.80274   0             0             0     0\n\n\n\n\nTrain a RandomForest regressor on pre-crisis data\n\n# Split the pre-crisis data into training and validation sets\nset.seed(42)\nsample_index_pre &lt;- sample(seq_len(nrow(pre_crisis_data)), 0.8 * nrow(pre_crisis_data))\nx_train_pre &lt;- pre_crisis_data[sample_index_pre, !(colnames(pre_crisis_data) %in% c(\"Price\", \"Property\"))]\ny_train_pre &lt;- pre_crisis_data[sample_index_pre, \"Price\"]\nx_val_pre &lt;- pre_crisis_data[-sample_index_pre, !(colnames(pre_crisis_data) %in% c(\"Price\", \"Property\"))]\ny_val_pre &lt;- pre_crisis_data[-sample_index_pre, \"Price\"]\n\n# Train a RandomForest regressor on pre-crisis data\nmodel_pre &lt;- randomForest(x_train_pre, y_train_pre, ntree=50, mtry=2, importance=TRUE)\ny_pred_pre &lt;- predict(model_pre, x_val_pre)\nmae_pre &lt;- mae(y_val_pre, y_pred_pre)\nmse_pre &lt;- mse(y_val_pre, y_pred_pre)\nr2_pre &lt;- postResample(y_pred_pre, y_val_pre)[2]\n\n\n\nPre-crisis Metrics: \n Mean Absolute Error:  13804.8 \n Mean Squared Error:  406471518 \n R Squared:  0.8607313 \n\n\nThe line geom_abline(intercept = 0, slope = 1, color = \"red\") adds a 45-degree line. If the model’s predictions were perfect, all points would fall on this line. Deviations from this line indicate prediction errors.\n\n\n\n\n\n\n\nTrain a RandomForest regressor on post-crisis data\n\n# Split the post-crisis data into training and validation sets\nsample_index_post &lt;- sample(seq_len(nrow(post_crisis_data)), 0.8 * nrow(post_crisis_data))\nx_train_post &lt;- post_crisis_data[sample_index_post, !(colnames(post_crisis_data) %in% c(\"Price\", \"Property\"))]\ny_train_post &lt;- post_crisis_data[sample_index_post, \"Price\"]\nx_val_post &lt;- post_crisis_data[-sample_index_post, !(colnames(post_crisis_data) %in% c(\"Price\", \"Property\"))]\ny_val_post &lt;- post_crisis_data[-sample_index_post, \"Price\"]\n\n# Train a RandomForest regressor on post-crisis data\nmodel_post &lt;- randomForest(x_train_post, y_train_post, ntree=50, mtry=2, importance=TRUE)\ny_pred_post &lt;- predict(model_post, x_val_post)\nmae_post &lt;- mae(y_val_post, y_pred_post)\nmse_post &lt;- mse(y_val_post, y_pred_post)\nr2_post &lt;- postResample(y_pred_post, y_val_post)[2]\n\n\n\nPost-crisis Metrics: \n Mean Absolute Error:  14026.44 \n Mean Squared Error:  374677800 \n R Squared:  0.8320167 \n\n\nThe line geom_abline(intercept = 0, slope = 1, color = \"red\") adds a 45-degree line. If the model’s predictions were perfect, all points would fall on this line. Deviations from this line indicate prediction errors.\n\n\n\n\n\n\n\nPredict on the testing data using the post-crisis model\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  27925   76378  102667  106735  126364  426766 \n\n\nThe predictions on the test data, using the post-crisis model, provide valuable insights into the potential selling prices of homes currently for sale. These insights can guide LHBA’s decision-making processes to better strategize their pricing, lending, and construction decisions to align with market trends and ensure profitability.\n\nMinimum Price: The lowest predicted selling price for a home is $27,821. This indicates the floor of the current market, suggesting the minimum price range LHBA might expect for homes in less desirable conditions or locations.\nFirst Quartile: 25% of the homes are predicted to sell for $76,776 or less. This can be seen as a benchmark for lower-priced homes in the market.\nMedian: The median predicted selling price is $102,752. This means that half of the homes are expected to sell for less than this amount and half for more, providing a central tendency for the current housing market.\nMean: The average predicted selling price is $106,188. This gives an overall expected value for the homes, which can be compared to past averages to understand market shifts.\nThird Quartile: 75% of the homes are predicted to sell for $124,978 or less. This can be viewed as a benchmark for the higher-priced homes, with the top 25% priced above this amount.\nMaximum Price: The highest predicted selling price is $390,797, reflecting the ceiling of the market. Homes priced in this range are likely to be in prime locations or possess premium features."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects are different than posts. Projects should be more expansive, impressive and generally more professional in nature compared to posts. Posts can be works in progress. Small ideas or things you did that you thought were interesting. Projects should really showcase your professional abilities. You don’t need to have too many, just make them good. And try to always have one “in the works” so that employers and collaborators can see that you’re driven.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Problem Set 10/apple_phone_reviews.html",
    "href": "Problem Set 10/apple_phone_reviews.html",
    "title": "Apple Phone Reviews",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidytext)\nlibrary(lubridate)\nlibrary(ggthemes)\nlibrary(SnowballC)\nIn this problem set, we’ll use Amazon reviews on Apple smart phones to measure some market sentiment and thoughts about several iPhone models. We have two data files, one with the reviews, and one with some product information. After merging the two files we can begin to filter them out for the products we’re interested in and than begin our analysis.\nAfter prepping the data, we will attempt to help solve a couple of business problems.\nreviews = read_csv(\"20191226-reviews.csv\") %&gt;%\n  rownames_to_column(var = \"review.ID\") %&gt;%\n  select(-name, -verified, -title)\nitems = read_csv(\"20191226-items.csv\") %&gt;%\n  select(asin, brand, title)\nA great deal of data cleaning and preparation is required for good text analysis. We first merge the data sets by the product ID “asin” which attaches product name and features to each review in the data. Then we begin to filter out for only apple branded products and then finally those with “iPhone” in the product name. Looking that the body feature we have the text of each review. It is clear that a great variety of product name and formatting exists. However, there are some common patterns we can leverage using regular expressions.\napple_reviews = left_join(reviews, items, by = \"asin\") %&gt;%\n  rename(product = title) %&gt;%\n  filter(brand == \"Apple\") %&gt;%\n  filter(str_detect(product, \"iPhone\")==TRUE) %&gt;%\n  mutate(p2 = str_replace(product, \"Apple \", \"\")) %&gt;%\n  mutate(p2 = str_extract(p2, regex(\"^[^,]*\"))) %&gt;%\n  select(-product) %&gt;%\n  rename(product = p2) %&gt;%\n  mutate(date = lubridate::mdy(date)) %&gt;%\n  mutate(year = lubridate::year(date),\n         month = lubridate::month(date)) %&gt;%\n  mutate(ym = paste(year, month, sep=\"-\")) %&gt;%\n  mutate(ym = lubridate::ym(ym))\n  \n\napple_reviews %&gt;%\n  count(product) \n\n# A tibble: 25 × 2\n   product                                                                     n\n   &lt;chr&gt;                                                                   &lt;int&gt;\n 1 Verizon Prepaid - iPhone 6S (32GB) - Space Gray                             2\n 2 iPhone 11                                                                   1\n 3 iPhone 11 Pro                                                               1\n 4 iPhone 6S                                                                1204\n 5 iPhone 6S Plus                                                            245\n 6 iPhone 7                                                                  449\n 7 iPhone 7 256GB Unlocked GSM 4G LTE Quad-Core Smartphone - Jet Black (R…     7\n 8 iPhone 7 256GB Unlocked GSM 4G LTE Quad-Core Smartphone - Rose Gold (R…     2\n 9 iPhone 7 32GB                                                             370\n10 iPhone 7 Plus                                                             109\n# ℹ 15 more rows\nAbove we can see the list of iPhone products and realize that a lot of cleaning still remains to be donde. 1. Remove storage size references, like any string of numbers starting after a space and followed by “GB”. 2. Remove leading text like “Verizon Prepaid”… 3. Remove other text which follows a size reference (everything after GB). 4. Then remove other things occurring rarely, but which are problematic.\napple_reviews = apple_reviews %&gt;%\n  mutate(product = str_replace(product, \" [0-9]*GB(.+)\", \"\")) %&gt;%\n  mutate(product = str_replace(product, \"[0-9]*GB$\", \"\")) %&gt;%\n  mutate(product = str_replace(product, \"^Verizon Prepaid - \", \"\")) %&gt;%\n  mutate(product = str_replace(product, \"\\\\([0-9]*GB(.+)\", \"\")) %&gt;%\n  mutate(product = str_replace(product, \"a1905\", \"\")) %&gt;%\n  mutate(product = str_trim(str_to_upper(product)))\n\napple_reviews %&gt;%\n  count(product, sort = TRUE)\n\n# A tibble: 13 × 2\n   product            n\n   &lt;chr&gt;          &lt;int&gt;\n 1 IPHONE 6S       1206\n 2 IPHONE X        1194\n 3 IPHONE 7         828\n 4 IPHONE 7 PLUS    479\n 5 IPHONE XS MAX    333\n 6 IPHONE XS        262\n 7 IPHONE 6S PLUS   245\n 8 IPHONE 8         188\n 9 IPHONE 8 PLUS    140\n10 IPHONE SE         63\n11 IPHONE XR         10\n12 IPHONE 11          1\n13 IPHONE 11 PRO      1\nThe scrubbed product list is now shown above. The product names are much cleaner and follow a similar naming convention. We’re now ready to start analyzing the review text.\nWe start by filtering so that we retain only reviews about a select batch of iPhone models. It is these models we wish to assess the market thoughts and sentiment.\napple_reviews = apple_reviews %&gt;%\n  filter(product %in% c(\"IPHONE 6S\", \"IPHONE 6S PLUS\", \"IPHONE 8\", \n                        \"IPHONE 8 PLUS\", \"IPHONE 7\",\n                        \"IPHONE 7 PLUS\", \"IPHONE X\"))\n\napple_reviews %&gt;%\n  count(product, sort = TRUE)\n\n# A tibble: 7 × 2\n  product            n\n  &lt;chr&gt;          &lt;int&gt;\n1 IPHONE 6S       1206\n2 IPHONE X        1194\n3 IPHONE 7         828\n4 IPHONE 7 PLUS    479\n5 IPHONE 6S PLUS   245\n6 IPHONE 8         188\n7 IPHONE 8 PLUS    140"
  },
  {
    "objectID": "Problem Set 10/apple_phone_reviews.html#view-important-words-for-each-product",
    "href": "Problem Set 10/apple_phone_reviews.html#view-important-words-for-each-product",
    "title": "Apple Phone Reviews",
    "section": "2. View Important Words for Each Product",
    "text": "2. View Important Words for Each Product\nWe will use TF-IDF measures to identify key words for a review.\n\nPart A: Calculate top TF-IDF Words for each Review\nWe will set individual reviews as our “document” level, so that term frequency is with respect to a review and inverse-document frequency across all reviews. Note that each review is uniquely identified by review.ID.\n\ntfidf = review.words %&gt;%\n  count(review.ID, word, sort = TRUE) %&gt;%\n  bind_tf_idf(word, review.ID, n)\narrange(tfidf, desc(tf_idf))\n\n# A tibble: 58,279 × 6\n   review.ID word           n    tf   idf tf_idf\n   &lt;chr&gt;     &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 42033     supposed       1 1      4.77   4.77\n 2 48836     supposed       1 1      4.77   4.77\n 3 32134     adaptor        1 0.5    7.41   3.71\n 4 41822     matters        1 0.5    7.00   3.50\n 5 48625     matters        1 0.5    7.00   3.50\n 6 31822     dislike        1 0.5    6.72   3.36\n 7 31897     met            1 0.5    6.16   3.08\n 8 25033     asked          1 0.5    5.62   2.81\n 9 37706     conditions     1 0.5    5.46   2.73\n10 31107     breathe        1 0.333  8.10   2.70\n# ℹ 58,269 more rows\n\n\nAbove we calculated the TF-IDF score of each word in each review. This score says something about the importance of each word in the review it was from. We will now characterize each review by the top 5 words by TF-IDF and keep only those words.\n\nreviews.tfidf = tfidf %&gt;%\n  left_join(apple_reviews, by = \"review.ID\") %&gt;%\n  select(review.ID, product, word, tf_idf, rating) %&gt;%\n  group_by(review.ID) %&gt;%\n  slice_max(order_by = tf_idf, n = 10, with_ties = TRUE) %&gt;%\n  ungroup()\n\nreviews.tfidf\n\n# A tibble: 28,242 × 5\n   review.ID product        word      tf_idf rating\n   &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 20539     IPHONE 6S PLUS noticable  1.24       5\n 2 20539     IPHONE 6S PLUS barely     0.821      5\n 3 20539     IPHONE 6S PLUS minor      0.705      5\n 4 20539     IPHONE 6S PLUS scratches  0.380      5\n 5 20539     IPHONE 6S PLUS screen     0.327      5\n 6 20539     IPHONE 6S PLUS great      0.295      5\n 7 20540     IPHONE 6S PLUS ti         1.62       1\n 8 20540     IPHONE 6S PLUS hace       1.30       1\n 9 20540     IPHONE 6S PLUS doesn’t    0.679      1\n10 20540     IPHONE 6S PLUS return     0.623      1\n# ℹ 28,232 more rows\n\n\n\ntopwords = reviews.tfidf %&gt;%\n  #filter(rating &lt; 4) %&gt;%\n  count(product, word)\n\n\n\n\n\n\n\ntopwords.tfidf = reviews.tfidf %&gt;%\n  count(product, word, sort = TRUE) %&gt;%\n  bind_tf_idf(word, product, n)\ntopwords.tfidf\n\n# A tibble: 11,173 × 6\n   product   word        n      tf   idf tf_idf\n   &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 IPHONE 6S battery   113 0.0136      0      0\n 2 IPHONE 6S great      90 0.0108      0      0\n 3 IPHONE 6S works      68 0.00816     0      0\n 4 IPHONE 6S good       67 0.00804     0      0\n 5 IPHONE X  new        58 0.00743     0      0\n 6 IPHONE 6S like       57 0.00684     0      0\n 7 IPHONE 6S new        54 0.00648     0      0\n 8 IPHONE X  like       53 0.00679     0      0\n 9 IPHONE X  good       51 0.00654     0      0\n10 IPHONE X  screen     51 0.00654     0      0\n# ℹ 11,163 more rows\n\n\n\n\n\n\n\n\nproduct.tfidf = review.words %&gt;%\n  count(product, word, sort = TRUE) %&gt;%\n  bind_tf_idf(word, product, n)\nproduct.tfidf\n\n# A tibble: 13,338 × 6\n   product   word        n      tf   idf tf_idf\n   &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 IPHONE 6S battery   504 0.0270      0      0\n 2 IPHONE X  new       301 0.0158      0      0\n 3 IPHONE 6S new       267 0.0143      0      0\n 4 IPHONE X  screen    250 0.0131      0      0\n 5 IPHONE 6S great     217 0.0116      0      0\n 6 IPHONE X  battery   204 0.0107      0      0\n 7 IPHONE X  like      202 0.0106      0      0\n 8 IPHONE 6S good      197 0.0105      0      0\n 9 IPHONE X  came      186 0.00974     0      0\n10 IPHONE 6S like      183 0.00980     0      0\n# ℹ 13,328 more rows"
  },
  {
    "objectID": "Problem Set 10/apple_phone_reviews.html#footnotes",
    "href": "Problem Set 10/apple_phone_reviews.html#footnotes",
    "title": "Apple Phone Reviews",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n^,↩︎"
  },
  {
    "objectID": "posts/Project Phase 2/p2.html",
    "href": "posts/Project Phase 2/p2.html",
    "title": "2020 Overwatch League Analysis",
    "section": "",
    "text": "Overwatch is a team team-based shooter game with many heros that each come with their own abilities. In 2020, Overwatch League’s gameplay involved teams of six engaging in 6v6 combat across various maps and objectives.\nThe game categorizes characters into three roles:\n\nTanks: These characters are the front line of every team. Tanks make space, absorb incoming damage, and set up plays for their teammates.\nDamage: These characters are the follow up to the front line. They are either launching rockets from up behind cover or are deep behind enemy lines taking out their support.\nSupport: These characters usually take a back seat when it comes to eliminating the enemy team, by healing their team they ensure that their front-liners can take whatever comes at them, without getting taken out."
  },
  {
    "objectID": "posts/Project Phase 2/p2.html#introduction-to-overwatch-2020",
    "href": "posts/Project Phase 2/p2.html#introduction-to-overwatch-2020",
    "title": "2020 Overwatch League Analysis",
    "section": "",
    "text": "Overwatch is a team team-based shooter game with many heros that each come with their own abilities. In 2020, Overwatch League’s gameplay involved teams of six engaging in 6v6 combat across various maps and objectives.\nThe game categorizes characters into three roles:\n\nTanks: These characters are the front line of every team. Tanks make space, absorb incoming damage, and set up plays for their teammates.\nDamage: These characters are the follow up to the front line. They are either launching rockets from up behind cover or are deep behind enemy lines taking out their support.\nSupport: These characters usually take a back seat when it comes to eliminating the enemy team, by healing their team they ensure that their front-liners can take whatever comes at them, without getting taken out."
  },
  {
    "objectID": "posts/Project Phase 2/p2.html#overwatch-league-2020-season-analysis",
    "href": "posts/Project Phase 2/p2.html#overwatch-league-2020-season-analysis",
    "title": "2020 Overwatch League Analysis",
    "section": "Overwatch League: 2020 Season Analysis",
    "text": "Overwatch League: 2020 Season Analysis\nThe analysis focuses on the 2020 Overwatch League season, played on Overwatch’s original format with six-player teams. This season is notable for its strategic and player dynamics, providing insight into player performances and tactics before Overwatch transitioned to a 5v5 format in its sequel, Overwatch 2.\nThe shift to 5v5 in Overwatch 2, particularly impacting the role and balance of Tank characters, offers an interesting contrast to the 2020 season data.\n\nPlayer Representation\nThe map below represents player nationality of which most players are from South Korea. We can see that most players are either from East Asian countries or Western Countries.\n\n\nCode\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\n\n# Load the provided data\nfile_path = 'overwatch_league_player_nationalities_updated.csv'  # Replace with your file path\nplayers_data = pd.read_csv(file_path)\n\n# Check if all values in 'Representation' are integers and convert accordingly\nif (players_data['Representation'] % 1 == 0).all():\n    players_data['Representation'] = players_data['Representation'].astype(int)\nelse:\n    players_data['Representation'] = players_data['Representation'].astype(float)\n\n# Load world data\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n# Merge the world data with the players data\nworld_data = world.merge(players_data, left_on='name', right_on='Country / Region', how='left')\n\n# Create the Plotly choropleth map\nfig = px.choropleth(world_data,\n                    geojson=world_data.geometry,\n                    locations=world_data.index,\n                    color=\"Representation\",\n                    color_continuous_scale=\"Viridis\",\n                    labels={'Representation':'Number of Players'})\n\nfig.update_geos(fitbounds=\"locations\")"
  },
  {
    "objectID": "posts/Project Phase 2/p2.html#player-performance-analysis",
    "href": "posts/Project Phase 2/p2.html#player-performance-analysis",
    "title": "2020 Overwatch League Analysis",
    "section": "Player Performance Analysis",
    "text": "Player Performance Analysis\nMy initial thought to analyze performance of players and heros is to examine their average eliminations per game. This will give us a good idea of who can best secure eliminations which are an important part of winnning not only team fights, but maps as a whole.\n\nTop Players by Eliminations\nTo identify the top players based on average eliminations per game, we have used a bar graph showing the top 20 players alongside the number of games played and sorted by color so we know if one role is better at achieving higher average eliminations than other roles. Including the number of games played can highlight outliers such as ChipSa who ranks number 3 in the league but only played one game all season.\n\n\nCode\nnum_games_per_player &lt;- eliminations_data %&gt;%\n    group_by(player_name) %&gt;%\n    summarise(num_games = n_distinct(esports_match_id), .groups = \"drop\")\n\nplayer_impact &lt;- merge(player_impact, num_games_per_player, by = \"player_name\")\n\ntop_20_players &lt;- player_impact %&gt;%\n    arrange(desc(average_eliminations)) %&gt;%\n    slice_head(n = 20)\n\ntop_20_players$dummy_legend &lt;- \"Number of Games\"\n\nggplot(top_20_players, aes(x = reorder(player_name, average_eliminations), y = average_eliminations, fill = common_role)) +\n    geom_bar(stat = \"identity\") +\n    geom_text(aes(label = num_games, color = dummy_legend), hjust = -0.3, vjust = 0.5, position = position_dodge(width = 0.9)) +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    scale_color_manual(values = \"black\", name = \"\", labels = \"Number of Games\") +\n    coord_flip() +\n    theme_minimal() +\n    labs(x = \"Player Name\", y = \"Average Eliminations Per Game\", fill = \"Player Role\", title = \"Top 20 Players by Average Eliminations Per Game\") +\n    theme(legend.position = \"bottom\", legend.title.align = 0.5)\n\n\n\n\n\n\n\nTop Players in each Role Compared\nAgain we use a bar graph to identify the top players based on elimination average but this time we are separating them into the top 5 based on role. This gives us a much clearer understanding compared with the previous graph that tanks generally have higher average eliminations while supports have the lowest.\n\n\nCode\n# Calculate the number of games for each player\nnum_games_per_player &lt;- eliminations_data %&gt;%\n    group_by(player_name) %&gt;%\n    summarise(num_games = n_distinct(esports_match_id), .groups = \"drop\")\n\n# Get the top 5 players within each role separately\ntop_players_by_role &lt;- eliminations_data %&gt;%\n    group_by(role, player_name) %&gt;%\n    summarise(average_eliminations = mean(stat_amount, na.rm = TRUE), .groups = \"drop\") %&gt;%\n    group_by(role) %&gt;% # Ensure the slicing is done within each role\n    slice_max(order_by = average_eliminations, n = 5, with_ties = FALSE) %&gt;%\n    ungroup() %&gt;%\n    arrange(role, desc(average_eliminations))\n\n# Merge this information with the number of games\ntop_players_by_role &lt;- merge(top_players_by_role, num_games_per_player, by = \"player_name\")\n\n# Create a dummy variable for the legend (for the number of games)\ntop_players_by_role$dummy_legend &lt;- \"Number of Games\"\n\nggplot(top_players_by_role, aes(x = reorder(player_name, average_eliminations), y = average_eliminations, fill = role)) +\n    geom_bar(stat = \"identity\", position = position_dodge(width = 0.7)) +\n    geom_text(aes(label = num_games, color = dummy_legend), hjust = -0.3, vjust = 0.5, position = position_dodge(width = 0.9)) +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    scale_color_manual(values = \"black\", name = \"\", labels = \"Number of Games\") +\n    coord_flip() +\n    labs(title = \"Top 5 Players by Average Eliminations within Each Role\", x = \"Player\", y = \"Average Eliminations\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\", legend.title.align = 0.5)\n\n\n\n\n\n\n\nTop Players in each Role Comparing all Roles played\nThis graph allows us to see which roles the players from the previous chart played best on in terms of average eliminations. This also gives us a better understanding of how these top players performed over a longer time. by showing their extremes as well as their first and third quartiles.\n\n\nCode\n# Identifying top 5 players in each role based on average eliminations\ntop_players_by_role &lt;- eliminations_data %&gt;%\n    group_by(player_name, role) %&gt;%\n    summarise(average_eliminations = mean(stat_amount, na.rm = TRUE), .groups = \"drop\") %&gt;%\n    group_by(role) %&gt;%\n    slice_max(order_by = average_eliminations, n = 5, with_ties = FALSE) %&gt;%\n    ungroup() %&gt;%\n    select(player_name)\n\n# Subsetting the eliminations_data to include only these top players\ntop_eliminations_data &lt;- eliminations_data %&gt;%\n    semi_join(top_players_by_role, by = \"player_name\")\n\n# Creating the boxplot with the subsetted data\nggplot(top_eliminations_data, aes(x = player_name, y = stat_amount, fill = role)) +\n    geom_boxplot(outlier.shape = NA) + # Optional: Hide outliers\n    coord_flip() +\n    labs(title = \"Boxplot of Eliminations by Top 5 Players in Each Role\", x = \"Player\", y = \"Number of Eliminations\") +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    theme_minimal() +\n    theme(legend.position = \"bottom\", axis.text.y = element_text(size = 7))"
  },
  {
    "objectID": "posts/Project Phase 2/p2.html#hero-performace-analysis",
    "href": "posts/Project Phase 2/p2.html#hero-performace-analysis",
    "title": "2020 Overwatch League Analysis",
    "section": "Hero Performace Analysis",
    "text": "Hero Performace Analysis\n\nHero Elimination Impact\nThe objective of this graph is to understand which heros stand out when it comes to average eliminations. We can see that Damage and Tank heros generally trend towards the higher end of the graph while Supports who are mostly focused on healing and keeping their team alive have less focus spent on getting eliminations. Moira stands out as a support in the number 3 spot which can be explained by her kit which requires Moira to deal damage to regenerate her healing capibilities.\n\n\nCode\navg_elim_by_hero &lt;- eliminations_data %&gt;%\n    group_by(hero_name, role) %&gt;% # Assuming there's a 'role' column\n    summarise(average_eliminations = mean(stat_amount, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;% # Remove grouping\n    arrange(desc(average_eliminations))\n\nggplot(avg_elim_by_hero, aes(x = reorder(hero_name, average_eliminations), y = average_eliminations, fill = role)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    coord_flip() +\n    labs(title = \"Average Eliminations by Hero\", x = \"Hero\", y = \"Average Eliminations\", fill = \"Role\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nElimination Distribution\nThis graph shows us the distribution of eliminations from game to game which highlights the differences between the Tank, Damage, and Support roles.\n\n\nCode\nggplot(eliminations_data, aes(x = stat_amount, fill = role)) +\n    geom_histogram(bins = 30, alpha = 0.7, position = \"dodge\") +\n    labs(title = \"Elimination Count Distribution by Role\", x = \"Number of Eliminations\", y = \"Frequency\") +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    theme_minimal()\n\n\n\n\n\n\n\nHero Performance by Elimination Boxplot\nThese box plots allow us to visualize hero performance based on eliminations per game so we can see which hero’s perform better, often. This also allows us to see which outliers are effecting the averages. We can also see some hero’s that are not best evaluated by eliminations such as Mercy who is a primarily healing focused support hero.\n\n\nCode\nggplot(eliminations_data, aes(x = hero_name, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    coord_flip() +\n    labs(title = \"Boxplot of Eliminations by Hero\", x = \"Hero\", y = \"Number of Eliminations\") +\n    theme_minimal()+\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support))\n\n\n\n\n\n\n\nRole Performance Boxplot\nThis box plot will allow us to visualize which role is having the most elimination impact over the entire league. We can see that there are some outlying games but overall the Damage role’s top 50% is much larger than the others however Tank covers a little bit of a wider range.\nWe can also see that Tanks have a higher average than both Support and Damage.\n\n\nCode\nggplot(eliminations_data, aes(x = role, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    labs(title = \"Boxplot of Eliminations by Role\", x = \"Role\", y = \"Number of Eliminations\") +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    theme_minimal()"
  },
  {
    "objectID": "posts/Project Phase 2/p2.html#general-analysises",
    "href": "posts/Project Phase 2/p2.html#general-analysises",
    "title": "2020 Overwatch League Analysis",
    "section": "General Analysises",
    "text": "General Analysises\n\nMap-Specific Hero Picks\nAnother very important aspect of the game is which map you are playing on. Teams who lose the previous map get to pick the next map to play on. This heat map allows us to visualize which heros are most played on which maps.\n\n\nCode\nhero_picks_count &lt;- data %&gt;%\n    group_by(esports_match_id, map_name, hero_name) %&gt;%\n    summarise(count = n(), .groups = \"drop\") %&gt;%\n    ungroup()\n\nhero_picks_aggregated &lt;- hero_picks_count %&gt;%\n    group_by(map_name, hero_name) %&gt;%\n    summarise(total_count = sum(count), .groups = \"drop\")\n\nhero_picks_wide &lt;- hero_picks_aggregated %&gt;%\n    pivot_wider(names_from = map_name, values_from = total_count, values_fill = list(total_count = 0))\n\nhero_picks_long &lt;- hero_picks_wide %&gt;%\n    gather(key = \"map_name\", value = \"pick_count\", -hero_name)\n\nggplot(hero_picks_long, aes(x = map_name, y = hero_name, fill = pick_count)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient(low = \"white\", high = \"steelblue\") +\n    labs(title = \"Hero Picks on Different Maps\", x = \"Map\", y = \"Hero\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nWe can see that Lucio, baptiste, Reinhardt, Tracer, Zenyatta, and Sigma were played on most maps. We can also see that Zarya, Roadhog, Junkrat and Bastion were not picked that much.\nThis heat map also allows so to see that Havana, Horizon Lunar Colony, and Numbani were not picked as maps to play on as much as the others."
  },
  {
    "objectID": "posts/Project Phase 2/p2.html#machine-learning",
    "href": "posts/Project Phase 2/p2.html#machine-learning",
    "title": "2020 Overwatch League Analysis",
    "section": "Machine Learning",
    "text": "Machine Learning\nLooking at all the previous information and seeing that support heros like lucio were played all the time, leads us to ask the question: “Why?” Lucio appeared so low on the hero analysis earlier but he was one of the most popular picks even though he didn’t have much elimination impact.\nTo answer this question, we have trained multiple Machine Learning Models to determine which features are going to be the best to look at when determining Hero and Player performance.\n\nPython\n\n\nCode\n# Import necessary libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Load the dataset\ndata = pd.read_csv(\"merged_esports_data_updated.csv\")\n\n# Preprocess the data (fill missing values, one-hot encoding)\ndata = data.fillna(data.median(numeric_only=True))\ndata = pd.get_dummies(data, columns=[\"map_type\", \"map_name\", \"player_name\", \"team_name\", \"hero_name\"])\n\n# Drop irrelevant columns\ndata = data.drop(columns=[\"team_one_name\", \"team_two_name\", \"match_id\"])\n\n# Split the data into features and target\nX = data.drop(\"match_winner\", axis=1)\ny = data[\"match_winner\"]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train models\nlr_model = LogisticRegression().fit(X_train, y_train)\nrf_model = RandomForestClassifier(n_estimators=50, random_state=42).fit(X_train, y_train)\ngb_model = GradientBoostingClassifier(n_estimators=100, random_state=42).fit(X_train, y_train)\n\n# Predictions and Evaluation\npredictions_lr = lr_model.predict(X_test)\npredictions_rf = rf_model.predict(X_test)\npredictions_gb = gb_model.predict(X_test)\n\n# Save predictions and true values for R\nresults = pd.DataFrame({\n    \"y_test\": y_test,\n    \"predictions_lr\": predictions_lr,\n    \"predictions_rf\": predictions_rf,\n    \"predictions_gb\": predictions_gb\n})\nresults.to_csv(\"model_results.csv\", index=False)\n\n# Save feature importances for R\nfeature_importances = pd.DataFrame({\n    \"feature\": X.columns,\n    \"importance_rf\": rf_model.feature_importances_,\n    \"importance_gb\": gb_model.feature_importances_\n})\nfeature_importances.to_csv(\"feature_importances.csv\", index=False)\n\n\n\n\nR\n\n\nCode\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(reshape2)\n\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\nCode\n# Read the model results and feature importances\nresults &lt;- read.csv(\"model_results.csv\")\nfeature_importances &lt;- read.csv(\"feature_importances.csv\")\n\n\n# Selecting the top 10 important features for each model\ntop_features_rf &lt;- feature_importances %&gt;%\n                   arrange(desc(importance_rf)) %&gt;%\n                   head(10)\n\ntop_features_gb &lt;- feature_importances %&gt;%\n                   arrange(desc(importance_gb)) %&gt;%\n                   head(10)\n\n# Confusion matrices and performance metrics\n# Logistic Regression\ncm_lr &lt;- confusionMatrix(as.factor(results$predictions_lr), as.factor(results$y_test))\nmetrics_lr &lt;- data.frame(Model = \"Logistic Regression\", Accuracy = cm_lr$overall['Accuracy'], Sensitivity = cm_lr$byClass['Sensitivity'], Specificity = cm_lr$byClass['Specificity'])\n\n# Random Forest\ncm_rf &lt;- confusionMatrix(as.factor(results$predictions_rf), as.factor(results$y_test))\nmetrics_rf &lt;- data.frame(Model = \"Random Forest\", Accuracy = cm_rf$overall['Accuracy'], Sensitivity = cm_rf$byClass['Sensitivity'], Specificity = cm_rf$byClass['Specificity'])\n\n# Gradient Boosting\ncm_gb &lt;- confusionMatrix(as.factor(results$predictions_gb), as.factor(results$y_test))\nmetrics_gb &lt;- data.frame(Model = \"Gradient Boosting\", Accuracy = cm_gb$overall['Accuracy'], Sensitivity = cm_gb$byClass['Sensitivity'], Specificity = cm_gb$byClass['Specificity'])\n\n# Combine metrics and melt for ggplot\ncombined_metrics &lt;- rbind(metrics_lr, metrics_rf, metrics_gb)\nmelted_metrics &lt;- melt(combined_metrics, id.vars = \"Model\")\n\n# Bar plot of accuracy, sensitivity, and specificity for each model\nggplot(melted_metrics, aes(x = variable, y = value, fill = variable)) +\n    geom_bar(stat = \"identity\", position = position_dodge()) +\n    facet_wrap(~Model) +\n    scale_fill_manual(values = c(\"Accuracy\" = tank, \"Sensitivity\" = damage, \"Specificity\" = support)) +\n    labs(x = \"Metric\", y = \"Value\", title = \"Model Performance Comparison\") +\n    theme_minimal()\n\n\n\n\n\nFrom analyzing the feature importance, accuracy, sensitivity, and specificity of each model, we can see that the Random Forest model does the best job at correctly predicting which team is going to win.\n\n\nCode\n# Random Forest\nggplot(top_features_rf, aes(x = reorder(feature, importance_rf), y = importance_rf)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip() +\n    labs(title = \"Top 10 Feature Importances - Random Forest\")\n\n\n\n\n\nThe Random Forest Model uses the in game statistics to determine which team is going to win, while the Gradient Boosting model mostly just picks it’s favorite team. Logistic Regression couldn’t identify any true negatives, and gradient boosting did not do that great either. So for the highest accuracy, and the most realistic true positive and best true negative predicting, Random Forest stands miles ahead of the other models trained.\nSince the Random Forest machine learning model decided that average_time_alive was the best feature to determine which team was going to win, I want to create a box plot showing which heros were able to survive longer on average.\n\n\nCode\ntime_alive_data &lt;- filter(data, stat_name == \"Average Time Alive\")\n\n\nggplot(time_alive_data, aes(x = hero_name, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    coord_flip() +\n    labs(title = \"Boxplot of Average Time Alive by Hero\", x = \"Hero\", y = \"Average Time Alive\") +\n    theme_minimal()+\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support))\n\n\n\n\n\n\n\nCode\navg_time_alive_by_hero &lt;- time_alive_data %&gt;%\n    group_by(hero_name, role) %&gt;% # Assuming there's a 'role' column\n    summarise(average_time_alive = mean(stat_amount, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;% # Remove grouping\n    arrange(desc(average_time_alive))\n\n\n`summarise()` has grouped output by 'hero_name'. You can override using the\n`.groups` argument.\n\n\nCode\nggplot(avg_time_alive_by_hero, aes(x = reorder(hero_name, average_time_alive), y = average_time_alive, fill = role)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    coord_flip() +\n    labs(title = \"Average Time Alive by Hero\", x = \"Hero\", y = \"Average Time Alive\", fill = \"Role\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\nThese graphs explain some of the other picks such as Mercy and Sombra, and also reinforce D.Va as one of our top picks but lets look at the objective-time and healing_done features to get a better picture.\n\n\nCode\nobjective_time_data &lt;- filter(data, stat_name == \"Objective Time\")\nggplot(objective_time_data, aes(x = hero_name, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    coord_flip() +\n    labs(title = \"Boxplot of Objective Time by Hero\", x = \"Hero\", y = \"Objective Time\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support))\n\n\n\n\n\nCode\navg_objective_time_by_hero &lt;- objective_time_data %&gt;%\n    group_by(hero_name, role) %&gt;%\n    summarise(average_objective_time = mean(stat_amount, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;%\n    arrange(desc(average_objective_time))\n\n\n`summarise()` has grouped output by 'hero_name'. You can override using the\n`.groups` argument.\n\n\nCode\nggplot(avg_objective_time_by_hero, aes(x = reorder(hero_name, average_objective_time), y = average_objective_time, fill = role)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    coord_flip() +\n    labs(title = \"Average Objective Time by Hero\", x = \"Hero\", y = \"Average Objective Time\", fill = \"Role\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\nThis gives us a better understanding of why Lucio is higher on the pick list. It also gives us some interesting information about certain Tanks that hold space on objectives really well, or on heros that can go solo contest an objective while the rest of the team goes to fight somewhere else with a positional advantage.\nLets look at healing_done to get a full picture.\n\n\nCode\nhealing_done_data &lt;- filter(data, stat_name == \"Healing Done\")\n\n\nggplot(healing_done_data, aes(x = hero_name, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    coord_flip() +\n    labs(title = \"Boxplot of Healing Done by Hero\", x = \"Hero\", y = \"Healing Done\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support))\n\n\n\n\n\nCode\navg_healing_done_by_hero &lt;- healing_done_data %&gt;%\n    group_by(hero_name, role) %&gt;%\n    summarise(average_healing_done = mean(stat_amount, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;%\n    arrange(desc(average_healing_done))\n\n\n`summarise()` has grouped output by 'hero_name'. You can override using the\n`.groups` argument.\n\n\nCode\nggplot(avg_healing_done_by_hero, aes(x = reorder(hero_name, average_healing_done), y = average_healing_done, fill = role)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    coord_flip() +\n    labs(title = \"Average Healing Done by Hero\", x = \"Hero\", y = \"Average Healing Done\", fill = \"Role\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/Problem Set 10/apple_phone_reviews.html",
    "href": "posts/Problem Set 10/apple_phone_reviews.html",
    "title": "Apple Phone Reviews",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidytext)\nlibrary(lubridate)\nlibrary(ggthemes)\nlibrary(SnowballC)\nIn this problem set, we’ll use Amazon reviews on Apple smart phones to measure some market sentiment and thoughts about several iPhone models. We have two data files, one with the reviews, and one with some product information. After merging the two files we can begin to filter them out for the products we’re interested in and than begin our analysis.\nAfter prepping the data, we will attempt to help solve a couple of business problems.\nreviews = read_csv(\"20191226-reviews.csv\") %&gt;%\n  rownames_to_column(var = \"review.ID\") %&gt;%\n  select(-name, -verified, -title)\nitems = read_csv(\"20191226-items.csv\") %&gt;%\n  select(asin, brand, title)\nA great deal of data cleaning and preparation is required for good text analysis. We first merge the data sets by the product ID “asin” which attaches product name and features to each review in the data. Then we begin to filter out for only apple branded products and then finally those with “iPhone” in the product name. Looking that the body feature we have the text of each review. It is clear that a great variety of product name and formatting exists. However, there are some common patterns we can leverage using regular expressions.\napple_reviews = left_join(reviews, items, by = \"asin\") %&gt;%\n  rename(product = title) %&gt;%\n  filter(brand == \"Apple\") %&gt;%\n  filter(str_detect(product, \"iPhone\")==TRUE) %&gt;%\n  mutate(p2 = str_replace(product, \"Apple \", \"\")) %&gt;%\n  mutate(p2 = str_extract(p2, regex(\"^[^,]*\"))) %&gt;%\n  select(-product) %&gt;%\n  rename(product = p2) %&gt;%\n  mutate(date = lubridate::mdy(date)) %&gt;%\n  mutate(year = lubridate::year(date),\n         month = lubridate::month(date)) %&gt;%\n  mutate(ym = paste(year, month, sep=\"-\")) %&gt;%\n  mutate(ym = lubridate::ym(ym))\n  \n\napple_reviews %&gt;%\n  count(product) \n\n# A tibble: 25 × 2\n   product                                                                     n\n   &lt;chr&gt;                                                                   &lt;int&gt;\n 1 Verizon Prepaid - iPhone 6S (32GB) - Space Gray                             2\n 2 iPhone 11                                                                   1\n 3 iPhone 11 Pro                                                               1\n 4 iPhone 6S                                                                1204\n 5 iPhone 6S Plus                                                            245\n 6 iPhone 7                                                                  449\n 7 iPhone 7 256GB Unlocked GSM 4G LTE Quad-Core Smartphone - Jet Black (R…     7\n 8 iPhone 7 256GB Unlocked GSM 4G LTE Quad-Core Smartphone - Rose Gold (R…     2\n 9 iPhone 7 32GB                                                             370\n10 iPhone 7 Plus                                                             109\n# ℹ 15 more rows\nAbove we can see the list of iPhone products and realize that a lot of cleaning still remains to be donde. 1. Remove storage size references, like any string of numbers starting after a space and followed by “GB”. 2. Remove leading text like “Verizon Prepaid”… 3. Remove other text which follows a size reference (everything after GB). 4. Then remove other things occurring rarely, but which are problematic.\napple_reviews = apple_reviews %&gt;%\n  mutate(product = str_replace(product, \" [0-9]*GB(.+)\", \"\")) %&gt;%\n  mutate(product = str_replace(product, \"[0-9]*GB$\", \"\")) %&gt;%\n  mutate(product = str_replace(product, \"^Verizon Prepaid - \", \"\")) %&gt;%\n  mutate(product = str_replace(product, \"\\\\([0-9]*GB(.+)\", \"\")) %&gt;%\n  mutate(product = str_replace(product, \"a1905\", \"\")) %&gt;%\n  mutate(product = str_trim(str_to_upper(product)))\n\napple_reviews %&gt;%\n  count(product, sort = TRUE)\n\n# A tibble: 13 × 2\n   product            n\n   &lt;chr&gt;          &lt;int&gt;\n 1 IPHONE 6S       1206\n 2 IPHONE X        1194\n 3 IPHONE 7         828\n 4 IPHONE 7 PLUS    479\n 5 IPHONE XS MAX    333\n 6 IPHONE XS        262\n 7 IPHONE 6S PLUS   245\n 8 IPHONE 8         188\n 9 IPHONE 8 PLUS    140\n10 IPHONE SE         63\n11 IPHONE XR         10\n12 IPHONE 11          1\n13 IPHONE 11 PRO      1\nThe scrubbed product list is now shown above. The product names are much cleaner and follow a similar naming convention. We’re now ready to start analyzing the review text.\nWe start by filtering so that we retain only reviews about a select batch of iPhone models. It is these models we wish to assess the market thoughts and sentiment.\napple_reviews = apple_reviews %&gt;%\n  filter(product %in% c(\"IPHONE 6S\", \"IPHONE 6S PLUS\", \"IPHONE 8\", \n                        \"IPHONE 8 PLUS\", \"IPHONE 7\",\n                        \"IPHONE 7 PLUS\", \"IPHONE X\"))\n\napple_reviews %&gt;%\n  count(product, sort = TRUE)\n\n# A tibble: 7 × 2\n  product            n\n  &lt;chr&gt;          &lt;int&gt;\n1 IPHONE 6S       1206\n2 IPHONE X        1194\n3 IPHONE 7         828\n4 IPHONE 7 PLUS    479\n5 IPHONE 6S PLUS   245\n6 IPHONE 8         188\n7 IPHONE 8 PLUS    140"
  },
  {
    "objectID": "posts/Problem Set 10/apple_phone_reviews.html#view-important-words-for-each-product",
    "href": "posts/Problem Set 10/apple_phone_reviews.html#view-important-words-for-each-product",
    "title": "Apple Phone Reviews",
    "section": "2. View Important Words for Each Product",
    "text": "2. View Important Words for Each Product\nWe will use TF-IDF measures to identify key words for a review.\n\nPart A: Calculate top TF-IDF Words for each Review\nWe will set individual reviews as our “document” level, so that term frequency is with respect to a review and inverse-document frequency across all reviews. Note that each review is uniquely identified by review.ID.\n\ntfidf = review.words %&gt;%\n  count(review.ID, word, sort = TRUE) %&gt;%\n  bind_tf_idf(word, review.ID, n)\narrange(tfidf, desc(tf_idf))\n\n# A tibble: 58,279 × 6\n   review.ID word           n    tf   idf tf_idf\n   &lt;chr&gt;     &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 42033     supposed       1 1      4.77   4.77\n 2 48836     supposed       1 1      4.77   4.77\n 3 32134     adaptor        1 0.5    7.41   3.71\n 4 41822     matters        1 0.5    7.00   3.50\n 5 48625     matters        1 0.5    7.00   3.50\n 6 31822     dislike        1 0.5    6.72   3.36\n 7 31897     met            1 0.5    6.16   3.08\n 8 25033     asked          1 0.5    5.62   2.81\n 9 37706     conditions     1 0.5    5.46   2.73\n10 31107     breathe        1 0.333  8.10   2.70\n# ℹ 58,269 more rows\n\n\nAbove we calculated the TF-IDF score of each word in each review. This score says something about the importance of each word in the review it was from. We will now characterize each review by the top 5 words by TF-IDF and keep only those words.\n\nreviews.tfidf = tfidf %&gt;%\n  left_join(apple_reviews, by = \"review.ID\") %&gt;%\n  select(review.ID, product, word, tf_idf, rating) %&gt;%\n  group_by(review.ID) %&gt;%\n  slice_max(order_by = tf_idf, n = 10, with_ties = TRUE) %&gt;%\n  ungroup()\n\nreviews.tfidf\n\n# A tibble: 28,242 × 5\n   review.ID product        word      tf_idf rating\n   &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 20539     IPHONE 6S PLUS noticable  1.24       5\n 2 20539     IPHONE 6S PLUS barely     0.821      5\n 3 20539     IPHONE 6S PLUS minor      0.705      5\n 4 20539     IPHONE 6S PLUS scratches  0.380      5\n 5 20539     IPHONE 6S PLUS screen     0.327      5\n 6 20539     IPHONE 6S PLUS great      0.295      5\n 7 20540     IPHONE 6S PLUS ti         1.62       1\n 8 20540     IPHONE 6S PLUS hace       1.30       1\n 9 20540     IPHONE 6S PLUS doesn’t    0.679      1\n10 20540     IPHONE 6S PLUS return     0.623      1\n# ℹ 28,232 more rows\n\n\n\ntopwords = reviews.tfidf %&gt;%\n  #filter(rating &lt; 4) %&gt;%\n  count(product, word)\n\n\n\n\n\n\n\ntopwords.tfidf = reviews.tfidf %&gt;%\n  count(product, word, sort = TRUE) %&gt;%\n  bind_tf_idf(word, product, n)\ntopwords.tfidf\n\n# A tibble: 11,173 × 6\n   product   word        n      tf   idf tf_idf\n   &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 IPHONE 6S battery   113 0.0136      0      0\n 2 IPHONE 6S great      90 0.0108      0      0\n 3 IPHONE 6S works      68 0.00816     0      0\n 4 IPHONE 6S good       67 0.00804     0      0\n 5 IPHONE X  new        58 0.00743     0      0\n 6 IPHONE 6S like       57 0.00684     0      0\n 7 IPHONE 6S new        54 0.00648     0      0\n 8 IPHONE X  like       53 0.00679     0      0\n 9 IPHONE X  good       51 0.00654     0      0\n10 IPHONE X  screen     51 0.00654     0      0\n# ℹ 11,163 more rows\n\n\n\n\n\n\n\n\nproduct.tfidf = review.words %&gt;%\n  count(product, word, sort = TRUE) %&gt;%\n  bind_tf_idf(word, product, n)\nproduct.tfidf\n\n# A tibble: 13,338 × 6\n   product   word        n      tf   idf tf_idf\n   &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 IPHONE 6S battery   504 0.0270      0      0\n 2 IPHONE X  new       301 0.0158      0      0\n 3 IPHONE 6S new       267 0.0143      0      0\n 4 IPHONE X  screen    250 0.0131      0      0\n 5 IPHONE 6S great     217 0.0116      0      0\n 6 IPHONE X  battery   204 0.0107      0      0\n 7 IPHONE X  like      202 0.0106      0      0\n 8 IPHONE 6S good      197 0.0105      0      0\n 9 IPHONE X  came      186 0.00974     0      0\n10 IPHONE 6S like      183 0.00980     0      0\n# ℹ 13,328 more rows"
  },
  {
    "objectID": "posts/Problem Set 10/apple_phone_reviews.html#footnotes",
    "href": "posts/Problem Set 10/apple_phone_reviews.html#footnotes",
    "title": "Apple Phone Reviews",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n^,↩︎"
  },
  {
    "objectID": "posts/Project Phase 3/p3.html",
    "href": "posts/Project Phase 3/p3.html",
    "title": "2020 Overwatch League Analysis",
    "section": "",
    "text": "Overwatch is a team team-based shooter game with many heros that each come with their own abilities. In 2020, Overwatch League’s gameplay involved teams of six engaging in 6v6 combat across various maps and objectives.\nThe game categorizes characters into three roles:\n\nTanks: These characters are the front line of every team. Tanks make space, absorb incoming damage, and set up plays for their teammates.\nDamage: These characters are the follow up to the front line. They are either launching rockets from up behind cover or are deep behind enemy lines taking out their support.\nSupport: These characters usually take a back seat when it comes to eliminating the enemy team, by healing their team they ensure that their front-liners can take whatever comes at them, without getting taken out."
  },
  {
    "objectID": "posts/Project Phase 3/p3.html#introduction-to-overwatch-2020",
    "href": "posts/Project Phase 3/p3.html#introduction-to-overwatch-2020",
    "title": "2020 Overwatch League Analysis",
    "section": "",
    "text": "Overwatch is a team team-based shooter game with many heros that each come with their own abilities. In 2020, Overwatch League’s gameplay involved teams of six engaging in 6v6 combat across various maps and objectives.\nThe game categorizes characters into three roles:\n\nTanks: These characters are the front line of every team. Tanks make space, absorb incoming damage, and set up plays for their teammates.\nDamage: These characters are the follow up to the front line. They are either launching rockets from up behind cover or are deep behind enemy lines taking out their support.\nSupport: These characters usually take a back seat when it comes to eliminating the enemy team, by healing their team they ensure that their front-liners can take whatever comes at them, without getting taken out."
  },
  {
    "objectID": "posts/Project Phase 3/p3.html#overwatch-league-2020-season-analysis",
    "href": "posts/Project Phase 3/p3.html#overwatch-league-2020-season-analysis",
    "title": "2020 Overwatch League Analysis",
    "section": "Overwatch League: 2020 Season Analysis",
    "text": "Overwatch League: 2020 Season Analysis\nThe analysis focuses on the 2020 Overwatch League season, played on Overwatch’s original format with six-player teams. This season is notable for its strategic and player dynamics, providing insight into player performances and tactics before Overwatch transitioned to a 5v5 format in its sequel, Overwatch 2.\nThe shift to 5v5 in Overwatch 2, particularly impacting the role and balance of Tank characters, offers an interesting contrast to the 2020 season data.\n\nPlayer Representation\nThe map below represents player nationality of which most players are from South Korea. We can see that most players are either from East Asian countries or Western Countries.\n\n\nCode\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\n\n# Load the provided data\nfile_path = 'overwatch_league_player_nationalities_updated.csv'  # Replace with your file path\nplayers_data = pd.read_csv(file_path)\n\n# Check if all values in 'Representation' are integers and convert accordingly\nif (players_data['Representation'] % 1 == 0).all():\n    players_data['Representation'] = players_data['Representation'].astype(int)\nelse:\n    players_data['Representation'] = players_data['Representation'].astype(float)\n\n# Load world data\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n# Merge the world data with the players data\nworld_data = world.merge(players_data, left_on='name', right_on='Country / Region', how='left')\n\n# Create the Plotly choropleth map\nfig = px.choropleth(world_data,\n                    geojson=world_data.geometry,\n                    locations=world_data.index,\n                    color=\"Representation\",\n                    color_continuous_scale=\"Viridis\",\n                    labels={'Representation':'Number of Players'})\n\nfig.update_geos(fitbounds=\"locations\")"
  },
  {
    "objectID": "posts/Project Phase 3/p3.html#player-performance-analysis",
    "href": "posts/Project Phase 3/p3.html#player-performance-analysis",
    "title": "2020 Overwatch League Analysis",
    "section": "Player Performance Analysis",
    "text": "Player Performance Analysis\nMy initial thought to analyze performance of players and heros is to examine their average eliminations per game. This will give us a good idea of who can best secure eliminations which are an important part of winnning not only team fights, but maps as a whole.\n\nTop Players by Eliminations\nTo identify the top players based on average eliminations per game, we have used a bar graph showing the top 20 players alongside the number of games played and sorted by color so we know if one role is better at achieving higher average eliminations than other roles. Including the number of games played can highlight outliers such as ChipSa who ranks number 3 in the league but only played one game all season.\n\n\nCode\nnum_games_per_player &lt;- eliminations_data %&gt;%\n    group_by(player_name) %&gt;%\n    summarise(num_games = n_distinct(esports_match_id), .groups = \"drop\")\n\nplayer_impact &lt;- merge(player_impact, num_games_per_player, by = \"player_name\")\n\ntop_20_players &lt;- player_impact %&gt;%\n    arrange(desc(average_eliminations)) %&gt;%\n    slice_head(n = 20)\n\ntop_20_players$dummy_legend &lt;- \"Number of Games\"\n\nggplot(top_20_players, aes(x = reorder(player_name, average_eliminations), y = average_eliminations, fill = common_role)) +\n    geom_bar(stat = \"identity\") +\n    geom_text(aes(label = num_games, color = dummy_legend), hjust = -0.3, vjust = 0.5, position = position_dodge(width = 0.9)) +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    scale_color_manual(values = \"black\", name = \"\", labels = \"Number of Games\") +\n    coord_flip() +\n    theme_minimal() +\n    labs(x = \"Player Name\", y = \"Average Eliminations Per Game\", fill = \"Player Role\", title = \"Top 20 Players by Average Eliminations Per Game\") +\n    theme(legend.position = \"bottom\", legend.title.align = 0.5)\n\n\n\n\n\n\n\nTop Players in each Role Compared\nAgain we use a bar graph to identify the top players based on elimination average but this time we are separating them into the top 5 based on role. This gives us a much clearer understanding compared with the previous graph that tanks generally have higher average eliminations while supports have the lowest.\n\n\nCode\n# Calculate the number of games for each player\nnum_games_per_player &lt;- eliminations_data %&gt;%\n    group_by(player_name) %&gt;%\n    summarise(num_games = n_distinct(esports_match_id), .groups = \"drop\")\n\n# Get the top 5 players within each role separately\ntop_players_by_role &lt;- eliminations_data %&gt;%\n    group_by(role, player_name) %&gt;%\n    summarise(average_eliminations = mean(stat_amount, na.rm = TRUE), .groups = \"drop\") %&gt;%\n    group_by(role) %&gt;% # Ensure the slicing is done within each role\n    slice_max(order_by = average_eliminations, n = 5, with_ties = FALSE) %&gt;%\n    ungroup() %&gt;%\n    arrange(role, desc(average_eliminations))\n\n# Merge this information with the number of games\ntop_players_by_role &lt;- merge(top_players_by_role, num_games_per_player, by = \"player_name\")\n\n# Create a dummy variable for the legend (for the number of games)\ntop_players_by_role$dummy_legend &lt;- \"Number of Games\"\n\nggplot(top_players_by_role, aes(x = reorder(player_name, average_eliminations), y = average_eliminations, fill = role)) +\n    geom_bar(stat = \"identity\", position = position_dodge(width = 0.7)) +\n    geom_text(aes(label = num_games, color = dummy_legend), hjust = -0.3, vjust = 0.5, position = position_dodge(width = 0.9)) +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    scale_color_manual(values = \"black\", name = \"\", labels = \"Number of Games\") +\n    coord_flip() +\n    labs(title = \"Top 5 Players by Average Eliminations within Each Role\", x = \"Player\", y = \"Average Eliminations\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\", legend.title.align = 0.5)\n\n\n\n\n\n\n\nTop Players in each Role Comparing all Roles played\nThis graph allows us to see which roles the players from the previous chart played best on in terms of average eliminations. This also gives us a better understanding of how these top players performed over a longer time. by showing their extremes as well as their first and third quartiles.\n\n\nCode\n# Identifying top 5 players in each role based on average eliminations\ntop_players_by_role &lt;- eliminations_data %&gt;%\n    group_by(player_name, role) %&gt;%\n    summarise(average_eliminations = mean(stat_amount, na.rm = TRUE), .groups = \"drop\") %&gt;%\n    group_by(role) %&gt;%\n    slice_max(order_by = average_eliminations, n = 5, with_ties = FALSE) %&gt;%\n    ungroup() %&gt;%\n    select(player_name)\n\n# Subsetting the eliminations_data to include only these top players\ntop_eliminations_data &lt;- eliminations_data %&gt;%\n    semi_join(top_players_by_role, by = \"player_name\")\n\n# Creating the boxplot with the subsetted data\nggplot(top_eliminations_data, aes(x = player_name, y = stat_amount, fill = role)) +\n    geom_boxplot(outlier.shape = NA) + # Optional: Hide outliers\n    coord_flip() +\n    labs(title = \"Boxplot of Eliminations by Top 5 Players in Each Role\", x = \"Player\", y = \"Number of Eliminations\") +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    theme_minimal() +\n    theme(legend.position = \"bottom\", axis.text.y = element_text(size = 7))"
  },
  {
    "objectID": "posts/Project Phase 3/p3.html#hero-performace-analysis",
    "href": "posts/Project Phase 3/p3.html#hero-performace-analysis",
    "title": "2020 Overwatch League Analysis",
    "section": "Hero Performace Analysis",
    "text": "Hero Performace Analysis\n\nHero Elimination Impact\nThe objective of this graph is to understand which heros stand out when it comes to average eliminations. We can see that Damage and Tank heros generally trend towards the higher end of the graph while Supports who are mostly focused on healing and keeping their team alive have less focus spent on getting eliminations. Moira stands out as a support in the number 3 spot which can be explained by her kit which requires Moira to deal damage to regenerate her healing capibilities.\n\n\nCode\navg_elim_by_hero &lt;- eliminations_data %&gt;%\n    group_by(hero_name, role) %&gt;% # Assuming there's a 'role' column\n    summarise(average_eliminations = mean(stat_amount, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;% # Remove grouping\n    arrange(desc(average_eliminations))\n\nggplot(avg_elim_by_hero, aes(x = reorder(hero_name, average_eliminations), y = average_eliminations, fill = role)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    coord_flip() +\n    labs(title = \"Average Eliminations by Hero\", x = \"Hero\", y = \"Average Eliminations\", fill = \"Role\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nElimination Distribution\nThis graph shows us the distribution of eliminations from game to game which highlights the differences between the Tank, Damage, and Support roles.\n\n\nCode\nggplot(eliminations_data, aes(x = stat_amount, fill = role)) +\n    geom_histogram(bins = 30, alpha = 0.7, position = \"dodge\") +\n    labs(title = \"Elimination Count Distribution by Role\", x = \"Number of Eliminations\", y = \"Frequency\") +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    theme_minimal()\n\n\n\n\n\n\n\nHero Performance by Elimination Boxplot\nThese box plots allow us to visualize hero performance based on eliminations per game so we can see which hero’s perform better, often. This also allows us to see which outliers are effecting the averages. We can also see some hero’s that are not best evaluated by eliminations such as Mercy who is a primarily healing focused support hero.\n\n\nCode\nggplot(eliminations_data, aes(x = hero_name, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    coord_flip() +\n    labs(title = \"Boxplot of Eliminations by Hero\", x = \"Hero\", y = \"Number of Eliminations\") +\n    theme_minimal()+\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support))\n\n\n\n\n\n\n\nRole Performance Boxplot\nThis box plot will allow us to visualize which role is having the most elimination impact over the entire league. We can see that there are some outlying games but overall the Damage role’s top 50% is much larger than the others however Tank covers a little bit of a wider range.\nWe can also see that Tanks have a higher average than both Support and Damage.\n\n\nCode\nggplot(eliminations_data, aes(x = role, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    labs(title = \"Boxplot of Eliminations by Role\", x = \"Role\", y = \"Number of Eliminations\") +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    theme_minimal()"
  },
  {
    "objectID": "posts/Project Phase 3/p3.html#general-analysises",
    "href": "posts/Project Phase 3/p3.html#general-analysises",
    "title": "2020 Overwatch League Analysis",
    "section": "General Analysises",
    "text": "General Analysises\n\nMap-Specific Hero Picks\nAnother very important aspect of the game is which map you are playing on. Teams who lose the previous map get to pick the next map to play on. This heat map allows us to visualize which heros are most played on which maps.\n\n\nCode\nhero_picks_count &lt;- data %&gt;%\n    group_by(esports_match_id, map_name, hero_name) %&gt;%\n    summarise(count = n(), .groups = \"drop\") %&gt;%\n    ungroup()\n\nhero_picks_aggregated &lt;- hero_picks_count %&gt;%\n    group_by(map_name, hero_name) %&gt;%\n    summarise(total_count = sum(count), .groups = \"drop\")\n\nhero_picks_wide &lt;- hero_picks_aggregated %&gt;%\n    pivot_wider(names_from = map_name, values_from = total_count, values_fill = list(total_count = 0))\n\nhero_picks_long &lt;- hero_picks_wide %&gt;%\n    gather(key = \"map_name\", value = \"pick_count\", -hero_name)\n\nggplot(hero_picks_long, aes(x = map_name, y = hero_name, fill = pick_count)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient(low = \"white\", high = \"steelblue\") +\n    labs(title = \"Hero Picks on Different Maps\", x = \"Map\", y = \"Hero\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nWe can see that Lucio, baptiste, Reinhardt, Tracer, Zenyatta, and Sigma were played on most maps. We can also see that Zarya, Roadhog, Junkrat and Bastion were not picked that much.\nThis heat map also allows so to see that Havana, Horizon Lunar Colony, and Numbani were not picked as maps to play on as much as the others."
  },
  {
    "objectID": "posts/Project Phase 3/p3.html#machine-learning",
    "href": "posts/Project Phase 3/p3.html#machine-learning",
    "title": "2020 Overwatch League Analysis",
    "section": "Machine Learning",
    "text": "Machine Learning\nLooking at all the previous information and seeing that support heros like lucio were played all the time, leads us to ask the question: “Why?” Lucio appeared so low on the hero analysis earlier but he was one of the most popular picks even though he didn’t have much elimination impact.\nTo answer this question, we have trained multiple Machine Learning Models to determine which features are going to be the best to look at when determining Hero and Player performance.\n\n\nCode\n# Import necessary libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Load the dataset\ndata = pd.read_csv(\"merged_esports_data_updated.csv\")\n\n# Preprocess the data (fill missing values, one-hot encoding)\ndata = data.fillna(data.median(numeric_only=True))\ndata = pd.get_dummies(data, columns=[\"map_type\", \"map_name\", \"player_name\", \"team_name\", \"hero_name\"])\n\n# Drop irrelevant columns\ndata = data.drop(columns=[\"team_one_name\", \"team_two_name\", \"match_id\"])\n\n# Split the data into features and target\nX = data.drop(\"match_winner\", axis=1)\ny = data[\"match_winner\"]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train models\nlr_model = LogisticRegression().fit(X_train, y_train)\nrf_model = RandomForestClassifier(n_estimators=50, random_state=42).fit(X_train, y_train)\ngb_model = GradientBoostingClassifier(n_estimators=100, random_state=42).fit(X_train, y_train)\n\n# Predictions and Evaluation\npredictions_lr = lr_model.predict(X_test)\npredictions_rf = rf_model.predict(X_test)\npredictions_gb = gb_model.predict(X_test)\n\n# Save predictions and true values for R\nresults = pd.DataFrame({\n    \"y_test\": y_test,\n    \"predictions_lr\": predictions_lr,\n    \"predictions_rf\": predictions_rf,\n    \"predictions_gb\": predictions_gb\n})\nresults.to_csv(\"model_results.csv\", index=False)\n\n# Save feature importances for R\nfeature_importances = pd.DataFrame({\n    \"feature\": X.columns,\n    \"importance_rf\": rf_model.feature_importances_,\n    \"importance_gb\": gb_model.feature_importances_\n})\nfeature_importances.to_csv(\"feature_importances.csv\", index=False)\n\n\n\n\nCode\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(reshape2)\n\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\nCode\n# Read the model results and feature importances\nresults &lt;- read.csv(\"model_results.csv\")\nfeature_importances &lt;- read.csv(\"feature_importances.csv\")\n\n\n# Selecting the top 10 important features for each model\ntop_features_rf &lt;- feature_importances %&gt;%\n                   arrange(desc(importance_rf)) %&gt;%\n                   head(10)\n\ntop_features_gb &lt;- feature_importances %&gt;%\n                   arrange(desc(importance_gb)) %&gt;%\n                   head(10)\n\n# Confusion matrices and performance metrics\n# Logistic Regression\ncm_lr &lt;- confusionMatrix(as.factor(results$predictions_lr), as.factor(results$y_test))\nmetrics_lr &lt;- data.frame(Model = \"Logistic Regression\", Accuracy = cm_lr$overall['Accuracy'], Sensitivity = cm_lr$byClass['Sensitivity'], Specificity = cm_lr$byClass['Specificity'])\n\n# Random Forest\ncm_rf &lt;- confusionMatrix(as.factor(results$predictions_rf), as.factor(results$y_test))\nmetrics_rf &lt;- data.frame(Model = \"Random Forest\", Accuracy = cm_rf$overall['Accuracy'], Sensitivity = cm_rf$byClass['Sensitivity'], Specificity = cm_rf$byClass['Specificity'])\n\n# Gradient Boosting\ncm_gb &lt;- confusionMatrix(as.factor(results$predictions_gb), as.factor(results$y_test))\nmetrics_gb &lt;- data.frame(Model = \"Gradient Boosting\", Accuracy = cm_gb$overall['Accuracy'], Sensitivity = cm_gb$byClass['Sensitivity'], Specificity = cm_gb$byClass['Specificity'])\n\n# Combine metrics and melt for ggplot\ncombined_metrics &lt;- rbind(metrics_lr, metrics_rf, metrics_gb)\nmelted_metrics &lt;- melt(combined_metrics, id.vars = \"Model\")\n\n# Bar plot of accuracy, sensitivity, and specificity for each model\nggplot(melted_metrics, aes(x = variable, y = value, fill = variable)) +\n    geom_bar(stat = \"identity\", position = position_dodge()) +\n    facet_wrap(~Model) +\n    scale_fill_manual(values = c(\"Accuracy\" = tank, \"Sensitivity\" = damage, \"Specificity\" = support)) +\n    labs(x = \"Metric\", y = \"Value\", title = \"Model Performance Comparison\") +\n    theme_minimal()\n\n\n\n\n\nFrom analyzing the feature importance, accuracy, sensitivity, and specificity of each model, we can see that the Random Forest model does the best job at correctly predicting which team is going to win.\n\n\nCode\n# Random Forest\nggplot(top_features_rf, aes(x = reorder(feature, importance_rf), y = importance_rf)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip() +\n    labs(title = \"Top 10 Feature Importances - Random Forest\")\n\n\n\n\n\nThe Random Forest Model uses the in game statistics to determine which team is going to win, while the Gradient Boosting model mostly just picks it’s favorite team. Logistic Regression couldn’t identify any true negatives, and gradient boosting did not do that great either. So for the highest accuracy, and the most realistic true positive and best true negative predicting, Random Forest stands miles ahead of the other models trained."
  },
  {
    "objectID": "posts/Project Phase 3/p3.html#taking-a-different-approach-based-on-the-machine-learning-model",
    "href": "posts/Project Phase 3/p3.html#taking-a-different-approach-based-on-the-machine-learning-model",
    "title": "2020 Overwatch League Analysis",
    "section": "Taking a different approach based on the Machine Learning Model",
    "text": "Taking a different approach based on the Machine Learning Model\nSince the Random Forest machine learning model decided that average_time_alive was the best feature to determine which team was going to win, I want to create a box plot showing which heros were able to survive longer on average.\n\nAverage Time Alive\n\n\nCode\ntime_alive_data &lt;- filter(data, stat_name == \"Average Time Alive\")\n\n\nggplot(time_alive_data, aes(x = hero_name, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    coord_flip() +\n    labs(title = \"Boxplot of Average Time Alive by Hero\", x = \"Hero\", y = \"Average Time Alive\") +\n    theme_minimal()+\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support))\n\n\n\n\n\n\n\nCode\navg_time_alive_by_hero &lt;- time_alive_data %&gt;%\n    group_by(hero_name, role) %&gt;% # Assuming there's a 'role' column\n    summarise(average_time_alive = mean(stat_amount, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;% # Remove grouping\n    arrange(desc(average_time_alive))\n\n\n`summarise()` has grouped output by 'hero_name'. You can override using the\n`.groups` argument.\n\n\nCode\nggplot(avg_time_alive_by_hero, aes(x = reorder(hero_name, average_time_alive), y = average_time_alive, fill = role)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    coord_flip() +\n    labs(title = \"Average Time Alive by Hero\", x = \"Hero\", y = \"Average Time Alive\", fill = \"Role\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\nThese graphs explain some of the other picks such as Mercy and Sombra, and also reinforce D.Va as one of our top picks but lets look at the objective-time and healing_done features to get a better picture.\n\n\nObjective Time\n\n\nCode\nobjective_time_data &lt;- filter(data, stat_name == \"Objective Time\")\nggplot(objective_time_data, aes(x = hero_name, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    coord_flip() +\n    labs(title = \"Boxplot of Objective Time by Hero\", x = \"Hero\", y = \"Objective Time\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support))\n\n\n\n\n\nCode\navg_objective_time_by_hero &lt;- objective_time_data %&gt;%\n    group_by(hero_name, role) %&gt;%\n    summarise(average_objective_time = mean(stat_amount, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;%\n    arrange(desc(average_objective_time))\n\n\n`summarise()` has grouped output by 'hero_name'. You can override using the\n`.groups` argument.\n\n\nCode\nggplot(avg_objective_time_by_hero, aes(x = reorder(hero_name, average_objective_time), y = average_objective_time, fill = role)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    coord_flip() +\n    labs(title = \"Average Objective Time by Hero\", x = \"Hero\", y = \"Average Objective Time\", fill = \"Role\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\nThis gives us a better understanding of why Lucio is higher on the pick list. It also gives us some interesting information about certain Tanks that hold space on objectives really well, or on heroes that can go solo contest an objective while the rest of the team goes to fight somewhere else with a positional advantage.\nLets look at healing_done to get a full picture.\n\n\nHealing Done\n\n\nCode\nhealing_done_data &lt;- filter(data, stat_name == \"Healing Done\")\n\n\nggplot(healing_done_data, aes(x = hero_name, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    coord_flip() +\n    labs(title = \"Boxplot of Healing Done by Hero\", x = \"Hero\", y = \"Healing Done\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support))\n\n\n\n\n\nCode\navg_healing_done_by_hero &lt;- healing_done_data %&gt;%\n    group_by(hero_name, role) %&gt;%\n    summarise(average_healing_done = mean(stat_amount, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;%\n    arrange(desc(average_healing_done))\n\n\n`summarise()` has grouped output by 'hero_name'. You can override using the\n`.groups` argument.\n\n\nCode\nggplot(avg_healing_done_by_hero, aes(x = reorder(hero_name, average_healing_done), y = average_healing_done, fill = role)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    coord_flip() +\n    labs(title = \"Average Healing Done by Hero\", x = \"Hero\", y = \"Average Healing Done\", fill = \"Role\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\nThis gives us a better picture on why Support heroes are so impactful. They are the only heroes in the game (other than Soldier: 76) that can heal others. This tells us why support heroes are picked so much but doesn’t reinforce why Lucio is so high on the pick list. To understand this we have to look at Lucio’s kit which includes a speed boost for all allies within range. This specific ability in his kit has been cited by the professional analysts in the league to be the key factor in why his pick rate is so high. Lucio allows your team to move across the map faster, make obtaining better positioning easier, and also allows for quick decisions that the opposing team wont be expecting."
  },
  {
    "objectID": "posts/Project Phase 3/p3.html#summary",
    "href": "posts/Project Phase 3/p3.html#summary",
    "title": "2020 Overwatch League Analysis",
    "section": "Summary",
    "text": "Summary\nOverall this analysis has given us a good understanding of which players and heroes performed the best throughout the 2020 Overwatch League Season. If this was a game played by robots we would now be able to accurately predict which team was going to win every time, and which heroes should be picked. Alas, there are many things that cannot be evaluated by the statistics tracked in game. The decision making skills of the in-game leaders, the individual decisions that all 12 players are making in the heat of the moment, as well as the abilities that cannot be well measured like Lucio’s speed boost."
  },
  {
    "objectID": "posts/Final Project/p3.html",
    "href": "posts/Final Project/p3.html",
    "title": "QMBE 3740 & CDS 1130 Final Project - 2020 Overwatch League Analysis",
    "section": "",
    "text": "Overwatch is a team-based shooter game with many heroes that come with their own abilities. In 2020, Overwatch League’s gameplay involved teams of six engaging in 6v6 combat across various maps and objectives.\nThe game categorizes characters into three roles:\n\nTanks: These characters are the front line of every team. Tanks make space, absorb incoming damage, and set up plays for their teammates.\nDamage: These characters are the follow up to the front line. They are either launching rockets from up behind cover or are deep behind enemy lines taking out their support.\nSupport: These characters usually take a back seat when it comes to eliminating the enemy team, by healing their team they ensure that their front-liners can take whatever comes at them, without getting taken out."
  },
  {
    "objectID": "posts/Final Project/p3.html#introduction-to-overwatch-2020",
    "href": "posts/Final Project/p3.html#introduction-to-overwatch-2020",
    "title": "QMBE 3740 & CDS 1130 Final Project - 2020 Overwatch League Analysis",
    "section": "",
    "text": "Overwatch is a team-based shooter game with many heroes that come with their own abilities. In 2020, Overwatch League’s gameplay involved teams of six engaging in 6v6 combat across various maps and objectives.\nThe game categorizes characters into three roles:\n\nTanks: These characters are the front line of every team. Tanks make space, absorb incoming damage, and set up plays for their teammates.\nDamage: These characters are the follow up to the front line. They are either launching rockets from up behind cover or are deep behind enemy lines taking out their support.\nSupport: These characters usually take a back seat when it comes to eliminating the enemy team, by healing their team they ensure that their front-liners can take whatever comes at them, without getting taken out."
  },
  {
    "objectID": "posts/Final Project/p3.html#overwatch-league-2020-season-analysis",
    "href": "posts/Final Project/p3.html#overwatch-league-2020-season-analysis",
    "title": "QMBE 3740 & CDS 1130 Final Project - 2020 Overwatch League Analysis",
    "section": "Overwatch League: 2020 Season Analysis",
    "text": "Overwatch League: 2020 Season Analysis\nThe analysis focuses on the 2020 Overwatch League season, played on Overwatch’s original format with six-player teams. This season is notable for its strategic and player dynamics, providing insight into player performances and tactics before Overwatch transitioned to a 5v5 format in its sequel, Overwatch 2.\nThe shift to 5v5 in Overwatch 2, particularly impacting the role and balance of Tank characters, offers an interesting contrast to the 2020 season data.\n\nPlayer Representation\nThe map below represents player nationality of which most players are from South Korea. We can see that most players are either from East Asian countries or Western Countries.\n\n\nCode\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\n\nfile_path = 'overwatch_league_player_nationalities_updated.csv'\nplayers_data = pd.read_csv(file_path)\n\nif (players_data['Representation'] % 1 == 0).all():\n    players_data['Representation'] = players_data['Representation'].astype(int)\nelse:\n    players_data['Representation'] = players_data['Representation'].astype(float)\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nworld_data = world.merge(players_data, left_on='name', right_on='Country / Region', how='left')\n\nfig = px.choropleth(world_data,\n                    geojson=world_data.geometry,\n                    locations=world_data.index,\n                    color=\"Representation\",\n                    color_continuous_scale=\"Viridis\",\n                    labels={'Representation':'Number of Players'})\nfig.update_geos(fitbounds=\"locations\")"
  },
  {
    "objectID": "posts/Final Project/p3.html#machine-learning",
    "href": "posts/Final Project/p3.html#machine-learning",
    "title": "QMBE 3740 & CDS 1130 Final Project - 2020 Overwatch League Analysis",
    "section": "Machine Learning",
    "text": "Machine Learning\n\n\nCode\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\ndata = pd.read_csv(\"merged_esports_data_updated.csv\")\ndata = data.fillna(data.median(numeric_only=True))\ndata = pd.get_dummies(data, columns=[\"map_type\", \"map_name\", \"player_name\", \"team_name\", \"hero_name\"])\ndata = data.drop(columns=[\"team_one_name\", \"team_two_name\", \"match_id\"])\nX = data.drop(\"match_winner\", axis=1)\ny = data[\"match_winner\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nlr_model = LogisticRegression().fit(X_train, y_train)\nrf_model = RandomForestClassifier(n_estimators=50, random_state=42).fit(X_train, y_train)\ngb_model = GradientBoostingClassifier(n_estimators=100, random_state=42).fit(X_train, y_train)\n\npredictions_lr = lr_model.predict(X_test)\npredictions_rf = rf_model.predict(X_test)\npredictions_gb = gb_model.predict(X_test)\n\nresults = pd.DataFrame({\n    \"y_test\": y_test,\n    \"predictions_lr\": predictions_lr,\n    \"predictions_rf\": predictions_rf,\n    \"predictions_gb\": predictions_gb\n})\nresults.to_csv(\"model_results.csv\", index=False)\n\nfeature_importances = pd.DataFrame({\n    \"feature\": X.columns,\n    \"importance_rf\": rf_model.feature_importances_,\n    \"importance_gb\": gb_model.feature_importances_\n})\nfeature_importances.to_csv(\"feature_importances.csv\", index=False)\n\n\nTo better understand which features are most relevant to a team winning a game, we are going to train and test machine learning models to be able to predict which team is going to win based on the dataset’s features.\nFrom analyzing the feature importance, accuracy, sensitivity, and specificity of each model, we can see that the Random Forest model does the best job at correctly predicting which team is going to win.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(reshape2)\n\nresults &lt;- read.csv(\"model_results.csv\")\nfeature_importances &lt;- read.csv(\"feature_importances.csv\")\n\ntop_features_rf &lt;- feature_importances %&gt;%\n                   arrange(desc(importance_rf)) %&gt;%\n                   head(10)\n\ntop_features_gb &lt;- feature_importances %&gt;%\n                   arrange(desc(importance_gb)) %&gt;%\n                   head(10)\n\ncm_lr &lt;- confusionMatrix(as.factor(results$predictions_lr), as.factor(results$y_test))\nmetrics_lr &lt;- data.frame(Model = \"Logistic Regression\", Accuracy = cm_lr$overall['Accuracy'], Sensitivity = cm_lr$byClass['Sensitivity'], Specificity = cm_lr$byClass['Specificity'])\n\ncm_rf &lt;- confusionMatrix(as.factor(results$predictions_rf), as.factor(results$y_test))\nmetrics_rf &lt;- data.frame(Model = \"Random Forest\", Accuracy = cm_rf$overall['Accuracy'], Sensitivity = cm_rf$byClass['Sensitivity'], Specificity = cm_rf$byClass['Specificity'])\n\ncm_gb &lt;- confusionMatrix(as.factor(results$predictions_gb), as.factor(results$y_test))\nmetrics_gb &lt;- data.frame(Model = \"Gradient Boosting\", Accuracy = cm_gb$overall['Accuracy'], Sensitivity = cm_gb$byClass['Sensitivity'], Specificity = cm_gb$byClass['Specificity'])\n\ncombined_metrics &lt;- rbind(metrics_lr, metrics_rf, metrics_gb)\nmelted_metrics &lt;- melt(combined_metrics, id.vars = \"Model\")\n\nggplot(melted_metrics, aes(x = variable, y = value, fill = variable)) +\n    geom_bar(stat = \"identity\", position = position_dodge()) +\n    facet_wrap(~Model) +\n    scale_fill_manual(values = c(\"Accuracy\" = tank, \"Sensitivity\" = damage, \"Specificity\" = support)) +\n    labs(x = \"Metric\", y = \"Value\", title = \"Model Performance Comparison\") +\n    theme_minimal()\n\n\n\n\n\n\n\nA Random Forest Model is a “forest” of decision trees which take different features of the dataset, draws a line between win and lose, then categorizes any data we give it until it is correct most of the time.\nFor example, the decision trees could determine that more than 90 seconds of average alive time, more than 40 seconds of objective time, and more than 10 eliminations a game mean that someone is more likely to win. Continuing this example, a hero that has 100 seconds of average alive time, 41 seconds of objective time, and 9 eliminations, the Random Forest would say this is most likely to be a win, as average alive time and objective time outweigh eliminations.\n\n\nCode\nggplot(top_features_rf, aes(x = reorder(feature, importance_rf), y = importance_rf)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip() +\n    labs(title = \"Top 10 Feature Importances - Random Forest\")\n\n\n\n\n\nThe Random Forest Model uses in-game statistics to predict the winning team, whereas the Gradient Boosting model tends to favor a specific team in its selections; which is unhelpful in the long term when rosters change. Logistic Regression demonstrated an inability to accurately identify true negatives, which was also seen in the Gradient Boosting model. Therefore, considering the criteria of accuracy, realistic true positive identification, and effective true negative prediction, the Random Forest model significantly outperforms the other trained models.\n\n\nCode\nggplot(top_features_gb, aes(x= reorder(feature, importance_gb), y = importance_gb)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip() +\n    labs(title = \"Top 10 Feature Importances - Gradient Boosting\")"
  },
  {
    "objectID": "posts/Final Project/p3.html#analyzing-features-based-on-the-random-forests-feature-importance",
    "href": "posts/Final Project/p3.html#analyzing-features-based-on-the-random-forests-feature-importance",
    "title": "QMBE 3740 & CDS 1130 Final Project - 2020 Overwatch League Analysis",
    "section": "Analyzing features based on the Random Forest’s Feature Importance",
    "text": "Analyzing features based on the Random Forest’s Feature Importance\n\nAverage Time Alive\n\n\nCode\ntime_alive_data &lt;- filter(data, stat_name == \"Average Time Alive\")\navg_time_alive_by_hero &lt;- time_alive_data %&gt;%\n    group_by(hero_name, role) %&gt;%\n    summarise(average_time_alive = mean(stat_amount, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;%\n    arrange(desc(average_time_alive))\n\nordered_heroes &lt;- rev(avg_time_alive_by_hero$hero_name)\ntime_alive_data$hero_name &lt;- factor(time_alive_data$hero_name, levels = ordered_heroes)\n\nggplot(time_alive_data, aes(x = hero_name, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    coord_flip() +\n    labs(title = \"Boxplot of Average Time Alive by Hero\", x = \"Hero\", y = \"Average Time Alive\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nLooking at this box plot we can see that Sombra has the highest Average Time Alive of any hero. This is most likely because she can use her translocator, which is a device she leaves on the floor somewhere and can instantly teleport back to at any time. Symmetra is the lowest on the list and this is most likely due to the fact that Symmetra is picked for a couple of seconds at the beginning of many rounds as her placable teleporter allows teams to quickly reposition right out of the gates.\n\n\nAverage Objective Time\n\n\nCode\nobjective_time_data &lt;- filter(data, stat_name == \"Objective Time\")\nobjective_time_by_hero &lt;- objective_time_data %&gt;%\n    group_by(hero_name, role) %&gt;%\n    summarise(average_objective_time = mean(stat_amount, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;%\n    arrange(desc(average_objective_time))\n\nordered_heroes_objective &lt;- rev(objective_time_by_hero$hero_name)\nobjective_time_data$hero_name &lt;- factor(objective_time_data$hero_name, levels = ordered_heroes_objective)\n\nggplot(objective_time_data, aes(x = hero_name, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    coord_flip() +\n    labs(title = \"Boxplot of Objective Time by Hero\", x = \"Hero\", y = \"Objective Time\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support))\n\n\n\n\n\n\n\nIn this Average Objective Time plot we can see that Orisa, Reinhardt, and D.Va have the highest average objective time this season. This lines up with the Tank’s role to create space for their team which includes taking and holding objective spaces. Heroes like Widowmaker who rely on off-angle positioning to be effective, are more likely to be near but not on the objective.\n\n\nAverage Healing Done\n\n\n\n\n\nIn this healing chart we can see that it is almost exclusively supports. This is because with the exception of Soldier: 76, supports are the only heroes that can heal their teammates. Moira being at the top of this list does not surprise me as she was one of the strongest picks during this season since she was able to deal damage and heal very effectively.\n\n\nAverage Eliminations\n\n\nCode\naverage_eliminations_by_hero &lt;- eliminations_data %&gt;%\n    group_by(hero_name, role) %&gt;%\n    summarise(average_eliminations = mean(stat_amount, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;%\n    arrange(desc(average_eliminations))\nordered_heroes_eliminations &lt;- rev(average_eliminations_by_hero$hero_name)\neliminations_data$hero_name &lt;- factor(eliminations_data$hero_name, levels = ordered_heroes_eliminations)\n\nggplot(eliminations_data, aes(x = hero_name, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    coord_flip() +\n    labs(title = \"Boxplot of Eliminations by Hero\", x = \"Hero\", y = \"Number of Eliminations\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support))\n\n\n\n\n\n\n\nIn ths graph, we can see that Damage and Tank heros generally trend towards the higher end of the graph while Supports who are mostly focused on healing and keeping their team alive have less focus spent on getting eliminations. Moira stands out as a support in the number 3 spot which can be explained by her kit which requires Moira to deal damage to regenerate her healing capibilities."
  },
  {
    "objectID": "posts/Final Project/p3.html#comparing-our-features-next-to-each-other",
    "href": "posts/Final Project/p3.html#comparing-our-features-next-to-each-other",
    "title": "QMBE 3740 & CDS 1130 Final Project - 2020 Overwatch League Analysis",
    "section": "Comparing our features next to each other",
    "text": "Comparing our features next to each other\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nowl2020_data = pd.read_csv('owl2020data.csv')\nhero_roles = {\n    \"D.Va\": \"Tank\", \"Reinhardt\": \"Tank\", \"Winston\": \"Tank\", \"Roadhog\": \"Tank\", \"Zarya\": \"Tank\", \n    \"Orisa\": \"Tank\", \"Sigma\": \"Tank\", \"Wrecking Ball\": \"Tank\", \"Doomfist\": \"Damage\", \n    \"Genji\": \"Damage\", \"McCree\": \"Damage\", \"Pharah\": \"Damage\", \"Reaper\": \"Damage\", \n    \"Soldier: 76\": \"Damage\", \"Sombra\": \"Damage\", \"Tracer\": \"Damage\", \"Bastion\": \"Damage\", \n    \"Hanzo\": \"Damage\", \"Junkrat\": \"Damage\", \"Mei\": \"Damage\", \"Torbjörn\": \"Damage\", \n    \"Widowmaker\": \"Damage\", \"D.Va\": \"Tank\", \"Orisa\": \"Tank\", \"Reinhardt\": \"Tank\", \n    \"Roadhog\": \"Tank\", \"Winston\": \"Tank\", \"Wrecking Ball\": \"Tank\", \"Zarya\": \"Tank\", \n    \"Ana\": \"Support\", \"Baptiste\": \"Support\", \"Brigitte\": \"Support\", \"Lúcio\": \"Support\", \n    \"Mercy\": \"Support\", \"Moira\": \"Support\", \"Zenyatta\": \"Support\"\n}\nowl2020_data['role'] = owl2020_data['hero_name'].map(hero_roles)\ncustom_palette = {\n    \"Tank\": \"#ff7eb6\",\n    \"Damage\": \"#4589ff\",\n    \"Support\": \"#3ddbd9\"\n}\n\nplt.figure(figsize=(18, 6))\n\nplt.subplot(1, 3, 1)\nsns.scatterplot(x='average_time_alive', y='eliminations', hue='role', data=owl2020_data, palette=custom_palette)\nplt.title('Average Time Alive vs. Eliminations')\nplt.xlabel('Average Time Alive (sec)')\nplt.ylabel('Eliminations')\n\nplt.subplot(1, 3, 2)\nsns.scatterplot(x='objective_time', y='eliminations', hue='role', data=owl2020_data, palette=custom_palette)\nplt.title('Objective Time vs. Eliminations')\nplt.xlabel('Objective Time (sec)')\nplt.ylabel('Eliminations')\n\nplt.subplot(1, 3, 3)\nsns.scatterplot(x='average_time_alive', y='objective_time', hue='role', data=owl2020_data, palette=custom_palette)\nplt.title('Average Time Alive vs. Objective Time')\nplt.xlabel('Average Time Alive (sec)')\nplt.ylabel('Objective Time (sec)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThese scatter plots compare the feautres of Average Time Alive, Objective Time, and Eliminations.\n\nAverage Time Alive - Eliminations\nThis plot shows a distribution of data points representing different roles in relation to how long they stay alive (in seconds) and how many eliminations they achieve. We can see that Tank heros are able to survive longer and get more eliminations as they are durable in fights, that Damage heros have a wide spread of variability as they are less durable but are able to pick up more eliminations, and Supports have fewer eliminations but a higher time alive which is a nod to their supportive play-style.\n\n\nObjective Time - Eliminations\nThis plot presents the relationship between time spent capturing objectives (in seconds) and the number of eliminations. The damage role again shows a high degree of variability in both metrics, Tank roles less so, and Support roles even less, which is consistent with Overwatch’s in-game roles where Damage roles aim for eliminations, Tanks control objective space, while Supports provide assistance.\n\n\nAverage Time Alivee - Objective Time\nThis plot shows the average time a player stays alive with the time they spend securing objectives. There’s a dense cluster of Support and Tank roles with high objective times, which, again, aligns with Overwatch’s role identities that prioritize the objective over seeking eliminations. Damage roles are more dispersed, which tells us that they balance engaging in fights and playing the objective."
  },
  {
    "objectID": "posts/Final Project/p3.html#summary",
    "href": "posts/Final Project/p3.html#summary",
    "title": "QMBE 3740 & CDS 1130 Final Project - 2020 Overwatch League Analysis",
    "section": "Summary",
    "text": "Summary\nOverall this analysis has given us a good understanding of which players and heroes performed the best throughout the 2020 Overwatch League Season. If this was a game played by robots we would now be able to accurately predict which team was going to win every time, and which heroes should be picked. Alas, there are many things that cannot be evaluated by the statistics tracked in game. The decision making skills of the in-game leaders, the individual decisions that all 12 players are making in the heat of the moment, as well as the abilities that cannot be well measured like Lucio’s speed boost."
  },
  {
    "objectID": "posts/Final Project/2020OWL-Analysis.html",
    "href": "posts/Final Project/2020OWL-Analysis.html",
    "title": "QMBE 3740 & CDS 1130 Final Project - 2020 Overwatch League Analysis",
    "section": "",
    "text": "Overwatch is a team-based shooter game with many heroes with different abilities. In 2020, the Overwatch League’s gameplay involved teams of six engaging in 6v6 combat across various maps and objectives.\nThe game categorizes characters into three roles:\n\nTanks: These characters are the front line of every team. Tanks make space, absorb incoming damage, and set up plays for their teammates.\nDamage: These characters are the follow-up to the front line. They are either launching rockets from behind cover or deep behind enemy lines, taking out their support.\nSupport: These characters usually take a back seat when eliminating the enemy team; by healing their team, they ensure their front-liners can handle whatever comes at them without getting taken out."
  },
  {
    "objectID": "posts/Final Project/2020OWL-Analysis.html#introduction-to-overwatch-2020",
    "href": "posts/Final Project/2020OWL-Analysis.html#introduction-to-overwatch-2020",
    "title": "QMBE 3740 & CDS 1130 Final Project - 2020 Overwatch League Analysis",
    "section": "",
    "text": "Overwatch is a team-based shooter game with many heroes with different abilities. In 2020, the Overwatch League’s gameplay involved teams of six engaging in 6v6 combat across various maps and objectives.\nThe game categorizes characters into three roles:\n\nTanks: These characters are the front line of every team. Tanks make space, absorb incoming damage, and set up plays for their teammates.\nDamage: These characters are the follow-up to the front line. They are either launching rockets from behind cover or deep behind enemy lines, taking out their support.\nSupport: These characters usually take a back seat when eliminating the enemy team; by healing their team, they ensure their front-liners can handle whatever comes at them without getting taken out."
  },
  {
    "objectID": "posts/Final Project/2020OWL-Analysis.html#overwatch-league-2020-season-analysis",
    "href": "posts/Final Project/2020OWL-Analysis.html#overwatch-league-2020-season-analysis",
    "title": "QMBE 3740 & CDS 1130 Final Project - 2020 Overwatch League Analysis",
    "section": "Overwatch League: 2020 Season Analysis",
    "text": "Overwatch League: 2020 Season Analysis\nThe analysis focuses on the 2020 Overwatch League season, played in Overwatch’s original format with six-player teams. This season is notable for its strategic and player dynamics, providing insight into player performances and tactics before Overwatch transitioned to a 5v5 format in its sequel, Overwatch 2.\nThe shift to 5v5 in Overwatch 2, particularly impacting the role and balance of Tank characters, offers an exciting contrast to the 2020 season data.\n\nPlayer Representation\nThe map below represents player nationality, of which most are from South Korea. We can see that most players are either from East Asian countries or Western Countries.\n\n\nCode\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\n\nfile_path = 'overwatch_league_player_nationalities_updated.csv'\nplayers_data = pd.read_csv(file_path)\n\nif (players_data['Representation'] % 1 == 0).all():\n    players_data['Representation'] = players_data['Representation'].astype(int)\nelse:\n    players_data['Representation'] = players_data['Representation'].astype(float)\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nworld_data = world.merge(players_data, left_on='name', right_on='Country / Region', how='left')\n\nfig = px.choropleth(world_data,\n                    geojson=world_data.geometry,\n                    locations=world_data.index,\n                    color=\"Representation\",\n                    color_continuous_scale=\"Viridis\",\n                    labels={'Representation':'Number of Players'})\nfig.update_geos(fitbounds=\"locations\")"
  },
  {
    "objectID": "posts/Final Project/2020OWL-Analysis.html#machine-learning",
    "href": "posts/Final Project/2020OWL-Analysis.html#machine-learning",
    "title": "QMBE 3740 & CDS 1130 Final Project - 2020 Overwatch League Analysis",
    "section": "Machine Learning",
    "text": "Machine Learning\n\n\nCode\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\ndata = pd.read_csv(\"merged_esports_data_updated.csv\")\ndata = data.fillna(data.median(numeric_only=True))\ndata = pd.get_dummies(data, columns=[\"map_type\", \"map_name\", \"player_name\", \"team_name\", \"hero_name\"])\ndata = data.drop(columns=[\"team_one_name\", \"team_two_name\", \"match_id\"])\nX = data.drop(\"match_winner\", axis=1)\ny = data[\"match_winner\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nlr_model = LogisticRegression().fit(X_train, y_train)\nrf_model = RandomForestClassifier(n_estimators=50, random_state=42).fit(X_train, y_train)\ngb_model = GradientBoostingClassifier(n_estimators=100, random_state=42).fit(X_train, y_train)\n\npredictions_lr = lr_model.predict(X_test)\npredictions_rf = rf_model.predict(X_test)\npredictions_gb = gb_model.predict(X_test)\n\nresults = pd.DataFrame({\n    \"y_test\": y_test,\n    \"predictions_lr\": predictions_lr,\n    \"predictions_rf\": predictions_rf,\n    \"predictions_gb\": predictions_gb\n})\nresults.to_csv(\"model_results.csv\", index=False)\n\nfeature_importances = pd.DataFrame({\n    \"feature\": X.columns,\n    \"importance_rf\": rf_model.feature_importances_,\n    \"importance_gb\": gb_model.feature_importances_\n})\nfeature_importances.to_csv(\"feature_importances.csv\", index=False)\n\n\nTo better understand which features are most relevant to a team winning a game, we will train and test machine learning models to predict which team will win based on the dataset’s features.\nFrom analyzing the importance, accuracy, sensitivity, and specificity of each model’s feature, we can see that the Random Forest model does the best job of correctly predicting which team will win.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(reshape2)\n\nresults &lt;- read.csv(\"model_results.csv\")\nfeature_importances &lt;- read.csv(\"feature_importances.csv\")\n\ntop_features_rf &lt;- feature_importances %&gt;%\n                   arrange(desc(importance_rf)) %&gt;%\n                   head(10)\n\ntop_features_gb &lt;- feature_importances %&gt;%\n                   arrange(desc(importance_gb)) %&gt;%\n                   head(10)\n\ncm_lr &lt;- confusionMatrix(as.factor(results$predictions_lr), as.factor(results$y_test))\nmetrics_lr &lt;- data.frame(Model = \"Logistic Regression\", Accuracy = cm_lr$overall['Accuracy'], Sensitivity = cm_lr$byClass['Sensitivity'], Specificity = cm_lr$byClass['Specificity'])\n\ncm_rf &lt;- confusionMatrix(as.factor(results$predictions_rf), as.factor(results$y_test))\nmetrics_rf &lt;- data.frame(Model = \"Random Forest\", Accuracy = cm_rf$overall['Accuracy'], Sensitivity = cm_rf$byClass['Sensitivity'], Specificity = cm_rf$byClass['Specificity'])\n\ncm_gb &lt;- confusionMatrix(as.factor(results$predictions_gb), as.factor(results$y_test))\nmetrics_gb &lt;- data.frame(Model = \"Gradient Boosting\", Accuracy = cm_gb$overall['Accuracy'], Sensitivity = cm_gb$byClass['Sensitivity'], Specificity = cm_gb$byClass['Specificity'])\n\ncombined_metrics &lt;- rbind(metrics_lr, metrics_rf, metrics_gb)\nmelted_metrics &lt;- melt(combined_metrics, id.vars = \"Model\")\n\nggplot(melted_metrics, aes(x = variable, y = value, fill = variable)) +\n    geom_bar(stat = \"identity\", position = position_dodge()) +\n    facet_wrap(~Model) +\n    scale_fill_manual(values = c(\"Accuracy\" = tank, \"Sensitivity\" = damage, \"Specificity\" = support)) +\n    labs(x = \"Metric\", y = \"Value\", title = \"Model Performance Comparison\") +\n    theme_minimal()\n\n\n\n\n\n\n\nA Random Forest Model is a “forest” of decision trees that take different features of the dataset, draw a line between win and lose, and then categorize any data we give it until it is correct most of the time.\nFor example, the decision trees could determine that more than 90 seconds of average alive time, more than 40 seconds of objective time, and more than ten eliminations in a game mean that someone is more likely to win. Continuing this example, a hero with 100 seconds of average alive time, 41 seconds of objective time, and nine eliminations, the Random Forest would say this is most likely to win, as average alive time and accurate time outweigh eliminations.\n\n\nCode\nggplot(top_features_rf, aes(x = reorder(feature, importance_rf), y = importance_rf)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip() +\n    labs(title = \"Top 10 Feature Importances - Random Forest\")\n\n\n\n\n\nThe Random Forest Model uses in-game statistics to predict the winning team. In contrast, the Gradient Boosting model favors a specific team in its selections, which is unhelpful in the long term when rosters change. Logistic regression demonstrated an inability to accurately identify true negatives, also seen in the Gradient Boosting model. Therefore, considering the criteria of accuracy, realistic true positive identification, and effective true negative prediction, the Random Forest model significantly outperforms the other trained models.\n\n\nCode\nggplot(top_features_gb, aes(x= reorder(feature, importance_gb), y = importance_gb)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip() +\n    labs(title = \"Top 10 Feature Importances - Gradient Boosting\")"
  },
  {
    "objectID": "posts/Final Project/2020OWL-Analysis.html#analyzing-features-based-on-the-random-forests-feature-importance",
    "href": "posts/Final Project/2020OWL-Analysis.html#analyzing-features-based-on-the-random-forests-feature-importance",
    "title": "QMBE 3740 & CDS 1130 Final Project - 2020 Overwatch League Analysis",
    "section": "Analyzing features based on the Random Forest’s Feature Importance",
    "text": "Analyzing features based on the Random Forest’s Feature Importance\n\nAverage Time Alive\n\n\nCode\ntime_alive_data &lt;- filter(data, stat_name == \"Average Time Alive\")\navg_time_alive_by_hero &lt;- time_alive_data %&gt;%\n    group_by(hero_name, role) %&gt;%\n    summarise(average_time_alive = mean(stat_amount, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;%\n    arrange(desc(average_time_alive))\n\nordered_heroes &lt;- rev(avg_time_alive_by_hero$hero_name)\ntime_alive_data$hero_name &lt;- factor(time_alive_data$hero_name, levels = ordered_heroes)\n\nggplot(time_alive_data, aes(x = hero_name, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    coord_flip() +\n    labs(title = \"Boxplot of Average Time Alive by Hero\", x = \"Hero\", y = \"Average Time Alive\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support)) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nLooking at this box plot, we can see that Sombra has the highest Average Time Alive of any hero. This is likely because she can use her translocator, a device she leaves on the floor somewhere and can instantly teleport back to at any time. Symmetra is the lowest on the list. This is most likely because Symmetra is picked for a couple of seconds at the beginning of many rounds, as her placable teleporter allows teams to quickly reposition right out of the gates.\n\n\nAverage Objective Time\n\n\nCode\nobjective_time_data &lt;- filter(data, stat_name == \"Objective Time\")\nobjective_time_by_hero &lt;- objective_time_data %&gt;%\n    group_by(hero_name, role) %&gt;%\n    summarise(average_objective_time = mean(stat_amount, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;%\n    arrange(desc(average_objective_time))\n\nordered_heroes_objective &lt;- rev(objective_time_by_hero$hero_name)\nobjective_time_data$hero_name &lt;- factor(objective_time_data$hero_name, levels = ordered_heroes_objective)\n\nggplot(objective_time_data, aes(x = hero_name, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    coord_flip() +\n    labs(title = \"Boxplot of Objective Time by Hero\", x = \"Hero\", y = \"Objective Time\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support))\n\n\n\n\n\n\n\nThis Average Objective Time plot shows that Orisa, Reinhardt, and D.Va have the highest average objective time this season. This aligns with the Tank’s role to create space for their team, including taking and holding accurate areas. Heroes like Widowmaker, who rely on effective off-angle positioning, are likelier to be near but not on the objective.\n\n\nAverage Healing Done\n\n\n\n\n\nIn this healing chart, we can see that it is almost exclusively supports. This is because, except for Soldier 76, supports are the only heroes that can heal their teammates. Moira being at the top of this list does not surprise me, as she was one of the strongest picks this season since she dealt damage and healed very effectively.\n\n\nAverage Eliminations\n\n\nCode\naverage_eliminations_by_hero &lt;- eliminations_data %&gt;%\n    group_by(hero_name, role) %&gt;%\n    summarise(average_eliminations = mean(stat_amount, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;%\n    arrange(desc(average_eliminations))\nordered_heroes_eliminations &lt;- rev(average_eliminations_by_hero$hero_name)\neliminations_data$hero_name &lt;- factor(eliminations_data$hero_name, levels = ordered_heroes_eliminations)\n\nggplot(eliminations_data, aes(x = hero_name, y = stat_amount, fill = role)) +\n    geom_boxplot() +\n    coord_flip() +\n    labs(title = \"Boxplot of Eliminations by Hero\", x = \"Hero\", y = \"Number of Eliminations\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"Tank\" = tank, \"Damage\" = damage, \"Support\" = support))\n\n\n\n\n\n\n\nIn this graph, we can see that Damage and Tank heroes generally trend toward the higher end of the chart, while Supports, who are mainly focused on healing and keeping their team alive, spend less time getting eliminations. Moira stands out as a support in the number 3 spot, which can be explained by her kit, which requires Moira to deal damage to regenerate her healing capabilities."
  },
  {
    "objectID": "posts/Final Project/2020OWL-Analysis.html#comparing-our-features-next-to-each-other",
    "href": "posts/Final Project/2020OWL-Analysis.html#comparing-our-features-next-to-each-other",
    "title": "QMBE 3740 & CDS 1130 Final Project - 2020 Overwatch League Analysis",
    "section": "Comparing our features next to each other",
    "text": "Comparing our features next to each other\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nowl2020_data = pd.read_csv('owl2020data.csv')\nhero_roles = {\n    \"D.Va\": \"Tank\", \"Reinhardt\": \"Tank\", \"Winston\": \"Tank\", \"Roadhog\": \"Tank\", \"Zarya\": \"Tank\", \n    \"Orisa\": \"Tank\", \"Sigma\": \"Tank\", \"Wrecking Ball\": \"Tank\", \"Doomfist\": \"Damage\", \n    \"Genji\": \"Damage\", \"McCree\": \"Damage\", \"Pharah\": \"Damage\", \"Reaper\": \"Damage\", \n    \"Soldier: 76\": \"Damage\", \"Sombra\": \"Damage\", \"Tracer\": \"Damage\", \"Bastion\": \"Damage\", \n    \"Hanzo\": \"Damage\", \"Junkrat\": \"Damage\", \"Mei\": \"Damage\", \"Torbjörn\": \"Damage\", \n    \"Widowmaker\": \"Damage\", \"D.Va\": \"Tank\", \"Orisa\": \"Tank\", \"Reinhardt\": \"Tank\", \n    \"Roadhog\": \"Tank\", \"Winston\": \"Tank\", \"Wrecking Ball\": \"Tank\", \"Zarya\": \"Tank\", \n    \"Ana\": \"Support\", \"Baptiste\": \"Support\", \"Brigitte\": \"Support\", \"Lúcio\": \"Support\", \n    \"Mercy\": \"Support\", \"Moira\": \"Support\", \"Zenyatta\": \"Support\"\n}\nowl2020_data['role'] = owl2020_data['hero_name'].map(hero_roles)\ncustom_palette = {\n    \"Tank\": \"#ff7eb6\",\n    \"Damage\": \"#4589ff\",\n    \"Support\": \"#3ddbd9\"\n}\n\nplt.figure(figsize=(18, 6))\n\nplt.subplot(1, 3, 1)\nsns.scatterplot(x='average_time_alive', y='eliminations', hue='role', data=owl2020_data, palette=custom_palette)\nplt.title('Average Time Alive vs. Eliminations')\nplt.xlabel('Average Time Alive (sec)')\nplt.ylabel('Eliminations')\n\nplt.subplot(1, 3, 2)\nsns.scatterplot(x='objective_time', y='eliminations', hue='role', data=owl2020_data, palette=custom_palette)\nplt.title('Objective Time vs. Eliminations')\nplt.xlabel('Objective Time (sec)')\nplt.ylabel('Eliminations')\n\nplt.subplot(1, 3, 3)\nsns.scatterplot(x='average_time_alive', y='objective_time', hue='role', data=owl2020_data, palette=custom_palette)\nplt.title('Average Time Alive vs. Objective Time')\nplt.xlabel('Average Time Alive (sec)')\nplt.ylabel('Objective Time (sec)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThese scatter plots compare the features of Average Time Alive, Objective Time, and Eliminations.\n\nAverage Time Alive - Eliminations\nThis plot shows a distribution of data points representing different roles concerning how long they stay alive (in seconds) and how many eliminations they achieve. We can see that Tank heroes can survive longer and get more eliminations as they are durable in fights. Damage heroes have a widespread variability as they are less stable but can pick up more eliminations. Supports have fewer eliminations but a higher time alive, which is a nod to their supportive play style.\n\n\nObjective Time - Eliminations\nThis plot presents the relationship between time spent capturing objectives (in seconds) and the number of eliminations. The damage role again shows a high degree of variability in both metrics, Tank roles less so, and Support roles even less, which is consistent with Overwatch’s in-game roles where Damage roles aim for eliminations, Tanks control objective space, while Supports assist.\n\n\nAverage Time Alivee - Objective Time\nThis plot shows the average time a player stays alive with the time they spend securing objectives. There’s a dense cluster of Support and Tank roles with high objective times, which aligns with Overwatch’s role identities that prioritize the objective over seeking eliminations. Damage roles are more dispersed, which tells us they balance engaging in fights and playing the objective."
  },
  {
    "objectID": "posts/Final Project/2020OWL-Analysis.html#summary",
    "href": "posts/Final Project/2020OWL-Analysis.html#summary",
    "title": "QMBE 3740 & CDS 1130 Final Project - 2020 Overwatch League Analysis",
    "section": "Summary",
    "text": "Summary\nOverall, this analysis has given us a good understanding of which players and heroes performed the best throughout the 2020 Overwatch League Season. If this were a game played by robots, we could now accurately predict which team would win every time and which heroes should be picked. Alas, many things cannot be evaluated by the statistics tracked in the game, such as the decision-making skills of the in-game leaders, the individual decisions that all 12 players are making in the heat of the moment, and the abilities that cannot be well measured, like Lucio’s speed boost."
  }
]