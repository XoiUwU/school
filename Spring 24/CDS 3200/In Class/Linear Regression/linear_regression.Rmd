---
title: "Linear Regression"
author: "Jasper Weinburd"
date: "`r Sys.Date()`"
output: openintro::lab_report
---

This is some text.

```{r load-packages, message=FALSE}
library(tidyverse)
library(openintro)
library(ISLR2)
# be sure both of the packages below are installed by running the following in the console:
# install.packages("statsr")
# install.packages("broom")
library(statsr)
library(broom)
library(plotly)
library(pracma)
library(GGally)
```

The data, loaded from the textbook's website

```{r}
Advertising <- read.csv('https://www.statlearning.com/s/Advertising.csv')
glimpse(Advertising)
```

### "Simple" Linear Regression

```{r, message = FALSE, warning = FALSE}
line1 <- lm(sales ~ TV, data = Advertising)
line1

plot(Advertising$TV, Advertising$sales)
abline(line1)

ggplot(data = Advertising, aes(x = TV, y = sales)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

We can use the model to make predictions:

```{r}
predict(line1, data.frame(TV = 200))
predict(line1, data.frame(TV = c(100, 200, 300)) )
```

More information about the model:

```{r}
summary(line1)
tidy(line1)
glance(line1)
```

Remember that:

* $H_0$: There is no relationship between `TV` and `sales`, i.e. $\beta_1 = 0$
* $H_1$: There is a relationship, i.e. $\beta_1 \neq 0$

Based on this model, would we reject the null hypothesis? **YES, because that's a very small p-value.**

To compute confidence intervals for the coefficients:

```{r}
confint(line1, level = 0.95)
```

Visualizing a confidence interval for both coefficients:

```{r}
ggplot(data = Advertising, aes(x = TV, y = sales)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE)
```

```{r}
pred_vals <- predict(line1, data.frame(TV = 200), interval = "prediction")
pred_vals
```

Visualizing a *prediction* interval:

```{r}
pred_vals <- predict(line1, interval = "prediction")
Ad_pred <- cbind(Advertising, pred_vals)

ggplot(data = Ad_pred, aes(x = TV, y = sales)) +
  geom_point() +
  geom_line( aes(y = lwr), color = "red", linetype = "dashed") +
  geom_line( aes(y = upr), color = "red", linetype = "dashed") +
  geom_smooth(method = "lm", se = TRUE)
```

### Comparing Models

Remember that our first model `line1` is
$$ \texttt{sales} = \hat \beta_0 + \hat \beta_1 \times \texttt{TV}$$

```{r}
summary(line1)
ggplot(data = Advertising, aes(x = TV, y = sales)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

Let's create a second model `line2` that predicts `sales` based on radio ads
$$ \texttt{sales} = \hat \beta_0 + \hat \beta_1 \times \texttt{radio}$$
Which model fits the data better?

```{r, message = FALSE, warning = FALSE}
line2 <- lm(sales ~ radio, data = Advertising)
summary(line2)

ggplot(data = Advertising, aes(x = radio, y = sales)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

Can we predict the `sales` with two variables?

```{r, echo = FALSE, warning = FALSE}
Advertising %>%
  plot_ly(x = ~TV, y = ~radio, z = ~sales, type="scatter3d", mode = "markers", size = .01)
```

### Multiple Linear Regression

```{r}
plane <- lm(sales ~ TV + radio, data = Advertising)
plane
```

```{r, message = FALSE, warning = FALSE, echo = FALSE}
m2 <- plane

mesh_size <- 1
margin <- 0

attach(Advertising)
x_min <- min(TV)
x_max <- max(TV)
y_min <- min(radio,na.rm = TRUE)
y_max <- max(radio,na.rm = TRUE)
xrange <- seq(x_min, x_max, mesh_size)
yrange <- seq(y_min, y_max, mesh_size)
xy <- meshgrid(x = xrange, y = yrange)
xx <- xy$X
yy <- xy$Y
dim_val <- dim(xx)
xx1 <- matrix(xx, length(xx), 1)
yy1 <- matrix(yy, length(yy), 1)
final <- data.frame(TV = xx1, radio = yy1)

pred <- m2 %>%
  predict(final)
#pred <- pred$.pred
pred <-matrix(pred, dim_val[1], dim_val[2])

fig3 <- plot_ly(Advertising, x = ~TV, y = ~radio, z = ~sales, type = "scatter3d", mode = "markers", name = "personal freedom", size = 0.01) %>%
  add_surface(x=xrange,y=yrange,z=pred,alpha=0.05,type="mesh3d",name="regression_fit") %>%
  layout(scene = list(aspectmode = "manual")) #%>%
  #add_trace(data = xy, x = ~pf_expression_control, y = ~pf_score, name = "regression fit", mode = "lines", alpha = 1, size = 0.001)

fig3
```

Mathematically, this model corresponds to the *linear* equation
Let's create a second model `line2` that predicts `sales` based on `radio` ads
$$ \texttt{sales} = \hat \beta_0 + \hat \beta_1 \times \texttt{TV} + \hat \beta_2 \times \texttt{radio}$$

```{r}
summary(plane)
```

Why stop at two predictors, we also have the data for `newspaper` ads.

```{r}
model3 <- lm(sales ~ TV + radio + newspaper, data = Advertising)
summary(model3)
```

Importantly, we haven't improved the model fit by much. Why is this? One variable is not considered significant.

```{r}
cor(Advertising[,2:5])
pairs(Advertising[,2:5])
```

#### Is there a Relationship Between the Response and Predictors?

* $H_0$: There is no relationship, i.e. $\beta_1 = \beta_2 = \beta_3 = 0$
* $H_1$: There is a relationship, i.e. at least one $\beta_i \neq 0$

```{r}
summary(model3)
```

#### Which Variables are truly Important Predictors?

* Backward Selection
* Forward Selection
* Mixed Selection

Backward Selection:

1. Choose a criterion, e.g. p-value, and corresponding stopping criterion, e.g. "all p-values are less than 0.05".
2. Starting from the full model, we check the criterion for each predictor.
    * the *full model* includes all predictors for which we have data.
3. Remove the predictor with the "worst" criterion, e.g. largest p-value.
4. Repeat steps 2 and 3 until the stopping criterion is reached.

How would you modify this procedure to use the $R^2$ statistic instead of p-value?

#### How Well Does the Model Fit the Data?

When choosing between candidate models, check the model fit using $R^2$ and the $RSE$.

```{r}
glance(plane)
glance(model3)
```

#### Given a Set of Predictor Values, what Response Value do we Predict?

We can construct confidence intervals for the coefficients:

```{r}
confint(plane, level = 0.95)
```

We can also make predictions, which always fall on the plane:

```{r, warning=FALSE}
predict(plane, data.frame(TV = 200, radio = 30) )
predict(plane, data.frame(TV = c(100, 200, 300), radio = c(20, 30, 40)) )
fig3
```

### Potential Problems

We'll now check the potential problems for our model `line1`.

A few of these will rely on the idea of a residual plot:

```{r}
ggplot(data = Advertising, aes(x = TV, y = sales)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)+
  geom_segment(aes(xend = TV, yend = fitted(line1)), 
               linetype = "dashed", color = "red")

ggplot(data = Advertising, aes(x = TV, y = residuals(line1)) ) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_segment(aes(xend = TV, yend = 0), 
               linetype = "dashed", color = "red")

ggplot(data = Advertising, aes(x = fitted(line1), y = residuals(line1)) ) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_segment(aes(xend = fitted(line1), yend = 0), 
               linetype = "dashed", color = "magenta")
```

a. Non-linearity
b. Correlation of error terms
c. Non-constant variance of error terms
d. Not Normal Residuals
e. Outliers
f. High-leverage points
g. Collinearity

#### Non-linearity

```{r}
ggplot(data = Advertising, aes(x = TV, y = sales)) +
  geom_point() +
  geom_smooth(se = FALSE)

ggplot(data = Advertising, aes(x = TV, y = residuals(line1))) +
  geom_point() +
  geom_smooth(se = FALSE)
```

#### Correlation of error terms & Non-constant variance of error terms

```{r}
ggplot(data = Advertising, aes(x = fitted(line1), y = residuals(line1)) ) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_segment(aes(xend = fitted(line1), yend = 0), 
               linetype = "dashed", color = "magenta")
```

#### (Very) Not normal residuals

```{r}
ggplot(data = Advertising, aes(x = residuals(line1)) ) +
  geom_histogram()
```

#### Outliers & High Leverage Points

Examine scatter plot of the data and/or residual plot.

#### Colinearity of Predictors

Since our model `line1` only has a single predictor, we won't have this problem. However, we did check this earlier to explain why `newspaper` was not statistically significant in the full model `model3`.

```{r}
ggpairs(Advertising[,2:5])
```
