---
title: "Lab 2: Linear Regression"
author: "Your Name"
date: "`r Sys.Date()`"
output: openintro::lab_report
---

Before beginning this lab, you may want to read or skim Section 3.6 in the textbook, or watch [this video*](https://www.youtube.com/watch?v=W0WILgmAGao&list=PL5-da3qGB5IBSSCPANhTgrw82ws7w_or9&index=6). That section includes lots of helpful commands to complete the exercises in this lab. You may also like to check out the file `linear_regression.Rmd` available in the Week 2 module on Canvas.

*Note that the authors use a regular "R Script" rather than a RMarkdown file (which is what this file is). Also, many of their graphics are produced with "base R" rather than the prettier `ggplot` commands we learned in the first lab.

---

```{r load-packages, message=FALSE}
library(tidyverse)
library(openintro)
library(ISLR2)
library(broom)
library(GGally) #ex4
library(lmtest) #ex3

```

This lab will use the `Auto` data set, which is included in the ISLR2 R-package. Use the command `names()` to find the variables of the data set. Try the command `?Auto` to see a full description of the data set and every variable -- this command must be used in the console.

```{r}
names(Auto)
```

## Simple Linear Regression
<!-- Exercise 3.8, page 123 -->

### Exercise 1

Use the `lm()` function to perform a simple linear regression with `mpg` as the response and `horsepower` as the predictor. Use the `summary()` and `tidy()` functions to print the results.

For the `tidy()` function, you'll need to load the `broom` package -- best practice is to add it to the other `library()` commands at the top of your RMarkdown document.

```{r 1}
ex1 <- lm(mpg ~ horsepower, data = Auto)
summary(ex1)
print(tidy(ex1))
```

Use your model to answer the following questions:

a. Is there a statistically significant relationship between the predictor and the response?

Yes there is a significant relationship between horsepower and mpg.

b. How strong is the relationship between the predictor and the response?

The F-statistic is high in this model with a low p-value. This indicates a statistically significant relationship between the model and `horsepower`

c. Is the relationship between the predictor and the response positive or negative?

It is negative. We can expect that as the horsepower goes up, the mpg goes down.

d. What is the predicted `mpg` associated with a `horsepower` of 98? What are the associated 95% confidence and prediction intervals?

The predicted `mpg` for a car with 98 `horsepower` is 24.46. The prediction intervals for 95% confidence is (23.97 and 24.96). This means that the model is 95% confident that for all cars with 98 `horsepower`, the `mpg` is going to be somewhere between 23.97 and 24.96.

```{r 1d}
predict(ex1, newdata = data.frame(horsepower = 98), interval = "confidence")
```

### Exercise 2

Make two plots to visualize your regression model:

a. Plot the response and the predictor using base R. Use the `abline()` function to display the least squares regression line.
b. Use `ggplot` and the `geom_smooth` layer.

```{r 2}
ggplot(Auto, aes(x = horsepower, y = mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

### Exercise 3

You will now evaluate the least squares regression fit of your model. For each of the following "Potential Problems", compute a measurement statistic, produce a plot, or both. Then comment on whether this might be an issue for your model from above. (You can review these on page 92 of our textbook.)

```{r ex3}
ex3 <- lm(mpg ~ horsepower, data = Auto)
```

a. Non-linearity

The data points appear to have an upwards curve meaning that the model is non-linear.

c. Non-constant variance of error terms

The model has some constant variance on the right but not as much on the left. This would tell us that the model has non-constant variance of error terms.

```{r 3ac}
Auto$residuals <- resid(ex3)
Auto$fitted.values <- fitted(ex3)
ggplot(Auto, aes(x = fitted.values, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "#d02670")
```

b. Correlation of error terms

The Durbin-Watson test has a value of 0.92 with a very low p-value. This tells us that there is a high positive autocorrelation between the residuals. Our graph doesn't show us any trends but because of the Durbin-Watson test, we know that there is mostly likely a large underlying trend not obvious to our eye.

```{r 3b}
dwtest(ex3)

Auto$residuals <- resid(ex3)
Auto$lagged_residuals <- c(NA, Auto$residuals[-length(Auto$residuals)])

ggplot(Auto, aes(x = lagged_residuals, y = residuals)) +
  geom_point(na.rm = TRUE) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "#d02670") +
  labs(x = "Lagged Residuals", y = "Residuals", title = "Residuals vs. Lagged Residuals")
```

d. Not Normal Residuals

Looking at the Q-Q graph shows us that the residuals are mosty normal in the middle of the graph. However they deviate from the line at the ends. This tells us that the graph has heavy tails, which could be a sign of many outliers.

```{r 3d}
ggplot(Auto, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line()
```

e. Outliers

Looking at the left and right graph, we can confirm what we saw in the previous graph. Towards the middle the points are closest to the line, and the tails have many outliers.

```{r 3e}
ggplot(Auto, aes(x = fitted.values, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "#d02670") +
  labs(x = "Fitted Values", y = "Residuals")
```

f. High-leverage points

From this graph we can find that most cars have low leverage and the subset that have higher horsepower also have higher leverage. This could be a bit of a problem since our residuals are higher at a larger horsepower meaning that our model is slighly affected by outliers.

```{r 3f}
Auto$leverage <- hatvalues(ex3)
ggplot(Auto, aes(x = horsepower, y = leverage)) +
  geom_point() +
  labs(x = "Horsepower", y = "Leverage")
```

g. Collinearity

This model only has one predictor and therefore doesnt haven any collinearity with another predictor.

## Multiple Linear Regression
<!-- Exercise 3.9, page 123-4 -->

### Exercise 4

Produce a scatterplot matrix which includes all of the variables in the data set. Also, compute the matrix of correlations between all of the variables.

```{r 4a}
pairs(Auto)

#correlation matrix
cor(Auto[ , sapply(Auto, is.numeric)])
```

Now try the command `ggpairs()` instead.

```{r 4b}
Auto_no_name <- Auto %>% select(-name)
# name was giving me an error because high cardinarlity
ggpairs(Auto_no_name)
```

### Exercise 5

Use the `lm()` function to perform a multiple linear regression with `mpg` as the response and all other variables except `name` as the predictors. Print the results and use them to answer the following:

a. Is there a statistically significant relationship between the predictors and the response as a whole?

The high F-statistic with the low p-value tells us that the predictors as a whole have a statistically significant relationship with `mpg`.

b. Which predictors appear to have a statistically significant relationship to the response in this model?

The predictors that have a p-value less than 0.05 and therefore a statistically significant relationship with `mpg` are `displacement`, `weight`, `year`, and `origin`.

c. What does the coefficient for the `year` variable suggest?

The Estimate Std. for `year` is 0.75 which means that on average, cars become more fuel efficient by 0.75 mpg every year.

```{r 5}
ex5 <- lm(mpg ~ . -name, data = Auto)
summary(ex5)
```

### Exercise 6

There is a problem with your multiple linear regression model. Examine the `origin` variable and it's description (use `?Auto` in the console) to diagnose the problem.

Replace occurences of "origin#" in the code below to create a new variable that accurately encodes the categorical nature of `origin`.

```{r 6}
Auto6 <- Auto %>%
  mutate(origin_cat = as.factor(origin))
```

Explore your new variable with the `contrasts()` function, replace your model from Exercise 7 with a new multiple regression model that incorporates your new variable instead of `origin` as a predictor.

### Exercise 7

Repeat the steps in Exercise 3, above to evaluate the least squares regression fit of your model with respect to each of the following "Potential Problems":

```{r 7}
ex7 <- lm(mpg ~ displacement + weight + year + origin, data = Auto)
```

a. Non-linearity

As we can see in the graph, the model is definetly non linear which can be found from the spread of the residuals.

```{r 7a}
Auto$residuals7 <- resid(ex7)
Auto$fitted.values7 <- fitted(ex7)

ggplot(Auto, aes(x = fitted.values7, y = residuals7)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "#d02670") +
  labs(title = "Residuals vs. Fitted Values", x = "Fitted Values", y = "Residuals")

```

b. Correlation of error terms

Compared to exercise 3, the Durbin-Watson test has a slighly higher value. This still means that there is a positive autocorrelation in the residuals, but not as much this time.

```{r 7b}
dwtest(ex7)
```

c. Non-constant variance of error terms

From this graph we can tell that the model has a high amount of non-constant variance of error tems. A low amount of non-constant variance would be represented if all the data points were along the red line. Our graph has data points all over the place which tells us it is high.

```{r 7c}
ggplot(Auto, aes(x = fitted.values, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "#d02670") +
  labs(title = "Heteroscedasticity Check: Residuals vs. Fitted Values", x = "Fitted Values", y = "Residuals")
```

d. Not Normal Residuals

This Q-Q graph tells us that the residuals are mostly normal. However again, there are heavy tails at each end which is a sign of outliers that could cause not normal residuals.

```{r 7d}
ggplot(Auto, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "Normal Q-Q Plot of Residuals")
```

e. Outliers

Looking to the left and the right of the graph, we can once again confirm our assumptions made from the previous graph. The center has the least amount of outliers while the left and right sides of the graph have many outliers.

```{r 7e}
ggplot(Auto, aes(x = fitted.values, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "#d02670") +
  labs(title = "Outliers Check: Residuals vs. Fitted Values", x = "Fitted Values", y = "Residuals")
```

f. High-leverage points

Unlike exercise 3, this model has very low leverage across the board with a few outliers. It may be nessissary, however, to remove the data point for the very far outlier to create a simpiler model that is unaffected by the outlier.

```{r 7f}
Auto$leverage7 <- hatvalues(ex7)

ggplot(Auto, aes(x = row.names(Auto), y = leverage7)) +
  geom_point() +
  labs(title = "High-leverage Points", x = "Observation", y = "Leverage")
```

g. Collinearity

From the plot of plots, we can see that the highest collinearity comes from displacement and weight in a positive relationship. As weight increases, displacement also increases. While year and weight do appear to have some relationship, it is not linear, same as for year and displacement.

```{r 7g}
pairs(~displacement + weight + year + origin, data = Auto, main = "Pairwise Scatterplots of Predictors")
```

### Exercise 8

Use the `*` or `:` symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?

```{r 8}
ex8 <- lm(mpg ~ horsepower*weight, data = Auto6)
summary(ex8)
```

### Exercise 9

Try a few different transformations of variable such as $\log(X), \sqrt{X}, X^2$. Comment on your findings.

```{r 9}
ex9_log <- lm(log(mpg) ~ log(horsepower), data = Auto6)
summary(ex9_log)

ex9_sqrt <- lm(sqrt(mpg) ~ sqrt(horsepower), data = Auto6)
summary(ex9_sqrt)

ex9_squared <- lm(I(mpg^2) ~ I(horsepower^2), data = Auto6)
summary(ex9_squared)
```

### Exercise 10

Write a function that takes a data frame as input, runs a linear regression, and outputs diagnostic plots to help in evaluating the "Potential Problems" of Exercises 3 and 7.

```{r 10}
run_regression_diagnostics <- function(df, formula) {
  model <- lm(formula, data = df)
  print(summary(model))
  par(mfrow = c(2, 2))
  plot(model)
}

# Example usage:
run_regression_diagnostics(Auto, mpg ~ horsepower)
```
