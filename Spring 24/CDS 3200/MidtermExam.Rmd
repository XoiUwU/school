---
title: "CDS 3200 - Midterm Exam"
author: "Xander"
date: "`r Sys.Date()`"
output: openintro::lab_report

---

<details>

<summary>Instructions</summary>

If you have any questions at all, please don't hesitate to reach out to me! If you believe I may have made a typo, please don't hesitate to reach out to me! If you would like to discuss the exam, please don't hesitate to come to [Office Hours](https://canvas.hamline.edu/courses/17135/pages/office-hours-2-2)! You can certainly discuss the exam with our tutors as well.

* The exam has 5 questions, all worth 25 points. You **must answer Q1 and Q2** (Conceptual), but you can **choose any two** of Q3, Q4, and Q5 (Applied). You will not earn extra credit for completing all the problems.

* The exam is a take home exam. You may spend as much or as little time as you like, but you must submit the exam by **6pm on Friday 3/8**.

* If you have trouble uploading your exam to Canvas, email both the `.Rmd` and `.html` files to Prof Jasper instead.

* This is an open-notes exam. This means that:

  * Your answers should be written in your own words, explained thoroughly, and refer to figures or R output that support your conclusions.

  * You **may** use any of the course materials including your own notes, the text book, your homework or lab assignments and related feedback/comments, and other materials provided on Canvas such as lecture notes, the textbook, and the videos from the textbook authors.

  * You **may** use R and RStudio including its help functions and packages.

  * You **may** look for additional help with R syntax (the specific form and grammar of commands) using general internet searches.

  * You **may not** use the internet to search for solutions, examples, or general ideas that answer the questions on the exam.

  * You **may not** discuss this exam with other people besides Prof Jasper.

* The list above creates a tempting gray area, but I am confident in each of you to follow the spirit and letter of Hameline's [Academic Honor Code](http://bulletin.hamline.edu/content.php?catoid=26&navoid=1060#aca_honorcode).


Please include here any libraries used in your exam:

```{r}
library(ISLR2)
library(ggplot2)
library(gridExtra)
library(caret)
library(broom)
```

</details>

## Questions

### Q1 (25 points)

 Please answer each question below with a short paragraph or two that explains the major concepts. When asked for an example, feel free to reuse examples from class or the textbook. Remember that your answer should be in your own words, but can refer to course materials as needed.

a. What is the difference between supervised and unsupervised learning? Give an example of each.

* For supervised learning, the model learns to map an input to an output, so it can predict the output on future data. Some supervised models include: linear regression, logistic regression, random forests, and neural networks.
* On the other hand, unsupervised learning models don't explicitly assign outputs. Instead of predicting the output of future data, the objective of unsupervised learning models is to discover underlying patterns without being confined to find a specific outcome. Some unsupervised models are: K-Means Clustering and Hierarchical Clustering.

b. What is bias-variance tradeoff? How does it relate to overfitting and underfitting?

* Bias is the error from missing relationships between features known as underfitting. Variance is the error from teaching your model too close to it's dataset, causing it to connect random noise together causing overfitting.
* The bias-variance tradeoff is trying to find a balance between the two as a decrease in one usually leads to an increase in the other. Increasing model complexity will reduse bias, but will also mean the model captures more noise.

c. What is the difference between classification and regression problems? Give an example of each.

* Classification is predicting which category a observation belongs to. For example, email services do this to mark messages as spam or not-spam, inbox or important.
* Regression is used to predict a number. For example, the prices of houses based on amenities, forecasting stock prices based on historical data.

d. What is the difference between the logit function and the linear regression model? How do they relate to each other?

* Linear regression is used to predict continuous variables such as house prices or stock prices as discussed earlier.
* The logit function is used to predict categorical variables such as spam message or not spam message.
* These two regression models are similar as they use nearly the same formula. The Linear Regression Formula is $\hat{y}=\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{n}x_{n}$. On the other hand, the logistic regression model, which uses the logit function is $\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1x_1 + \cdots + \beta_nx_n$, where $p$ is the probability of an observation belonging to a specific category. In both formulas, the right side (the linear combination of predictors) is identical. The key difference between the two is the output. Linear regression produces a continuous value, while the logistic regression model outputs a probability that ranges between 0 and 1, which is useful for classification.

### Q2 (25 Points)

  Suppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X. Carefully describe how we might estimate the standard deviation of our prediction. (If you use a method or tool we learned in this class, be sure to explain how you would implement it algorithmically without relying on a function in R.)

* laksjfkalsf

### Q3 (25 Points)

 In this question you'll predict the per capita crime rate using linear regression on the `Boston` data set.

a. For the predictors listed below, fit a simple linear regression model. In which models is there a statistically significan't association between the predictor and response? Create a 2-by-2 grid of plots to check your results visually.
    *the average number of rooms per dwelling
    * median value of owner-occupied homes
    *student-teacher ratio by town
    * nitrogen oxides concentrarion

```{r 3a}
predictors <- c("rm", "medv", "ptratio", "nox")

simple_linear_models_3a <- lapply(predictors, function(pred) {
  lm_formula <- as.formula(paste("crim ~", pred))
  lm(lm_formula, data = Boston)
})

model_summaries_3a <- lapply(simple_linear_models_3a, summary)
model_summaries_3a
```

```{r 3a2x2}
par(mfrow = c(2, 2))
invisible(lapply(predictors, function(pred) {
  plot_formula <- as.formula(paste("crim ~", pred))
  plot(plot_formula, data = Boston, main=pred)
  abline(lm(plot_formula, data = Boston), col = "#d02670")
}))
```

b. Fit a multiple regression model using all four of the predictors listed in part (a). Using a significance level of $\alpha = 0.01$, for which predictors can we reject the null hypothesis that the slope $\beta_j = 0$? Which of these predictors has a statistically significant association with the crime rate in the multiple regression model?

```{r 3b}
multi_model_3b <- lm(crim ~ rm + medv + ptratio + nox, data = Boston)
summary(multi_model_3b)
```

c. How do your results from (a) compare to your results from (b)?
d. Is there evidence of a non-linear association between any of the predictors listed in part (a) and the response? To answer this, try fitting a model of the form $Y = \beta_0 + \beta_1 X+\beta_2 X^2 + \beta_3 X^3 + \epsilon$ for each predictor $X$.

```{r 3d}
cubic_models_3d <- lapply(predictors, function(pred) {
  lm_formula <- as.formula(paste("crim ~ poly(", pred, ", 3)", sep = ""))
  lm(lm_formula, data = Boston)
})

cubic_summaries_3d <- lapply(cubic_models_3d, summary)
cubic_summaries_3d
```

### Q4 (25 Points)

 In this question, you will predict the weekly trend of the S&P 500 stock index using the `Weekly` data set. The data is similar to the `Smarket` data discussed in your textbook, which may be helpful to review.

a. Produce some numerical and graphical summaries of the `Weekly` data. Do there appear to be any patterns?

```{r 4a}
histograms <- list(
  ggplot(Weekly, aes(x = Year)) +
    geom_histogram(fill = "#4589ff", color = "black") +
    labs(title = "Year Distribution", x = "Year", y = "Frequency"),
  ggplot(Weekly, aes(x = Lag1)) +
    geom_histogram(fill = "#4589ff", color = "black") +
    labs(title = "Lag1 Distribution", x = "Lag1", y = "Frequency"),
  ggplot(Weekly, aes(x = Lag2)) +
    geom_histogram(fill = "#4589ff", color = "black") +
    labs(title = "Lag2 Distribution", x = "Lag2", y = "Frequency"),
  ggplot(Weekly, aes(x = Lag3)) +
    geom_histogram(fill = "#4589ff", color = "black") +
    labs(title = "Lag3 Distribution", x = "Lag3", y = "Frequency"),
  ggplot(Weekly, aes(x = Lag4)) +
    geom_histogram(fill = "#4589ff", color = "black") +
    labs(title = "Lag4 Distribution", x = "Lag4", y = "Frequency"),
  ggplot(Weekly, aes(x = Lag5)) +
    geom_histogram(fill = "#4589ff", color = "black") +
    labs(title = "Lag5 Distribution", x = "Lag5", y = "Frequency")
)

boxplot <- ggplot(Weekly, aes(x = "", y = Volume)) +
  geom_boxplot(fill = "#4589ff", color = "black") +
  labs(title = "Volume Distribution", x = NULL, y = "Volume")

time_series <- ggplot(Weekly, aes(x = Year, y = Volume)) +
  geom_line(color = "#4589ff") +
  labs(title = "S&P 500 over Time", x = "Year", y = "S&P 500 Index")

grid.arrange(
  histograms[[1]], histograms[[2]], histograms[[3]],
  histograms[[4]], histograms[[5]], histograms[[6]],
  boxplot, time_series,
  ncol = 2
)
```

b. Using the full data set, perform a logistic regression with `Direction` as the response and the five "lag" variables as predictors. Which (if any) variables appear to be statistically significant?

```{r 4b}
train_control <- trainControl(method = "none")

# Fit the logistic regression model using caret
set.seed(123)  # For reproducibility
logit_model <- train(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, 
                     data = Weekly, 
                     method = "glm", 
                     family = binomial(), 
                     trControl = train_control)
```

```{r}
tidy(logit_model$finalModel)
```

c. Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix says about the types of mistakes made by logistic regression.

```{r 4c}
logit_preds <- predict(logit_model, newdata = Weekly)
confusion_matrix <- confusionMatrix(logit_preds, Weekly$Direction)

print(confusion_matrix)
cat("Overall Accuracy:", confusion_matrix$overall['Accuracy'], "\n")
```

d. Now fit the logistic regression model using a training data period from 1990 to 2008, with `Lag2` as the only predictor. Compute the confusion matrix and overall fraction of correct predictions for the data that you held back when training (that is, the data from 2009 and 2010).

```{r 4d}
train <- subset(Weekly, Year <= 2008)
test <- subset(Weekly, Year > 2008)

logit_model_lag2 <- glm(Direction ~ Lag2, data = train, family = binomial)
test_probs_lag2 <- predict(logit_model_lag2, newdata = test, type = "response")
test_preds_lag2 <- ifelse(test_probs_lag2 > 0.5, "Up", "Down")

confusion_matrix_lag2 <- table(Predicted = test_preds_lag2, Actual = test$Direction)
accuracy_lag2 <- sum(diag(confusion_matrix_lag2)) / sum(confusion_matrix_lag2)
confusion_matrix_lag2
accuracy_lag2
```

e. Repeat part (d) using each of the following models:
    *LDA
    * QDA
    * KNN with $K=1$.

```{r 4e}
train_control <- trainControl(method = "none")
set.seed(123)

lda_model <- train(Direction ~ Lag2, data = train, method = "lda", trControl = train_control)
qda_model <- train(Direction ~ Lag2, data = train, method = "qda", trControl = train_control)
knn_model <- train(Direction ~ Lag2, data = train, method = "knn", trControl = train_control, tuneGrid = data.frame(k = 1))

lda_pred <- predict(lda_model, newdata = test)
qda_pred <- predict(qda_model, newdata = test)
knn_pred <- predict(knn_model, newdata = test)

lda_confusion <- confusionMatrix(lda_pred, test$Direction)
qda_confusion <- confusionMatrix(qda_pred, test$Direction)
knn_confusion <- confusionMatrix(knn_pred, test$Direction)

print(lda_confusion)
print(qda_confusion)
print(knn_confusion)
```

f. Which of these methods appears to provide the beset results on this data?

Based on the method results above, Logistic Regrsiion with Lag2 only and Linear Discriminant Analysis both had an accuracy of 0.625.
